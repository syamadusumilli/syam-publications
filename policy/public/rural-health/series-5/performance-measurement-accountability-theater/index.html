<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Performance Measurement: Accountability Theater | Syam Adusumilli</title>
<meta name="keywords" content="rural-health, rhtp, state-policy, series-5, medicaid, telehealth">
<meta name="description" content="RHTP Series 5: Performance Measurement: Accountability Theater">
<meta name="author" content="Syam Adusumilli">
<link rel="canonical" href="http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1324d480049e0a82077f08d0ee9a7ce0c8f917d468a61974dd878f3838870c9c.css" integrity="sha256-EyTUgASeCoIHfwjQ7pp84Mj5F9Rophl03YePODiHDJw=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/">
  <meta property="og:site_name" content="Syam Adusumilli">
  <meta property="og:title" content="Performance Measurement: Accountability Theater">
  <meta property="og:description" content="RHTP Series 5: Performance Measurement: Accountability Theater">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="rural-health">
    <meta property="article:published_time" content="2026-05-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-05-08T00:00:00+00:00">
    <meta property="article:tag" content="Rural-Health">
    <meta property="article:tag" content="Rhtp">
    <meta property="article:tag" content="State-Policy">
    <meta property="article:tag" content="Series-5">
    <meta property="article:tag" content="Medicaid">
    <meta property="article:tag" content="Telehealth">
    <meta property="og:image" content="http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/cover.webp">
      <meta property="og:see_also" content="http://localhost:1313/rural-health/series-5/seeing-differently-design-approaches-to-state-agency-implementation/">
      <meta property="og:see_also" content="http://localhost:1313/rural-health/series-5/state-agency-decision-authority-matrix/">
      <meta property="og:see_also" content="http://localhost:1313/rural-health/series-5/which-state-agency-structures-support-transformation/">
      <meta property="og:see_also" content="http://localhost:1313/rural-health/series-5/federal-state-relationship-cooperative-federalism-and-its-discontents/">
      <meta property="og:see_also" content="http://localhost:1313/rural-health/series-5/procurement-and-contracting-the-compliance-trap/">
      <meta property="og:see_also" content="http://localhost:1313/rural-health/series-5/stakeholder-coordination-the-limits-of-convening/">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/cover.webp">
<meta name="twitter:title" content="Performance Measurement: Accountability Theater">
<meta name="twitter:description" content="RHTP Series 5: Performance Measurement: Accountability Theater">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Rural Health Transformation Project",
      "item": "http://localhost:1313/rural-health/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Series 5: State Agency Structures",
      "item": "http://localhost:1313/rural-health/series-5/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Performance Measurement: Accountability Theater",
      "item": "http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Performance Measurement: Accountability Theater",
  "name": "Performance Measurement: Accountability Theater",
  "description": "RHTP Series 5: Performance Measurement: Accountability Theater",
  "keywords": [
    "rural-health", "rhtp", "state-policy", "series-5", "medicaid", "telehealth"
  ],
  "articleBody": "RHTP 5.D Performance Measurement: Accountability Theater\nPerformance measurement should enable learning and accountability. RHTP requires states to track progress, report outcomes, and demonstrate that federal investment produces results. The logic is unassailable: taxpayers deserve evidence that their dollars accomplish stated purposes. CMS requires reporting to ensure states implement as promised. States need data to identify what works and adjust what does not.\nIn practice, measurement often becomes theater rather than learning. States with limited capacity spend resources producing reports that no one reads. States with sophisticated systems may game metrics rather than improve outcomes. The burden of measurement falls hardest on the least-resourced states, consuming energy that could fund services. Meaningful accountability, where measurement actually improves programs, remains rare.\nThis article examines the fundamental tension between accountability demands and capacity realities. CMS prescribes measurement requirements that assume capabilities many states lack. States produce compliant reports that satisfy federal oversight without generating useful information. The gap between required and meaningful measurement reveals how accountability systems can undermine the very outcomes they purport to measure.\nAnalytical uncertainty pervades this assessment. We can describe what states report but rarely assess whether reporting improves implementation. The counterfactual, what would happen without measurement requirements, cannot be tested. States may claim that measurement burden harms programs, but this claim is difficult to verify. This article surfaces these uncertainties while assessing available evidence.\nThe Fundamental Tension The Case for Rigorous Measurement Public funds require accountability. Federal grants represent taxpayer dollars transferred to state governments for specified purposes. Without measurement, there is no way to verify that funds achieved their purposes. States could claim success while delivering nothing. Measurement enables oversight that protects public investment.\nWhat gets measured gets managed. The management adage reflects real organizational dynamics. Activities that are tracked receive attention. Activities that are not tracked get neglected. Requiring measurement focuses state attention on outcomes that matter, shifting energy from activities that feel productive to activities that produce results.\nLearning requires data. Transformation involves trying approaches whose effectiveness is uncertain. Without measurement, states cannot distinguish approaches that work from approaches that fail. The iterative improvement that successful transformation requires depends on feedback loops that measurement creates.\nPeer comparison enables improvement. Standardized measurement across states enables identification of high performers whose practices can inform others. States struggling with workforce recruitment can learn from states achieving better retention. Such learning requires comparable data that only standardized measurement produces.\nFederal credibility depends on demonstrated results. RHTP represents a $50 billion investment in rural health transformation. Congressional support for continued funding depends on evidence that investment produces outcomes. Without measurement demonstrating impact, future appropriations become vulnerable to criticism that RHTP wasted federal funds.\nThe Case for Measurement Restraint Capacity varies dramatically. Some states have sophisticated data infrastructure, experienced evaluation staff, and measurement systems developed through prior federal programs. Other states have one part-time employee managing RHTP data alongside other responsibilities. Requiring sophisticated measurement from under-resourced agencies produces compliance exercises, not useful information.\nMeasurement burden diverts implementation resources. Staff hours spent collecting data, writing reports, and responding to CMS inquiries are staff hours not spent implementing programs. States already stretched thin must choose between doing the work and documenting the work. Excessive measurement requirements guarantee that documentation wins.\nProcess metrics substitute for outcome metrics. Outcomes are hard to measure. Health outcomes take years to materialize. Attribution is contested. Data sources are incomplete. Facing these challenges, states measure what they can rather than what matters: activities conducted, trainings delivered, technologies deployed. These process metrics create the appearance of accountability while revealing little about actual impact.\nGaming corrupts metrics. When measurement has consequences, organizations optimize for metrics rather than outcomes. States learn which indicators CMS tracks most closely and focus effort there. Metrics that can be influenced through coding changes or definitional adjustments receive creative attention. The measured system diverges from the actual system.\nLow-capacity states produce low-quality data regardless of requirements. Demanding sophisticated measurement from agencies that lack measurement capability does not create capability. It creates reports that satisfy formal requirements while containing unreliable information. CMS receives data; the data is not meaningful.\nWhat Would Resolve the Tension Evidence on measurement effectiveness would inform this debate but largely does not exist. We lack rigorous research comparing program outcomes under different measurement regimes. Do programs with more extensive measurement requirements achieve better outcomes than programs with lighter requirements? Does measurement burden correlate with implementation success or failure? These questions remain unanswered.\nComparative assessment of state reporting quality would clarify the capacity distribution. How many states produce reports that enable meaningful learning? How many produce compliance documents with little analytical value? Systematic assessment of report quality would reveal where measurement serves its purpose and where it becomes theater.\nAttribution methodology remains contested. Even if states collect accurate data on health outcomes, attributing changes to RHTP investment rather than other factors (economic shifts, demographic changes, unrelated policy changes) requires analytical sophistication that few states possess. Measurement systems cannot produce credible accountability without credible attribution.\nWhy the Tension Cannot Be Fully Resolved Accountability and capacity exist in genuine tension. Requiring measurement from states that lack capacity produces low-quality data. Exempting low-capacity states from measurement requirements eliminates accountability for large portions of federal investment. Neither approach is satisfactory.\nThe political economy of measurement reinforces compliance orientation. Federal officials face criticism for programs that lack accountability mechanisms. State officials face consequences for failing to submit required reports. Both sets of officials are rewarded for the appearance of accountability, regardless of whether accountability mechanisms improve outcomes. The incentive structure produces measurement theater even when all participants recognize its limitations.\nLearning and accountability serve different purposes that sometimes conflict. Learning requires honest acknowledgment of failure. Accountability systems penalize failure acknowledgment. States that truthfully report that an approach did not work risk funding reductions, enhanced monitoring, and political criticism. States that frame setbacks as “implementation challenges” while claiming ultimate success protect themselves from consequences. Measurement systems designed for accountability may impede the learning that transformation requires.\nFederal Framework CMS Performance Measurement Requirements RHTP cooperative agreements specify extensive measurement and reporting requirements. States must submit quarterly progress reports documenting activity against plan milestones, financial expenditures, emerging challenges, and course corrections. Reports follow standardized templates enabling cross-state comparison while allowing narrative explanation of state-specific circumstances.\nAnnual performance reviews assess whether states meet stated objectives and maintain policy alignment. Reviews consider both quantitative metrics and qualitative implementation quality. Poor performance triggers enhanced monitoring, technical assistance requirements, or funding adjustments.\nStandard metrics span multiple domains:\nProcess metrics track activities conducted: number of telehealth consultations provided, community health workers trained, providers receiving loan repayment, patients receiving transportation assistance. These metrics are relatively straightforward to collect and verify.\nOutput metrics measure immediate products: technology platforms deployed, training programs established, regional networks formed, care coordination agreements executed. Outputs represent intermediate results between activities and outcomes.\nOutcome metrics assess ultimate impact: emergency department utilization, preventable hospitalizations, maternal mortality, access to primary care, workforce retention. These metrics require longer timeframes, sophisticated data systems, and attribution methodology that distinguishes RHTP effects from other influences.\nThe Office of Rural Health Transformation reviews performance annually against stated metrics, policy adherence, and resource deployment efficiency. States that underperform face consequences including funding reductions, enhanced reporting requirements, mandatory corrective action plans, or in extreme cases, cooperative agreement termination.\nHow Federal Requirements Intensify Tensions Federal metrics assume data infrastructure that many states lack. Calculating emergency department utilization rates for rural populations requires linked claims data, geographic identifiers, and analytical capability. States without all-payer claims databases or geographic information systems cannot produce these calculations with the precision federal templates assume.\nQuarterly reporting timelines compress data collection windows. States must submit reports within 30 days of quarter end. Data from subawardees may not arrive until weeks after quarter close. Quality review takes additional time. The compression produces reports based on incomplete information, submitted primarily to meet deadlines rather than to inform decisions.\nStandardized templates obscure important variation. State circumstances differ dramatically. A template designed for general applicability cannot capture state-specific context that makes numbers meaningful. A 15% increase in telehealth utilization means something different in Alaska (where telehealth is essential) than in New Jersey (where alternative access exists). Standardization enables comparison at the cost of context.\nWhere Federal Specifications Assume Nonexistent Capacity The gap between required and feasible measurement varies by state. Capacity assessment identifies four state clusters:\nHigh-capacity states (approximately 10-12 states) have robust data infrastructure from prior federal programs, experienced evaluation staff, established relationships with academic partners for analysis, and leadership that values data-informed decision-making. These states can meet CMS requirements and potentially use measurement for genuine learning.\nModerate-capacity states (approximately 15-20 states) have basic data systems and some analytical staff but lack the sophistication CMS requirements assume. These states can produce compliant reports but struggle to use measurement for program improvement.\nLow-capacity states (approximately 10-15 states) have minimal data infrastructure, no dedicated evaluation staff, and limited analytical capability. These states produce reports primarily through heroic individual effort, often by staff for whom RHTP reporting is one of many responsibilities.\nVery-low-capacity states (approximately 5-8 states) lack the basic prerequisites for meaningful measurement. Reports from these states reflect compliance exercises with limited connection to actual program performance.\nCMS requirements do not adjust for this variation. All states receive the same templates, the same deadlines, and the same expectations. The fiction that all states can produce equivalent measurement underlies requirements that make sense for high-capacity states and burden low-capacity states without generating useful information.\nState Measurement Approaches States have adopted varied approaches to performance measurement, reflecting different assessments of compliance burden, learning value, and capacity constraints.\nMeasurement Approach Assessment Approach Characteristics Example States Compliance Burden Learning Value Gaming Risk Minimal Compliance Basic required metrics only; no additional analysis Several small states Low Low Low Administrative Data Claims and administrative records as primary source Ohio, Pennsylvania, Florida Moderate Moderate Moderate Real-Time Monitoring Dashboards with rapid feedback loops California, North Carolina High (requires infrastructure) High (when capacity exists) Variable Community-Defined Community-identified indicators alongside federal metrics New Mexico, tribal programs Variable Potentially high Lower Minimal Compliance Approach States following minimal compliance collect only what CMS requires and invest no additional resources in measurement beyond federal mandates. Reports satisfy formal requirements without generating information useful for program improvement.\nSeveral smaller states with limited administrative capacity have adopted this approach, recognizing that their resources cannot support sophisticated measurement while simultaneously implementing programs. They prioritize implementation over documentation, accepting that their reports will lack the analytical depth of larger states.\nThe approach has clear advantages. Resources flow to services rather than measurement. Staff focus on program delivery rather than data collection. The compliance burden, while still significant, does not overwhelm limited capacity.\nThe approach has significant limitations. States learn nothing from their own experience. Problems persist because they are not detected. Successful approaches are not identified for replication. The state operates blind, hoping that implementation choices prove wise but lacking evidence to assess.\nAdministrative Data Approach States with all-payer claims databases and robust Medicaid data systems use administrative records as the primary measurement source. Claims data reveal utilization patterns, emergency department visits, hospitalizations, and procedure volumes without requiring new data collection.\nOhio, Pennsylvania, and Florida exemplify this approach. Their existing data infrastructure, developed for Medicaid oversight and health planning, provides measurement capacity that RHTP can leverage. Staff analyze existing data rather than collecting new data, reducing burden while enabling meaningful assessment.\nThe approach works well for utilization metrics but struggles with patient experience, access to care, and outcome measures that claims data cannot capture. A patient who cannot get an appointment generates no claim. A patient whose condition worsened due to care delays may generate claims that look similar to a patient whose condition worsened despite excellent care. Administrative data reveal what happened but often not why.\nGaming risk is moderate. Providers can influence claims through coding practices. Changes in reported diagnoses or procedure codes can shift metrics without changing actual care. States relying heavily on administrative data must monitor for coding drift that distorts measurement.\nReal-Time Monitoring Approach States with sophisticated data infrastructure have implemented dashboards providing rapid feedback on key indicators. Program managers can see weekly or monthly trends rather than waiting for quarterly reports. Problems are detected quickly. Successful approaches become visible promptly.\nCalifornia’s RHTP monitoring system integrates data from multiple sources: Medicaid claims, hospital discharge data, vital statistics, provider surveys, and subawardee reports. Dashboards present synthesized information enabling comparison across regions, initiatives, and time periods.\nNorth Carolina similarly invested in monitoring infrastructure, building on systems developed for Medicaid transformation. The state tracks hub network formation, NCCARE360 referral completion, and workforce pipeline progression through integrated platforms.\nThe approach requires substantial investment. Dashboard development, data integration, and ongoing maintenance consume resources. States lacking prior infrastructure investment cannot implement real-time monitoring quickly. The approach is available only to states that made prior investments that RHTP now leverages.\nLearning value is high when capacity exists because rapid feedback enables course correction. States identify implementation challenges while adjustments remain possible rather than discovering problems after years of ineffective operation.\nGaming risk is variable. Real-time visibility can detect gaming attempts quickly. Alternatively, sophisticated actors can identify which metrics receive dashboard attention and game those specifically.\nCommunity-Defined Measurement Some states incorporate community-identified indicators alongside federal metrics. Community advisory bodies identify what outcomes matter most to rural residents. State measurement systems track these community priorities even when they differ from federal specifications.\nNew Mexico’s RHTP measurement includes indicators developed through tribal consultation: traditional healing access, cultural competency of services, community health worker engagement in tribal communities. These metrics do not appear on CMS templates but matter deeply to communities served.\nThe approach recognizes that federal metrics may miss what communities value. Rural residents may care less about emergency department utilization rates than about whether they can see a provider without driving two hours. Community-defined measurement captures these priorities.\nCompliance burden is variable. If community indicators align with required metrics, no additional burden results. If community priorities diverge substantially from federal specifications, states must maintain parallel measurement systems.\nGaming risk is potentially lower because community members observe their own communities. A state cannot claim improved access if community members experience continued difficulty obtaining care. Community oversight provides accountability that federal monitoring cannot.\nWhy States Chose Different Approaches Prior infrastructure investment largely determines current options. States that invested in data systems during ACA implementation, Medicaid expansion, or previous federal grants have capacity that states lacking such investment cannot quickly develop. Measurement approach reflects accumulated capability more than current strategic choice.\nPolitical context shapes measurement investment. Governors and legislators who value evidence-based policy support measurement infrastructure. Those skeptical of government programs may resist data collection as bureaucratic overhead.\nFederal program history matters. States with experience in federal programs emphasizing evaluation (CDC grants, HRSA cooperative agreements, CMS innovation models) developed capacity that transfers to RHTP. States whose federal experience involved lighter measurement requirements lack this foundation.\nThe Measurement Paradox in Practice The following vignettes illustrate how identical measurement requirements produce radically different functions depending on state capacity.\nVignette: Two States, Same Requirements, Completely Different Realities State A has a robust Office of Health Analytics with 12 full-time evaluation staff. The office developed through a decade of federal grants requiring sophisticated measurement, including a State Innovation Model award and a Medicaid transformation grant. Staff have graduate training in epidemiology, health services research, and biostatistics. The state maintains an all-payer claims database, integrated vital statistics, and agreements with academic partners for complex analysis.\nWhen RHTP reporting requirements arrived, State A’s measurement infrastructure absorbed them comfortably. CMS templates became one output among many from existing systems. Staff integrated RHTP indicators into dashboards already tracking Medicaid performance. Quarterly reports drew from analyses conducted for other purposes. The incremental burden was modest.\nMore importantly, State A uses measurement for program improvement. Monthly indicator reviews identify emerging problems. When emergency department utilization increased unexpectedly in one region, staff investigated, identified a gap in after-hours primary care access, and worked with regional partners to address it. The state adjusted workforce deployment based on retention data. Measurement drove decisions.\nState B designated a program coordinator to manage all aspects of RHTP implementation. This person, previously responsible for a small chronic disease prevention program, now oversees a $200 million federal investment. Her other responsibilities did not decrease. She has no evaluation training, no analytical support, and no dedicated data systems.\nWhen RHTP reporting requirements arrived, State B’s coordinator faced an impossible task. Collecting data from subawardees required developing new forms, explaining requirements to organizations with their own capacity constraints, and chasing submissions past deadlines. Quality review was impossible; there was no time and no expertise. The coordinator entered whatever data arrived into CMS templates, often with gaps and inconsistencies.\nState B’s reports satisfy compliance requirements. They arrive on time (usually). They contain numbers in the required fields (mostly). CMS accepts them as fulfilling reporting obligations. But the reports have no connection to program decisions. No one in State B reads the reports after submission. No decisions change based on what the reports reveal. The data are not accurate enough to support meaningful analysis even if anyone had time to conduct it.\nBoth states submit quarterly reports. Both states receive the same CMS response acknowledging receipt. An outside observer reviewing the reports might not immediately recognize the difference. But State A’s measurement enables learning while State B’s measurement is pure theater: an elaborate performance that satisfies formal requirements while accomplishing nothing.\nAlternative Perspectives The Capacity Realism View The capacity realism perspective argues that demanding sophisticated measurement from under-resourced agencies is pointless. Requirements designed for states with robust infrastructure become compliance burdens for states lacking such infrastructure. The pretense that all states can produce equivalent measurement generates bureaucratic waste without improving accountability.\nEvidence supporting this view:\nLow-capacity states consistently produce lower-quality data regardless of requirement stringency. Increasing requirements does not improve data quality; it increases burden without corresponding benefit.\nStaff in low-capacity states describe measurement as the aspect of federal programs most disconnected from actual implementation. They view reporting as compliance exercise rather than useful activity.\nNo evidence demonstrates that measurement requirements improve outcomes in low-capacity states. The presumed mechanism, that measurement enables learning, does not function when measurement infrastructure does not support learning.\nEvidence against this view:\nEven minimal measurement may prevent the worst failures. States that must report some indicators cannot completely abandon program implementation. The requirement to document activity creates baseline accountability.\nMeasurement requirements have prompted some states to invest in capacity they would otherwise have neglected. The prospect of reporting failures has motivated infrastructure development.\nAssessment: The capacity realism view has substantial merit. Complex measurement requirements burden states that can least afford them. Simpler, more focused measurement would serve transformation better than comprehensive requirements that low-capacity states cannot meaningfully implement. However, eliminating measurement entirely would remove even minimal accountability from federal investment.\nThe Gaming Inevitability View The gaming inevitability perspective argues that any measurement system with consequences will be gamed. Organizations optimize for measured indicators rather than underlying outcomes. Resources flow to activities that improve metrics rather than activities that improve results. The more consequential the measurement, the more sophisticated the gaming.\nEvidence supporting this view:\nHistorical examples abound. Hospital readmission penalties led to observation stays that avoided readmission classification without improving patient outcomes. Surgical mortality reporting led to risk aversion that denied care to sick patients. Teacher evaluation tied to test scores led to teaching to tests rather than genuine education.\nRHTP metrics are similarly vulnerable. States can improve telehealth utilization counts through definition changes (what counts as a telehealth visit). Workforce recruitment metrics can improve through retention definition adjustments (how long must someone stay to count as retained). Gaming requires less effort than genuine improvement.\nEvidence against this view:\nNot all measurement is equally gameable. Some metrics resist manipulation. Physical infrastructure either exists or does not. Provider credentials can be verified. Hospital closures cannot be hidden.\nGaming requires sophistication. Low-capacity states may lack the analytical capability to identify gaming opportunities. Their reports may be inaccurate due to incapacity rather than strategic manipulation.\nAssessment: Gaming risk is real and should inform measurement design. Metrics that resist manipulation should receive greater weight than metrics easily influenced through definitional adjustments. However, the existence of gaming does not eliminate measurement value entirely. Triangulating across multiple indicators, comparing self-reported data with independent sources, and focusing on harder-to-game metrics can maintain meaningful accountability despite gaming attempts.\nThe Learning Organization View The learning organization perspective argues that measurement should serve learning rather than accountability. Traditional accountability measurement punishes failure acknowledgment, discouraging honest reporting. Learning-oriented measurement rewards failure identification because identifying what does not work enables improvement.\nEvidence supporting this view:\nOrganizations that treat measurement as learning tool rather than judgment mechanism often produce better outcomes. Toyota’s production system, frequently cited as a management model, treats problems as opportunities for improvement rather than occasions for blame.\nSome federal programs have experimented with learning-oriented evaluation. CMS Innovation Center models included “rapid cycle evaluation” designed to identify what works and what does not without penalizing states whose approaches proved ineffective.\nEvidence against this view:\nPublic accountability cannot be eliminated. Taxpayers have legitimate interest in knowing whether their investment produced results. Learning-oriented measurement that never produces accountability creates its own problems.\nThe distinction between learning and accountability may be impossible to maintain in political environments. Even if program administrators adopt learning orientation, political opponents will use measurement data to attack programs they oppose.\nAssessment: The learning organization view offers valuable insight. Measurement systems that enable honest failure acknowledgment are more likely to produce improvement than systems that punish failure. However, purely learning-oriented measurement without any accountability function is unlikely to survive political scrutiny. The challenge is designing systems that enable learning while maintaining sufficient accountability to sustain public support.\nImplications for RHTP Where Measurement Enables Implementation Measurement supports implementation when:\nStates have infrastructure to produce accurate data. Measurement based on reliable information can reveal patterns that inform decisions.\nStaff have time and capability to analyze measurement results. Data that are collected but never examined serve no purpose.\nOrganizational culture treats measurement as useful rather than burdensome. Leadership must value evidence and act on findings.\nFeedback loops connect measurement to decisions. Identifying a problem matters only if the identification leads to response.\nThese conditions exist in perhaps 10-15 states. For these states, CMS measurement requirements may genuinely support transformation by requiring attention to outcomes that might otherwise be neglected.\nWhere Measurement Burdens Implementation Measurement burdens implementation when:\nData collection consumes resources needed for service delivery. Staff choosing between providing care and documenting care face impossible tradeoffs.\nReports satisfy compliance without generating insight. The labor of measurement produces nothing useful.\nAccuracy is impossible given available systems. Unreliable data mislead rather than inform.\nThese conditions characterize perhaps 20-25 states. For these states, CMS measurement requirements divert energy from implementation without corresponding benefit. The burden is not offset by learning value.\nWarning Signs of Measurement Theater Observable indicators suggest when measurement has become theater rather than learning:\nReports submitted at deadline without internal review. When reports go directly from data entry to CMS without anyone reading them, measurement serves compliance only.\nSame narrative explanations quarter after quarter. When states copy prior explanations without updating, measurement has become rote exercise.\nMetrics that never change despite varied implementation circumstances. Perfect consistency suggests data are constructed rather than collected.\nNo documented program changes based on measurement findings. When measurement never influences decisions, its purpose is performative.\nStaff describing measurement as their most frustrating responsibility. When those closest to measurement view it as worthless, they are probably right.\nWhat Meaningful Accountability Requires Meaningful accountability, where measurement actually improves programs, requires:\nAppropriate scope. Fewer metrics collected well produce more accountability than many metrics collected poorly. CMS should reduce measurement burden while increasing focus on indicators that matter most.\nCapacity-matched expectations. Requirements should reflect state capability. Demanding sophisticated measurement from states lacking sophisticated capacity produces theater.\nLearning orientation. States should be rewarded for identifying what does not work, not punished. Honest reporting serves transformation better than optimistic spin.\nVerification mechanisms. Self-reported data should be validated through independent sources where possible. Trust but verify.\nConsequences proportionate to capacity. States with measurement infrastructure that produce poor results should face different consequences than states lacking measurement infrastructure entirely.\nRecommendations For State Agencies Invest in learning systems, not just compliance systems. The marginal dollar spent on measurement should improve implementation, not merely satisfy federal requirements. If measurement does not inform decisions, it serves no purpose beyond compliance.\nFocus on fewer, more meaningful indicators. Attempting to track everything produces data on nothing. Identify the three to five indicators most likely to reveal whether transformation is occurring. Track those well.\nBuild measurement capacity as implementation infrastructure. Measurement capability is not overhead; it is essential infrastructure for transformation. States that can assess their own progress can adjust their approaches. States that cannot assess progress operate blind.\nConnect measurement to decisions through formal processes. Require that measurement results be reviewed before major decisions. Create accountability for acting on measurement findings, not just for collecting data.\nFor CMS Reduce measurement burden; focus on fewer, more meaningful indicators. Current requirements assume capacity that most states lack. Streamlining requirements would improve measurement quality by enabling states to focus on fewer metrics.\nDifferentiate requirements by state capacity. High-capacity states can produce sophisticated measurement. Low-capacity states cannot. Requiring the same outputs from dramatically different starting points produces compliance theater in low-capacity states.\nEmphasize verification over self-reporting. Where possible, use administrative data that states cannot manipulate rather than self-reported metrics they can construct. Triangulate across data sources to identify discrepancies.\nCreate learning orientation by rewarding honest failure acknowledgment. States that identify approaches that did not work and adjust should receive positive recognition, not penalties. The goal is improvement, not the appearance of success.\nInvest in state measurement capacity as a legitimate RHTP use. If measurement is essential for accountability, measurement infrastructure is legitimate investment. States should be encouraged to use RHTP funds to build capacity that enables meaningful measurement.\nFor Evaluators and Observers Assess measurement system quality, not just metric availability. The existence of reported data does not indicate meaningful measurement. Evaluation should examine whether states can actually produce reliable information, not merely whether they submit reports.\nDistinguish measurement that informs from measurement that performs. The same reports can serve learning or compliance depending on organizational context. Evaluation should assess how measurement is used, not merely what measurement exists.\nTrack measurement burden as implementation factor. States overwhelmed by measurement requirements may fail for reasons unrelated to their transformation approach. Evaluation should distinguish implementation failures from measurement-induced failures.\nExamine gaming and its consequences. When states optimize for metrics rather than outcomes, evaluation should detect this pattern and assess its implications. Metrics that correlate with gaming susceptibility should receive less interpretive weight.\nTransition to Article 5E Performance measurement shapes and is shaped by federal-state relationships. Article 5E examines the tension between federal mandate and state autonomy that underlies RHTP’s cooperative agreement structure. States that produce sophisticated measurement may receive greater federal flexibility. States that produce compliance theater may receive enhanced oversight. The relationship between measurement capacity and federal trust creates dynamics that affect all aspects of RHTP implementation. The accountability demands examined in this article cannot be understood apart from the federal-state power dynamics that Article 5E addresses.\nCross-References 2A (RHTP Structure): Federal measurement requirements and annual re-scoring processes that create accountability context for state measurement.\n5A (Lead Agency Structures): Authority for measurement decisions varies by lead agency designation. Agencies with evaluation capacity can leverage existing infrastructure.\n5C (Procurement): Measurement systems often require procurement. States face similar timeline and compliance challenges for measurement infrastructure that they face for implementation infrastructure.\n5E (Federal Relationship): How measurement quality shapes CMS relationship and federal flexibility. Collaborative relationships may enable measurement burden reduction.\n4-Synthesis (Evidence): Evidence on transformation approach effectiveness depends on state measurement capacity. Weak measurement limits what can be learned from RHTP implementation.\nSeries 3 State Profiles: Individual state profiles document measurement approaches and capacity, providing specific examples of patterns described in this article.\nSources Centers for Medicare and Medicaid Services. “CMS Announces $50 Billion in Awards to Strengthen Rural Health in All 50 States.” CMS Newsroom, 29 Dec. 2025.\nForvis Mazars. “Federal Grant Program Evaluation Part 1: Navigating New Regs.” Forvis Mazars, 1 July 2025.\nGovernment Accountability Office. “Grants Management: Enhancing Performance Accountability Provisions Could Lead to Better Results.” GAO-06-1046, 29 Sept. 2006.\nGovernment Accountability Office. “Managing for Results in Government.” GAO, 2025.\nGovernment Accountability Office. “Performance and Accountability Report, Fiscal Year 2024.” GAO-25-900570, 2024.\nHeinrich, Carolyn J. “Outcomes-Based Performance Management in the Public Sector: Implications for Government Accountability and Effectiveness.” Public Administration Review, vol. 62, no. 6, 2002, pp. 712-725.\nMoynihan, Donald P. The Dynamics of Performance Management: Constructing Information and Reform. Georgetown University Press, 2008.\nNational Academy for State Health Policy. “State Health Agency Capacity Assessment.” NASHP, 2025.\nOffice of Management and Budget. “2 CFR Part 200: Uniform Administrative Requirements, Cost Principles, and Audit Requirements for Federal Awards.” OMB, 2024.\nRadin, Beryl A. Challenging the Performance Movement: Accountability, Complexity, and Democratic Values. Georgetown University Press, 2006.\nU.S. Department of Health and Human Services. “45 CFR Part 75: Uniform Administrative Requirements, Cost Principles, and Audit Requirements for HHS Awards.” HHS, 2024.\nRHTP 5.D Series 5: State Agencies Rural Health Transformation Project Version 1: January 2026\n",
  "wordCount" : "4891",
  "inLanguage": "en",
  "image":"http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/cover.webp","datePublished": "2026-05-08T00:00:00Z",
  "dateModified": "2026-05-08T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Syam Adusumilli"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Syam Adusumilli",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Syam Adusumilli (Alt + H)">Syam Adusumilli</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/rural-health/" title="Rural Health">
                    <span>Rural Health</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/work-requirements/" title="Work Requirements">
                    <span>Work Requirements</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">
<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/rural-health/">Rural Health Transformation Project</a>&nbsp;»&nbsp;<a href="http://localhost:1313/rural-health/series-5/">Series 5: State Agency Structures</a></div>
    <h1 class="post-title entry-hint-parent">
      Performance Measurement: Accountability Theater
    </h1>
    <div class="post-description">
      RHTP Series 5: Performance Measurement: Accountability Theater
    </div>
    <div class="post-meta"><span title='2026-05-08 00:00:00 +0000 UTC'>May 8, 2026</span>&nbsp;·&nbsp;<span>23 min</span>&nbsp;·&nbsp;<span>4891 words</span>&nbsp;·&nbsp;<span>Syam Adusumilli</span>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="http://localhost:1313/cover.webp" alt="Performance Measurement: Accountability Theater">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-fundamental-tension" aria-label="The Fundamental Tension">The Fundamental Tension</a><ul>
                        
                <li>
                    <a href="#the-case-for-rigorous-measurement" aria-label="The Case for Rigorous Measurement">The Case for Rigorous Measurement</a></li>
                <li>
                    <a href="#the-case-for-measurement-restraint" aria-label="The Case for Measurement Restraint">The Case for Measurement Restraint</a></li>
                <li>
                    <a href="#what-would-resolve-the-tension" aria-label="What Would Resolve the Tension">What Would Resolve the Tension</a></li>
                <li>
                    <a href="#why-the-tension-cannot-be-fully-resolved" aria-label="Why the Tension Cannot Be Fully Resolved">Why the Tension Cannot Be Fully Resolved</a></li></ul>
                </li>
                <li>
                    <a href="#federal-framework" aria-label="Federal Framework">Federal Framework</a><ul>
                        
                <li>
                    <a href="#cms-performance-measurement-requirements" aria-label="CMS Performance Measurement Requirements">CMS Performance Measurement Requirements</a></li>
                <li>
                    <a href="#how-federal-requirements-intensify-tensions" aria-label="How Federal Requirements Intensify Tensions">How Federal Requirements Intensify Tensions</a></li>
                <li>
                    <a href="#where-federal-specifications-assume-nonexistent-capacity" aria-label="Where Federal Specifications Assume Nonexistent Capacity">Where Federal Specifications Assume Nonexistent Capacity</a></li></ul>
                </li>
                <li>
                    <a href="#state-measurement-approaches" aria-label="State Measurement Approaches">State Measurement Approaches</a><ul>
                        
                <li>
                    <a href="#measurement-approach-assessment" aria-label="Measurement Approach Assessment">Measurement Approach Assessment</a></li>
                <li>
                    <a href="#minimal-compliance-approach" aria-label="Minimal Compliance Approach">Minimal Compliance Approach</a></li>
                <li>
                    <a href="#administrative-data-approach" aria-label="Administrative Data Approach">Administrative Data Approach</a></li>
                <li>
                    <a href="#real-time-monitoring-approach" aria-label="Real-Time Monitoring Approach">Real-Time Monitoring Approach</a></li>
                <li>
                    <a href="#community-defined-measurement" aria-label="Community-Defined Measurement">Community-Defined Measurement</a></li>
                <li>
                    <a href="#why-states-chose-different-approaches" aria-label="Why States Chose Different Approaches">Why States Chose Different Approaches</a></li></ul>
                </li>
                <li>
                    <a href="#the-measurement-paradox-in-practice" aria-label="The Measurement Paradox in Practice">The Measurement Paradox in Practice</a><ul>
                        
                <li>
                    <a href="#vignette-two-states-same-requirements-completely-different-realities" aria-label="Vignette: Two States, Same Requirements, Completely Different Realities">Vignette: Two States, Same Requirements, Completely Different Realities</a></li></ul>
                </li>
                <li>
                    <a href="#alternative-perspectives" aria-label="Alternative Perspectives">Alternative Perspectives</a><ul>
                        
                <li>
                    <a href="#the-capacity-realism-view" aria-label="The Capacity Realism View">The Capacity Realism View</a></li>
                <li>
                    <a href="#the-gaming-inevitability-view" aria-label="The Gaming Inevitability View">The Gaming Inevitability View</a></li>
                <li>
                    <a href="#the-learning-organization-view" aria-label="The Learning Organization View">The Learning Organization View</a></li></ul>
                </li>
                <li>
                    <a href="#implications-for-rhtp" aria-label="Implications for RHTP">Implications for RHTP</a><ul>
                        
                <li>
                    <a href="#where-measurement-enables-implementation" aria-label="Where Measurement Enables Implementation">Where Measurement Enables Implementation</a></li>
                <li>
                    <a href="#where-measurement-burdens-implementation" aria-label="Where Measurement Burdens Implementation">Where Measurement Burdens Implementation</a></li>
                <li>
                    <a href="#warning-signs-of-measurement-theater" aria-label="Warning Signs of Measurement Theater">Warning Signs of Measurement Theater</a></li>
                <li>
                    <a href="#what-meaningful-accountability-requires" aria-label="What Meaningful Accountability Requires">What Meaningful Accountability Requires</a></li></ul>
                </li>
                <li>
                    <a href="#recommendations" aria-label="Recommendations">Recommendations</a><ul>
                        
                <li>
                    <a href="#for-state-agencies" aria-label="For State Agencies">For State Agencies</a></li>
                <li>
                    <a href="#for-cms" aria-label="For CMS">For CMS</a></li>
                <li>
                    <a href="#for-evaluators-and-observers" aria-label="For Evaluators and Observers">For Evaluators and Observers</a></li></ul>
                </li>
                <li>
                    <a href="#transition-to-article-5e" aria-label="Transition to Article 5E">Transition to Article 5E</a></li>
                <li>
                    <a href="#cross-references" aria-label="Cross-References">Cross-References</a></li>
                <li>
                    <a href="#sources" aria-label="Sources">Sources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="article-with-sidebar">



<nav class="series-sidebar" id="series-sidebar">
  <button class="sidebar-toggle" onclick="document.getElementById('series-sidebar').classList.toggle('collapsed')">
    <span class="toggle-icon">◀</span>
  </button>
  <div class="sidebar-inner">
    <div class="sidebar-project-label">Rural Health Transformation Project</div>
    <h3><a href="http://localhost:1313/rural-health/series-5/">Series 5: State Agency Structures</a></h3>
    <ul>
      
      <li>
        <a href="http://localhost:1313/rural-health/series-5/seeing-differently-design-approaches-to-state-agency-implementation/">Seeing Differently: Design Approaches to State Agency Implementation</a>
      </li>
      
      <li>
        <a href="http://localhost:1313/rural-health/series-5/state-agency-decision-authority-matrix/">State Agency Decision Authority Matrix</a>
      </li>
      
      <li>
        <a href="http://localhost:1313/rural-health/series-5/which-state-agency-structures-support-transformation/">Which State Agency Structures Support Transformation?</a>
      </li>
      
      <li>
        <a href="http://localhost:1313/rural-health/series-5/federal-state-relationship-cooperative-federalism-and-its-discontents/">Federal-State Relationship: Cooperative Federalism and Its Discontents</a>
      </li>
      
      <li class="current">
        <a href="http://localhost:1313/rural-health/series-5/performance-measurement-accountability-theater/">Performance Measurement: Accountability Theater</a>
      </li>
      
      <li>
        <a href="http://localhost:1313/rural-health/series-5/procurement-and-contracting-the-compliance-trap/">Procurement and Contracting: The Compliance Trap</a>
      </li>
      
      <li>
        <a href="http://localhost:1313/rural-health/series-5/stakeholder-coordination-the-limits-of-convening/">Stakeholder Coordination: The Limits of Convening</a>
      </li>
      
      <li>
        <a href="http://localhost:1313/rural-health/series-5/lead-agency-structures-and-the-accountability-illusion/">Lead Agency Structures and the Accountability Illusion</a>
      </li>
      
    </ul>
    
    <h3 class="other-series-heading"><a href="http://localhost:1313/rural-health/">All Series</a></h3>
    <ul class="other-series">
      
      
      <li><a href="http://localhost:1313/rural-health/series-1/">Series 1: Understanding Rural and Deep Rural America</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-10/">Series 10: Regional Profiles</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-11/">Series 11: Clinical Reality</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-12/">Series 12: Coverage and Financing</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-13/">Series 13: Trust and Navigation</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-14/">Series 14: Infrastructure Models</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-15/">Series 15: Regulatory and Workforce</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-16/">Series 16: Integration and Scenarios</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-2/">Series 2: The Federal Architecture of Rural Health</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-3/">Series 3: State-by-State Analysis</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-4/">Series 4: Evidence-Based Strategies</a></li>
      
      
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-6/">Series 6: Intermediary Organizations</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-7/">Series 7: Rural Provider Landscape</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-8/">Series 8: Community Infrastructure</a></li>
      
      
      
      <li><a href="http://localhost:1313/rural-health/series-9/">Series 9: Special Populations</a></li>
      
      
    </ul>
    
  </div>
</nav>
<div class="post-content">
<div style="margin: 0.75rem 0 1.5rem; padding: 0.5rem 0.75rem;
     background: var(--code-bg); border-radius: 4px; font-size: 0.85rem;">
  <a href="../performance-measurement-accountability-theater-summary/" style="text-decoration: none; color: var(--primary);">
    Read the Executive Summary
  </a>
</div>

<p>RHTP 5.D
Performance Measurement: Accountability Theater</p>
<p>Performance measurement should enable learning and accountability. RHTP requires states to track progress, report outcomes, and demonstrate that federal investment produces results. The logic is unassailable: taxpayers deserve evidence that their dollars accomplish stated purposes. CMS requires reporting to ensure states implement as promised. States need data to identify what works and adjust what does not.</p>
<p><strong>In practice, measurement often becomes theater rather than learning.</strong> States with limited capacity spend resources producing reports that no one reads. States with sophisticated systems may game metrics rather than improve outcomes. The burden of measurement falls hardest on the least-resourced states, consuming energy that could fund services. Meaningful accountability, where measurement actually improves programs, remains rare.</p>
<p><strong>This article examines the fundamental tension between accountability demands and capacity realities.</strong> CMS prescribes measurement requirements that assume capabilities many states lack. States produce compliant reports that satisfy federal oversight without generating useful information. The gap between required and meaningful measurement reveals how accountability systems can undermine the very outcomes they purport to measure.</p>
<p><strong>Analytical uncertainty pervades this assessment.</strong> We can describe what states report but rarely assess whether reporting improves implementation. The counterfactual, what would happen without measurement requirements, cannot be tested. States may claim that measurement burden harms programs, but this claim is difficult to verify. This article surfaces these uncertainties while assessing available evidence.</p>
<h2 id="the-fundamental-tension">The Fundamental Tension<a hidden class="anchor" aria-hidden="true" href="#the-fundamental-tension">#</a></h2>
<h3 id="the-case-for-rigorous-measurement">The Case for Rigorous Measurement<a hidden class="anchor" aria-hidden="true" href="#the-case-for-rigorous-measurement">#</a></h3>
<p><strong>Public funds require accountability.</strong> Federal grants represent taxpayer dollars transferred to state governments for specified purposes. Without measurement, there is no way to verify that funds achieved their purposes. States could claim success while delivering nothing. Measurement enables oversight that protects public investment.</p>
<p><strong>What gets measured gets managed.</strong> The management adage reflects real organizational dynamics. Activities that are tracked receive attention. Activities that are not tracked get neglected. Requiring measurement focuses state attention on outcomes that matter, shifting energy from activities that feel productive to activities that produce results.</p>
<p><strong>Learning requires data.</strong> Transformation involves trying approaches whose effectiveness is uncertain. Without measurement, states cannot distinguish approaches that work from approaches that fail. The iterative improvement that successful transformation requires depends on feedback loops that measurement creates.</p>
<p><strong>Peer comparison enables improvement.</strong> Standardized measurement across states enables identification of high performers whose practices can inform others. States struggling with workforce recruitment can learn from states achieving better retention. Such learning requires comparable data that only standardized measurement produces.</p>
<p><strong>Federal credibility depends on demonstrated results.</strong> RHTP represents a $50 billion investment in rural health transformation. Congressional support for continued funding depends on evidence that investment produces outcomes. Without measurement demonstrating impact, future appropriations become vulnerable to criticism that RHTP wasted federal funds.</p>
<h3 id="the-case-for-measurement-restraint">The Case for Measurement Restraint<a hidden class="anchor" aria-hidden="true" href="#the-case-for-measurement-restraint">#</a></h3>
<p><strong>Capacity varies dramatically.</strong> Some states have sophisticated data infrastructure, experienced evaluation staff, and measurement systems developed through prior federal programs. Other states have one part-time employee managing RHTP data alongside other responsibilities. Requiring sophisticated measurement from under-resourced agencies produces compliance exercises, not useful information.</p>
<p><strong>Measurement burden diverts implementation resources.</strong> Staff hours spent collecting data, writing reports, and responding to CMS inquiries are staff hours not spent implementing programs. States already stretched thin must choose between doing the work and documenting the work. Excessive measurement requirements guarantee that documentation wins.</p>
<p><strong>Process metrics substitute for outcome metrics.</strong> Outcomes are hard to measure. Health outcomes take years to materialize. Attribution is contested. Data sources are incomplete. Facing these challenges, states measure what they can rather than what matters: activities conducted, trainings delivered, technologies deployed. These process metrics create the appearance of accountability while revealing little about actual impact.</p>
<p><strong>Gaming corrupts metrics.</strong> When measurement has consequences, organizations optimize for metrics rather than outcomes. States learn which indicators CMS tracks most closely and focus effort there. Metrics that can be influenced through coding changes or definitional adjustments receive creative attention. The measured system diverges from the actual system.</p>
<p><strong>Low-capacity states produce low-quality data regardless of requirements.</strong> Demanding sophisticated measurement from agencies that lack measurement capability does not create capability. It creates reports that satisfy formal requirements while containing unreliable information. CMS receives data; the data is not meaningful.</p>
<h3 id="what-would-resolve-the-tension">What Would Resolve the Tension<a hidden class="anchor" aria-hidden="true" href="#what-would-resolve-the-tension">#</a></h3>
<p><strong>Evidence on measurement effectiveness would inform this debate but largely does not exist.</strong> We lack rigorous research comparing program outcomes under different measurement regimes. Do programs with more extensive measurement requirements achieve better outcomes than programs with lighter requirements? Does measurement burden correlate with implementation success or failure? These questions remain unanswered.</p>
<p><strong>Comparative assessment of state reporting quality would clarify the capacity distribution.</strong> How many states produce reports that enable meaningful learning? How many produce compliance documents with little analytical value? Systematic assessment of report quality would reveal where measurement serves its purpose and where it becomes theater.</p>
<p><strong>Attribution methodology remains contested.</strong> Even if states collect accurate data on health outcomes, attributing changes to RHTP investment rather than other factors (economic shifts, demographic changes, unrelated policy changes) requires analytical sophistication that few states possess. Measurement systems cannot produce credible accountability without credible attribution.</p>
<h3 id="why-the-tension-cannot-be-fully-resolved">Why the Tension Cannot Be Fully Resolved<a hidden class="anchor" aria-hidden="true" href="#why-the-tension-cannot-be-fully-resolved">#</a></h3>
<p><strong>Accountability and capacity exist in genuine tension.</strong> Requiring measurement from states that lack capacity produces low-quality data. Exempting low-capacity states from measurement requirements eliminates accountability for large portions of federal investment. Neither approach is satisfactory.</p>
<p><strong>The political economy of measurement reinforces compliance orientation.</strong> Federal officials face criticism for programs that lack accountability mechanisms. State officials face consequences for failing to submit required reports. Both sets of officials are rewarded for the appearance of accountability, regardless of whether accountability mechanisms improve outcomes. The incentive structure produces measurement theater even when all participants recognize its limitations.</p>
<p><strong>Learning and accountability serve different purposes that sometimes conflict.</strong> Learning requires honest acknowledgment of failure. Accountability systems penalize failure acknowledgment. States that truthfully report that an approach did not work risk funding reductions, enhanced monitoring, and political criticism. States that frame setbacks as &ldquo;implementation challenges&rdquo; while claiming ultimate success protect themselves from consequences. Measurement systems designed for accountability may impede the learning that transformation requires.</p>
<h2 id="federal-framework">Federal Framework<a hidden class="anchor" aria-hidden="true" href="#federal-framework">#</a></h2>
<h3 id="cms-performance-measurement-requirements">CMS Performance Measurement Requirements<a hidden class="anchor" aria-hidden="true" href="#cms-performance-measurement-requirements">#</a></h3>
<p>RHTP cooperative agreements specify extensive measurement and reporting requirements. <strong>States must submit quarterly progress reports</strong> documenting activity against plan milestones, financial expenditures, emerging challenges, and course corrections. Reports follow standardized templates enabling cross-state comparison while allowing narrative explanation of state-specific circumstances.</p>
<p><strong>Annual performance reviews</strong> assess whether states meet stated objectives and maintain policy alignment. Reviews consider both quantitative metrics and qualitative implementation quality. Poor performance triggers enhanced monitoring, technical assistance requirements, or funding adjustments.</p>
<p><strong>Standard metrics span multiple domains:</strong></p>
<p>Process metrics track activities conducted: number of telehealth consultations provided, community health workers trained, providers receiving loan repayment, patients receiving transportation assistance. These metrics are relatively straightforward to collect and verify.</p>
<p>Output metrics measure immediate products: technology platforms deployed, training programs established, regional networks formed, care coordination agreements executed. Outputs represent intermediate results between activities and outcomes.</p>
<p>Outcome metrics assess ultimate impact: emergency department utilization, preventable hospitalizations, maternal mortality, access to primary care, workforce retention. These metrics require longer timeframes, sophisticated data systems, and attribution methodology that distinguishes RHTP effects from other influences.</p>
<p><strong>The Office of Rural Health Transformation reviews performance annually</strong> against stated metrics, policy adherence, and resource deployment efficiency. States that underperform face consequences including funding reductions, enhanced reporting requirements, mandatory corrective action plans, or in extreme cases, cooperative agreement termination.</p>
<h3 id="how-federal-requirements-intensify-tensions">How Federal Requirements Intensify Tensions<a hidden class="anchor" aria-hidden="true" href="#how-federal-requirements-intensify-tensions">#</a></h3>
<p><strong>Federal metrics assume data infrastructure that many states lack.</strong> Calculating emergency department utilization rates for rural populations requires linked claims data, geographic identifiers, and analytical capability. States without all-payer claims databases or geographic information systems cannot produce these calculations with the precision federal templates assume.</p>
<p><strong>Quarterly reporting timelines compress data collection windows.</strong> States must submit reports within 30 days of quarter end. Data from subawardees may not arrive until weeks after quarter close. Quality review takes additional time. The compression produces reports based on incomplete information, submitted primarily to meet deadlines rather than to inform decisions.</p>
<p><strong>Standardized templates obscure important variation.</strong> State circumstances differ dramatically. A template designed for general applicability cannot capture state-specific context that makes numbers meaningful. A 15% increase in telehealth utilization means something different in Alaska (where telehealth is essential) than in New Jersey (where alternative access exists). Standardization enables comparison at the cost of context.</p>
<h3 id="where-federal-specifications-assume-nonexistent-capacity">Where Federal Specifications Assume Nonexistent Capacity<a hidden class="anchor" aria-hidden="true" href="#where-federal-specifications-assume-nonexistent-capacity">#</a></h3>
<p><strong>The gap between required and feasible measurement varies by state.</strong> Capacity assessment identifies four state clusters:</p>
<p>High-capacity states (approximately 10-12 states) have robust data infrastructure from prior federal programs, experienced evaluation staff, established relationships with academic partners for analysis, and leadership that values data-informed decision-making. These states can meet CMS requirements and potentially use measurement for genuine learning.</p>
<p>Moderate-capacity states (approximately 15-20 states) have basic data systems and some analytical staff but lack the sophistication CMS requirements assume. These states can produce compliant reports but struggle to use measurement for program improvement.</p>
<p>Low-capacity states (approximately 10-15 states) have minimal data infrastructure, no dedicated evaluation staff, and limited analytical capability. These states produce reports primarily through heroic individual effort, often by staff for whom RHTP reporting is one of many responsibilities.</p>
<p>Very-low-capacity states (approximately 5-8 states) lack the basic prerequisites for meaningful measurement. Reports from these states reflect compliance exercises with limited connection to actual program performance.</p>
<p><strong>CMS requirements do not adjust for this variation.</strong> All states receive the same templates, the same deadlines, and the same expectations. The fiction that all states can produce equivalent measurement underlies requirements that make sense for high-capacity states and burden low-capacity states without generating useful information.</p>
<h2 id="state-measurement-approaches">State Measurement Approaches<a hidden class="anchor" aria-hidden="true" href="#state-measurement-approaches">#</a></h2>
<p>States have adopted varied approaches to performance measurement, reflecting different assessments of compliance burden, learning value, and capacity constraints.</p>
<h3 id="measurement-approach-assessment">Measurement Approach Assessment<a hidden class="anchor" aria-hidden="true" href="#measurement-approach-assessment">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Approach</th>
          <th>Characteristics</th>
          <th>Example States</th>
          <th>Compliance Burden</th>
          <th>Learning Value</th>
          <th>Gaming Risk</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Minimal Compliance</strong></td>
          <td>Basic required metrics only; no additional analysis</td>
          <td>Several small states</td>
          <td>Low</td>
          <td>Low</td>
          <td>Low</td>
      </tr>
      <tr>
          <td><strong>Administrative Data</strong></td>
          <td>Claims and administrative records as primary source</td>
          <td>Ohio, Pennsylvania, Florida</td>
          <td>Moderate</td>
          <td>Moderate</td>
          <td>Moderate</td>
      </tr>
      <tr>
          <td><strong>Real-Time Monitoring</strong></td>
          <td>Dashboards with rapid feedback loops</td>
          <td>California, North Carolina</td>
          <td>High (requires infrastructure)</td>
          <td>High (when capacity exists)</td>
          <td>Variable</td>
      </tr>
      <tr>
          <td><strong>Community-Defined</strong></td>
          <td>Community-identified indicators alongside federal metrics</td>
          <td>New Mexico, tribal programs</td>
          <td>Variable</td>
          <td>Potentially high</td>
          <td>Lower</td>
      </tr>
  </tbody>
</table>
<h3 id="minimal-compliance-approach">Minimal Compliance Approach<a hidden class="anchor" aria-hidden="true" href="#minimal-compliance-approach">#</a></h3>
<p><strong>States following minimal compliance collect only what CMS requires</strong> and invest no additional resources in measurement beyond federal mandates. Reports satisfy formal requirements without generating information useful for program improvement.</p>
<p><strong>Several smaller states with limited administrative capacity</strong> have adopted this approach, recognizing that their resources cannot support sophisticated measurement while simultaneously implementing programs. They prioritize implementation over documentation, accepting that their reports will lack the analytical depth of larger states.</p>
<p><strong>The approach has clear advantages.</strong> Resources flow to services rather than measurement. Staff focus on program delivery rather than data collection. The compliance burden, while still significant, does not overwhelm limited capacity.</p>
<p><strong>The approach has significant limitations.</strong> States learn nothing from their own experience. Problems persist because they are not detected. Successful approaches are not identified for replication. The state operates blind, hoping that implementation choices prove wise but lacking evidence to assess.</p>
<h3 id="administrative-data-approach">Administrative Data Approach<a hidden class="anchor" aria-hidden="true" href="#administrative-data-approach">#</a></h3>
<p><strong>States with all-payer claims databases and robust Medicaid data systems</strong> use administrative records as the primary measurement source. Claims data reveal utilization patterns, emergency department visits, hospitalizations, and procedure volumes without requiring new data collection.</p>
<p><strong>Ohio, Pennsylvania, and Florida exemplify this approach.</strong> Their existing data infrastructure, developed for Medicaid oversight and health planning, provides measurement capacity that RHTP can leverage. Staff analyze existing data rather than collecting new data, reducing burden while enabling meaningful assessment.</p>
<p><strong>The approach works well for utilization metrics</strong> but struggles with patient experience, access to care, and outcome measures that claims data cannot capture. A patient who cannot get an appointment generates no claim. A patient whose condition worsened due to care delays may generate claims that look similar to a patient whose condition worsened despite excellent care. Administrative data reveal what happened but often not why.</p>
<p><strong>Gaming risk is moderate.</strong> Providers can influence claims through coding practices. Changes in reported diagnoses or procedure codes can shift metrics without changing actual care. States relying heavily on administrative data must monitor for coding drift that distorts measurement.</p>
<h3 id="real-time-monitoring-approach">Real-Time Monitoring Approach<a hidden class="anchor" aria-hidden="true" href="#real-time-monitoring-approach">#</a></h3>
<p><strong>States with sophisticated data infrastructure have implemented dashboards</strong> providing rapid feedback on key indicators. Program managers can see weekly or monthly trends rather than waiting for quarterly reports. Problems are detected quickly. Successful approaches become visible promptly.</p>
<p><strong>California&rsquo;s RHTP monitoring system integrates data from multiple sources</strong>: Medicaid claims, hospital discharge data, vital statistics, provider surveys, and subawardee reports. Dashboards present synthesized information enabling comparison across regions, initiatives, and time periods.</p>
<p><strong>North Carolina similarly invested in monitoring infrastructure</strong>, building on systems developed for Medicaid transformation. The state tracks hub network formation, NCCARE360 referral completion, and workforce pipeline progression through integrated platforms.</p>
<p><strong>The approach requires substantial investment.</strong> Dashboard development, data integration, and ongoing maintenance consume resources. States lacking prior infrastructure investment cannot implement real-time monitoring quickly. The approach is available only to states that made prior investments that RHTP now leverages.</p>
<p><strong>Learning value is high when capacity exists</strong> because rapid feedback enables course correction. States identify implementation challenges while adjustments remain possible rather than discovering problems after years of ineffective operation.</p>
<p><strong>Gaming risk is variable.</strong> Real-time visibility can detect gaming attempts quickly. Alternatively, sophisticated actors can identify which metrics receive dashboard attention and game those specifically.</p>
<h3 id="community-defined-measurement">Community-Defined Measurement<a hidden class="anchor" aria-hidden="true" href="#community-defined-measurement">#</a></h3>
<p><strong>Some states incorporate community-identified indicators</strong> alongside federal metrics. Community advisory bodies identify what outcomes matter most to rural residents. State measurement systems track these community priorities even when they differ from federal specifications.</p>
<p><strong>New Mexico&rsquo;s RHTP measurement includes indicators developed through tribal consultation</strong>: traditional healing access, cultural competency of services, community health worker engagement in tribal communities. These metrics do not appear on CMS templates but matter deeply to communities served.</p>
<p><strong>The approach recognizes that federal metrics may miss what communities value.</strong> Rural residents may care less about emergency department utilization rates than about whether they can see a provider without driving two hours. Community-defined measurement captures these priorities.</p>
<p><strong>Compliance burden is variable.</strong> If community indicators align with required metrics, no additional burden results. If community priorities diverge substantially from federal specifications, states must maintain parallel measurement systems.</p>
<p><strong>Gaming risk is potentially lower</strong> because community members observe their own communities. A state cannot claim improved access if community members experience continued difficulty obtaining care. Community oversight provides accountability that federal monitoring cannot.</p>
<h3 id="why-states-chose-different-approaches">Why States Chose Different Approaches<a hidden class="anchor" aria-hidden="true" href="#why-states-chose-different-approaches">#</a></h3>
<p><strong>Prior infrastructure investment largely determines current options.</strong> States that invested in data systems during ACA implementation, Medicaid expansion, or previous federal grants have capacity that states lacking such investment cannot quickly develop. Measurement approach reflects accumulated capability more than current strategic choice.</p>
<p><strong>Political context shapes measurement investment.</strong> Governors and legislators who value evidence-based policy support measurement infrastructure. Those skeptical of government programs may resist data collection as bureaucratic overhead.</p>
<p><strong>Federal program history matters.</strong> States with experience in federal programs emphasizing evaluation (CDC grants, HRSA cooperative agreements, CMS innovation models) developed capacity that transfers to RHTP. States whose federal experience involved lighter measurement requirements lack this foundation.</p>
<h2 id="the-measurement-paradox-in-practice">The Measurement Paradox in Practice<a hidden class="anchor" aria-hidden="true" href="#the-measurement-paradox-in-practice">#</a></h2>
<p>The following vignettes illustrate how identical measurement requirements produce radically different functions depending on state capacity.</p>
<h3 id="vignette-two-states-same-requirements-completely-different-realities">Vignette: Two States, Same Requirements, Completely Different Realities<a hidden class="anchor" aria-hidden="true" href="#vignette-two-states-same-requirements-completely-different-realities">#</a></h3>
<p><strong>State A</strong> has a robust Office of Health Analytics with 12 full-time evaluation staff. The office developed through a decade of federal grants requiring sophisticated measurement, including a State Innovation Model award and a Medicaid transformation grant. Staff have graduate training in epidemiology, health services research, and biostatistics. The state maintains an all-payer claims database, integrated vital statistics, and agreements with academic partners for complex analysis.</p>
<p>When RHTP reporting requirements arrived, <strong>State A&rsquo;s measurement infrastructure absorbed them comfortably.</strong> CMS templates became one output among many from existing systems. Staff integrated RHTP indicators into dashboards already tracking Medicaid performance. Quarterly reports drew from analyses conducted for other purposes. The incremental burden was modest.</p>
<p><strong>More importantly, State A uses measurement for program improvement.</strong> Monthly indicator reviews identify emerging problems. When emergency department utilization increased unexpectedly in one region, staff investigated, identified a gap in after-hours primary care access, and worked with regional partners to address it. The state adjusted workforce deployment based on retention data. Measurement drove decisions.</p>
<p><strong>State B</strong> designated a program coordinator to manage all aspects of RHTP implementation. This person, previously responsible for a small chronic disease prevention program, now oversees a $200 million federal investment. Her other responsibilities did not decrease. She has no evaluation training, no analytical support, and no dedicated data systems.</p>
<p>When RHTP reporting requirements arrived, <strong>State B&rsquo;s coordinator faced an impossible task.</strong> Collecting data from subawardees required developing new forms, explaining requirements to organizations with their own capacity constraints, and chasing submissions past deadlines. Quality review was impossible; there was no time and no expertise. The coordinator entered whatever data arrived into CMS templates, often with gaps and inconsistencies.</p>
<p><strong>State B&rsquo;s reports satisfy compliance requirements.</strong> They arrive on time (usually). They contain numbers in the required fields (mostly). CMS accepts them as fulfilling reporting obligations. But <strong>the reports have no connection to program decisions.</strong> No one in State B reads the reports after submission. No decisions change based on what the reports reveal. The data are not accurate enough to support meaningful analysis even if anyone had time to conduct it.</p>
<p><strong>Both states submit quarterly reports.</strong> Both states receive the same CMS response acknowledging receipt. An outside observer reviewing the reports might not immediately recognize the difference. But State A&rsquo;s measurement enables learning while State B&rsquo;s measurement is pure theater: an elaborate performance that satisfies formal requirements while accomplishing nothing.</p>
<h2 id="alternative-perspectives">Alternative Perspectives<a hidden class="anchor" aria-hidden="true" href="#alternative-perspectives">#</a></h2>
<h3 id="the-capacity-realism-view">The Capacity Realism View<a hidden class="anchor" aria-hidden="true" href="#the-capacity-realism-view">#</a></h3>
<p>The capacity realism perspective argues that <strong>demanding sophisticated measurement from under-resourced agencies is pointless.</strong> Requirements designed for states with robust infrastructure become compliance burdens for states lacking such infrastructure. The pretense that all states can produce equivalent measurement generates bureaucratic waste without improving accountability.</p>
<p><strong>Evidence supporting this view:</strong></p>
<p>Low-capacity states consistently produce lower-quality data regardless of requirement stringency. Increasing requirements does not improve data quality; it increases burden without corresponding benefit.</p>
<p>Staff in low-capacity states describe measurement as the aspect of federal programs most disconnected from actual implementation. They view reporting as compliance exercise rather than useful activity.</p>
<p>No evidence demonstrates that measurement requirements improve outcomes in low-capacity states. The presumed mechanism, that measurement enables learning, does not function when measurement infrastructure does not support learning.</p>
<p><strong>Evidence against this view:</strong></p>
<p>Even minimal measurement may prevent the worst failures. States that must report some indicators cannot completely abandon program implementation. The requirement to document activity creates baseline accountability.</p>
<p>Measurement requirements have prompted some states to invest in capacity they would otherwise have neglected. The prospect of reporting failures has motivated infrastructure development.</p>
<p><strong>Assessment:</strong> The capacity realism view has substantial merit. Complex measurement requirements burden states that can least afford them. Simpler, more focused measurement would serve transformation better than comprehensive requirements that low-capacity states cannot meaningfully implement. However, eliminating measurement entirely would remove even minimal accountability from federal investment.</p>
<h3 id="the-gaming-inevitability-view">The Gaming Inevitability View<a hidden class="anchor" aria-hidden="true" href="#the-gaming-inevitability-view">#</a></h3>
<p>The gaming inevitability perspective argues that <strong>any measurement system with consequences will be gamed.</strong> Organizations optimize for measured indicators rather than underlying outcomes. Resources flow to activities that improve metrics rather than activities that improve results. The more consequential the measurement, the more sophisticated the gaming.</p>
<p><strong>Evidence supporting this view:</strong></p>
<p>Historical examples abound. Hospital readmission penalties led to observation stays that avoided readmission classification without improving patient outcomes. Surgical mortality reporting led to risk aversion that denied care to sick patients. Teacher evaluation tied to test scores led to teaching to tests rather than genuine education.</p>
<p>RHTP metrics are similarly vulnerable. States can improve telehealth utilization counts through definition changes (what counts as a telehealth visit). Workforce recruitment metrics can improve through retention definition adjustments (how long must someone stay to count as retained). Gaming requires less effort than genuine improvement.</p>
<p><strong>Evidence against this view:</strong></p>
<p>Not all measurement is equally gameable. Some metrics resist manipulation. Physical infrastructure either exists or does not. Provider credentials can be verified. Hospital closures cannot be hidden.</p>
<p>Gaming requires sophistication. Low-capacity states may lack the analytical capability to identify gaming opportunities. Their reports may be inaccurate due to incapacity rather than strategic manipulation.</p>
<p><strong>Assessment:</strong> Gaming risk is real and should inform measurement design. Metrics that resist manipulation should receive greater weight than metrics easily influenced through definitional adjustments. However, the existence of gaming does not eliminate measurement value entirely. Triangulating across multiple indicators, comparing self-reported data with independent sources, and focusing on harder-to-game metrics can maintain meaningful accountability despite gaming attempts.</p>
<h3 id="the-learning-organization-view">The Learning Organization View<a hidden class="anchor" aria-hidden="true" href="#the-learning-organization-view">#</a></h3>
<p>The learning organization perspective argues that <strong>measurement should serve learning rather than accountability.</strong> Traditional accountability measurement punishes failure acknowledgment, discouraging honest reporting. Learning-oriented measurement rewards failure identification because identifying what does not work enables improvement.</p>
<p><strong>Evidence supporting this view:</strong></p>
<p>Organizations that treat measurement as learning tool rather than judgment mechanism often produce better outcomes. Toyota&rsquo;s production system, frequently cited as a management model, treats problems as opportunities for improvement rather than occasions for blame.</p>
<p>Some federal programs have experimented with learning-oriented evaluation. CMS Innovation Center models included &ldquo;rapid cycle evaluation&rdquo; designed to identify what works and what does not without penalizing states whose approaches proved ineffective.</p>
<p><strong>Evidence against this view:</strong></p>
<p>Public accountability cannot be eliminated. Taxpayers have legitimate interest in knowing whether their investment produced results. Learning-oriented measurement that never produces accountability creates its own problems.</p>
<p>The distinction between learning and accountability may be impossible to maintain in political environments. Even if program administrators adopt learning orientation, political opponents will use measurement data to attack programs they oppose.</p>
<p><strong>Assessment:</strong> The learning organization view offers valuable insight. Measurement systems that enable honest failure acknowledgment are more likely to produce improvement than systems that punish failure. However, purely learning-oriented measurement without any accountability function is unlikely to survive political scrutiny. The challenge is designing systems that enable learning while maintaining sufficient accountability to sustain public support.</p>
<h2 id="implications-for-rhtp">Implications for RHTP<a hidden class="anchor" aria-hidden="true" href="#implications-for-rhtp">#</a></h2>
<h3 id="where-measurement-enables-implementation">Where Measurement Enables Implementation<a hidden class="anchor" aria-hidden="true" href="#where-measurement-enables-implementation">#</a></h3>
<p><strong>Measurement supports implementation when:</strong></p>
<p>States have infrastructure to produce accurate data. Measurement based on reliable information can reveal patterns that inform decisions.</p>
<p>Staff have time and capability to analyze measurement results. Data that are collected but never examined serve no purpose.</p>
<p>Organizational culture treats measurement as useful rather than burdensome. Leadership must value evidence and act on findings.</p>
<p>Feedback loops connect measurement to decisions. Identifying a problem matters only if the identification leads to response.</p>
<p>These conditions exist in perhaps 10-15 states. <strong>For these states, CMS measurement requirements may genuinely support transformation</strong> by requiring attention to outcomes that might otherwise be neglected.</p>
<h3 id="where-measurement-burdens-implementation">Where Measurement Burdens Implementation<a hidden class="anchor" aria-hidden="true" href="#where-measurement-burdens-implementation">#</a></h3>
<p><strong>Measurement burdens implementation when:</strong></p>
<p>Data collection consumes resources needed for service delivery. Staff choosing between providing care and documenting care face impossible tradeoffs.</p>
<p>Reports satisfy compliance without generating insight. The labor of measurement produces nothing useful.</p>
<p>Accuracy is impossible given available systems. Unreliable data mislead rather than inform.</p>
<p>These conditions characterize perhaps 20-25 states. <strong>For these states, CMS measurement requirements divert energy from implementation</strong> without corresponding benefit. The burden is not offset by learning value.</p>
<h3 id="warning-signs-of-measurement-theater">Warning Signs of Measurement Theater<a hidden class="anchor" aria-hidden="true" href="#warning-signs-of-measurement-theater">#</a></h3>
<p><strong>Observable indicators suggest when measurement has become theater rather than learning:</strong></p>
<p>Reports submitted at deadline without internal review. When reports go directly from data entry to CMS without anyone reading them, measurement serves compliance only.</p>
<p>Same narrative explanations quarter after quarter. When states copy prior explanations without updating, measurement has become rote exercise.</p>
<p>Metrics that never change despite varied implementation circumstances. Perfect consistency suggests data are constructed rather than collected.</p>
<p>No documented program changes based on measurement findings. When measurement never influences decisions, its purpose is performative.</p>
<p>Staff describing measurement as their most frustrating responsibility. When those closest to measurement view it as worthless, they are probably right.</p>
<h3 id="what-meaningful-accountability-requires">What Meaningful Accountability Requires<a hidden class="anchor" aria-hidden="true" href="#what-meaningful-accountability-requires">#</a></h3>
<p><strong>Meaningful accountability, where measurement actually improves programs, requires:</strong></p>
<p>Appropriate scope. Fewer metrics collected well produce more accountability than many metrics collected poorly. CMS should reduce measurement burden while increasing focus on indicators that matter most.</p>
<p>Capacity-matched expectations. Requirements should reflect state capability. Demanding sophisticated measurement from states lacking sophisticated capacity produces theater.</p>
<p>Learning orientation. States should be rewarded for identifying what does not work, not punished. Honest reporting serves transformation better than optimistic spin.</p>
<p>Verification mechanisms. Self-reported data should be validated through independent sources where possible. Trust but verify.</p>
<p>Consequences proportionate to capacity. States with measurement infrastructure that produce poor results should face different consequences than states lacking measurement infrastructure entirely.</p>
<h2 id="recommendations">Recommendations<a hidden class="anchor" aria-hidden="true" href="#recommendations">#</a></h2>
<h3 id="for-state-agencies">For State Agencies<a hidden class="anchor" aria-hidden="true" href="#for-state-agencies">#</a></h3>
<p><strong>Invest in learning systems, not just compliance systems.</strong> The marginal dollar spent on measurement should improve implementation, not merely satisfy federal requirements. If measurement does not inform decisions, it serves no purpose beyond compliance.</p>
<p><strong>Focus on fewer, more meaningful indicators.</strong> Attempting to track everything produces data on nothing. Identify the three to five indicators most likely to reveal whether transformation is occurring. Track those well.</p>
<p><strong>Build measurement capacity as implementation infrastructure.</strong> Measurement capability is not overhead; it is essential infrastructure for transformation. States that can assess their own progress can adjust their approaches. States that cannot assess progress operate blind.</p>
<p><strong>Connect measurement to decisions through formal processes.</strong> Require that measurement results be reviewed before major decisions. Create accountability for acting on measurement findings, not just for collecting data.</p>
<h3 id="for-cms">For CMS<a hidden class="anchor" aria-hidden="true" href="#for-cms">#</a></h3>
<p><strong>Reduce measurement burden; focus on fewer, more meaningful indicators.</strong> Current requirements assume capacity that most states lack. Streamlining requirements would improve measurement quality by enabling states to focus on fewer metrics.</p>
<p><strong>Differentiate requirements by state capacity.</strong> High-capacity states can produce sophisticated measurement. Low-capacity states cannot. Requiring the same outputs from dramatically different starting points produces compliance theater in low-capacity states.</p>
<p><strong>Emphasize verification over self-reporting.</strong> Where possible, use administrative data that states cannot manipulate rather than self-reported metrics they can construct. Triangulate across data sources to identify discrepancies.</p>
<p><strong>Create learning orientation by rewarding honest failure acknowledgment.</strong> States that identify approaches that did not work and adjust should receive positive recognition, not penalties. The goal is improvement, not the appearance of success.</p>
<p><strong>Invest in state measurement capacity as a legitimate RHTP use.</strong> If measurement is essential for accountability, measurement infrastructure is legitimate investment. States should be encouraged to use RHTP funds to build capacity that enables meaningful measurement.</p>
<h3 id="for-evaluators-and-observers">For Evaluators and Observers<a hidden class="anchor" aria-hidden="true" href="#for-evaluators-and-observers">#</a></h3>
<p><strong>Assess measurement system quality, not just metric availability.</strong> The existence of reported data does not indicate meaningful measurement. Evaluation should examine whether states can actually produce reliable information, not merely whether they submit reports.</p>
<p><strong>Distinguish measurement that informs from measurement that performs.</strong> The same reports can serve learning or compliance depending on organizational context. Evaluation should assess how measurement is used, not merely what measurement exists.</p>
<p><strong>Track measurement burden as implementation factor.</strong> States overwhelmed by measurement requirements may fail for reasons unrelated to their transformation approach. Evaluation should distinguish implementation failures from measurement-induced failures.</p>
<p><strong>Examine gaming and its consequences.</strong> When states optimize for metrics rather than outcomes, evaluation should detect this pattern and assess its implications. Metrics that correlate with gaming susceptibility should receive less interpretive weight.</p>
<h2 id="transition-to-article-5e">Transition to Article 5E<a hidden class="anchor" aria-hidden="true" href="#transition-to-article-5e">#</a></h2>
<p>Performance measurement shapes and is shaped by federal-state relationships. <strong>Article 5E examines the tension between federal mandate and state autonomy</strong> that underlies RHTP&rsquo;s cooperative agreement structure. States that produce sophisticated measurement may receive greater federal flexibility. States that produce compliance theater may receive enhanced oversight. The relationship between measurement capacity and federal trust creates dynamics that affect all aspects of RHTP implementation. The accountability demands examined in this article cannot be understood apart from the federal-state power dynamics that Article 5E addresses.</p>
<h2 id="cross-references">Cross-References<a hidden class="anchor" aria-hidden="true" href="#cross-references">#</a></h2>
<p><strong>2A (RHTP Structure):</strong> Federal measurement requirements and annual re-scoring processes that create accountability context for state measurement.</p>
<p><strong>5A (Lead Agency Structures):</strong> Authority for measurement decisions varies by lead agency designation. Agencies with evaluation capacity can leverage existing infrastructure.</p>
<p><strong>5C (Procurement):</strong> Measurement systems often require procurement. States face similar timeline and compliance challenges for measurement infrastructure that they face for implementation infrastructure.</p>
<p><strong>5E (Federal Relationship):</strong> How measurement quality shapes CMS relationship and federal flexibility. Collaborative relationships may enable measurement burden reduction.</p>
<p><strong>4-Synthesis (Evidence):</strong> Evidence on transformation approach effectiveness depends on state measurement capacity. Weak measurement limits what can be learned from RHTP implementation.</p>
<p><strong>Series 3 State Profiles:</strong> Individual state profiles document measurement approaches and capacity, providing specific examples of patterns described in this article.</p>
<h2 id="sources">Sources<a hidden class="anchor" aria-hidden="true" href="#sources">#</a></h2>
<p>Centers for Medicare and Medicaid Services. &ldquo;CMS Announces $50 Billion in Awards to Strengthen Rural Health in All 50 States.&rdquo; CMS Newsroom, 29 Dec. 2025.</p>
<p>Forvis Mazars. &ldquo;Federal Grant Program Evaluation Part 1: Navigating New Regs.&rdquo; Forvis Mazars, 1 July 2025.</p>
<p>Government Accountability Office. &ldquo;Grants Management: Enhancing Performance Accountability Provisions Could Lead to Better Results.&rdquo; GAO-06-1046, 29 Sept. 2006.</p>
<p>Government Accountability Office. &ldquo;Managing for Results in Government.&rdquo; GAO, 2025.</p>
<p>Government Accountability Office. &ldquo;Performance and Accountability Report, Fiscal Year 2024.&rdquo; GAO-25-900570, 2024.</p>
<p>Heinrich, Carolyn J. &ldquo;Outcomes-Based Performance Management in the Public Sector: Implications for Government Accountability and Effectiveness.&rdquo; Public Administration Review, vol. 62, no. 6, 2002, pp. 712-725.</p>
<p>Moynihan, Donald P. The Dynamics of Performance Management: Constructing Information and Reform. Georgetown University Press, 2008.</p>
<p>National Academy for State Health Policy. &ldquo;State Health Agency Capacity Assessment.&rdquo; NASHP, 2025.</p>
<p>Office of Management and Budget. &ldquo;2 CFR Part 200: Uniform Administrative Requirements, Cost Principles, and Audit Requirements for Federal Awards.&rdquo; OMB, 2024.</p>
<p>Radin, Beryl A. Challenging the Performance Movement: Accountability, Complexity, and Democratic Values. Georgetown University Press, 2006.</p>
<p>U.S. Department of Health and Human Services. &ldquo;45 CFR Part 75: Uniform Administrative Requirements, Cost Principles, and Audit Requirements for HHS Awards.&rdquo; HHS, 2024.</p>
<p><em>RHTP 5.D</em>
<em>Series 5: State Agencies</em>
<em>Rural Health Transformation Project</em>
<em>Version 1: January 2026</em></p>


    </div>
  </div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/rural-health/">Rural-Health</a></li>
      <li><a href="http://localhost:1313/tags/rhtp/">Rhtp</a></li>
      <li><a href="http://localhost:1313/tags/state-policy/">State-Policy</a></li>
      <li><a href="http://localhost:1313/tags/series-5/">Series-5</a></li>
      <li><a href="http://localhost:1313/tags/medicaid/">Medicaid</a></li>
      <li><a href="http://localhost:1313/tags/telehealth/">Telehealth</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/rural-health/series-5/federal-state-relationship-cooperative-federalism-and-its-discontents/">
    <span class="title">« Prev</span>
    <br>
    <span>Federal-State Relationship: Cooperative Federalism and Its Discontents</span>
  </a>
  <a class="next" href="http://localhost:1313/rural-health/series-5/procurement-and-contracting-the-compliance-trap/">
    <span class="title">Next »</span>
    <br>
    <span>Procurement and Contracting: The Compliance Trap</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Performance Measurement: Accountability Theater on x"
            href="https://x.com/intent/tweet/?text=Performance%20Measurement%3a%20Accountability%20Theater&amp;url=http%3a%2f%2flocalhost%3a1313%2frural-health%2fseries-5%2fperformance-measurement-accountability-theater%2f&amp;hashtags=rural-health%2crhtp%2cstate-policy%2cseries-5%2cmedicaid%2ctelehealth">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Performance Measurement: Accountability Theater on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2frural-health%2fseries-5%2fperformance-measurement-accountability-theater%2f&amp;title=Performance%20Measurement%3a%20Accountability%20Theater&amp;summary=Performance%20Measurement%3a%20Accountability%20Theater&amp;source=http%3a%2f%2flocalhost%3a1313%2frural-health%2fseries-5%2fperformance-measurement-accountability-theater%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Syam Adusumilli</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><div class="ggh-footer-branding">
  <a href="https://groundgame.health" target="_blank" rel="noopener">
    <img src="/images/ggh-logo.png" alt="GroundGame.Health" />
  </a>
</div>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
