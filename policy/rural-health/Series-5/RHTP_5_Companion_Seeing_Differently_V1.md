# RHTP 5.Companion
# Seeing Differently: Design Approaches to State Agency Implementation

*Series 5: State Agencies*

The organizational chart shows the Department of Health as lead agency. The consultant recommends better coordination mechanisms. The federal monitor suggests relationship-building investments. The evaluator proposes improved metrics for inter-agency collaboration.

Everyone is solving the wrong problem.

The coordination challenge exists because someone designed a system requiring coordination. The relationship dependency exists because someone designed a system that fails without strong relationships. The measurement gap exists because someone designed requirements exceeding state capacity to document.

**These are design failures, not implementation failures.** Solving them requires different design, not better measurement of dysfunctional design.

The Series 5 Synthesis concluded that leadership, relationships, capacity, and political commitment matter more than structures for implementation success. The natural policy response is to measure these things: develop relationship quality indicators, track leadership attention metrics, assess political commitment scores, document capacity development progress.

This response extends the compliance orientation that Series 5 critiques. It treats measurement as the solution to problems that measurement created. It assumes that tracking alignment produces alignment, that documenting relationships builds relationships, that assessing commitment generates commitment.

It does not. **Measurement changes what it measures.** Relationships that become measured become performed. Leadership attention that becomes tracked becomes theatrical. The authentic phenomena that predict implementation success cannot survive their transformation into metrics.

This companion document takes a different approach. Rather than asking how to improve state agency implementation through better measurement, it asks **how to see state agency implementation differently**. The goal is not better metrics but better design: creating conditions where alignment emerges naturally rather than conditions where alignment must be monitored into existence.

## Part I: The Measurement Trap

### How Measurement Crowds Out Alignment

Series 5 documented performance measurement as accountability theater. States produce reports satisfying requirements without informing decisions. Measurement consumes capacity that could serve communities. Gaming indicators replaces achieving outcomes. The measurement system performs accountability rather than creating it.

The standard response is better measurement. More meaningful indicators. Outcome metrics rather than process metrics. Learning systems rather than compliance systems.

This response assumes measurement solves measurement problems. It does not.

**Measurement changes what it measures.** The act of measuring transforms the phenomenon measured. Relationships subjected to assessment become relationships performed for assessors. When CMS evaluates federal-state relationship quality, states optimize for relationship appearance. The authentic collaboration that produces implementation success differs fundamentally from collaboration performed for evaluators. Both may look similar in documentation. They produce different results.

**Measurement consumes attention.** Every hour documenting coordination is an hour not coordinating. Every meeting about measurement systems is a meeting not about implementation. Every staff position dedicated to reporting is a position not dedicated to service. The overhead of accountability crowds out the substance accountability supposedly ensures.

**Measurement creates adversarial dynamics.** Evaluation inherently judges. Judgment creates defensiveness. Defensiveness undermines trust. Trust enables the authentic relationships that implementation requires. **The measurement relationship is structurally adversarial** even when evaluators intend partnership. States being assessed cannot fully trust assessors. The dynamic is inherent to the relationship, not a function of evaluator intentions.

**Measurement assumes the measurer knows what matters.** Federal indicators reflect federal priorities. State indicators reflect state priorities. Community indicators reflect community priorities. These may align or diverge. The act of selecting indicators embeds assumptions about value that the indicator system cannot question. When federal indicators dominate (as they do in RHTP), federal assumptions about value dominate regardless of whether those assumptions fit local reality.

### What Measurement Cannot Capture

The factors predicting implementation success, according to Series 5, are precisely those measurement degrades.

**Leadership focus** that emerges from genuine priority differs from leadership focus responding to measurement incentives. A governor who cares about rural health because rural communities matter to them behaves differently from a governor performing care because leadership attention metrics require it. The external behavior may look similar. The underlying commitment differs. When pressure mounts and tradeoffs sharpen, genuine commitment persists while performed commitment evaporates. Measurement cannot distinguish them until the moment of pressure reveals the difference.

**Relationship quality** emerging from trust differs from relationship quality performed for assessment. When states know CMS evaluates their federal relationship, they optimize for appearance. Calls get made that would not otherwise occur. Documentation gets produced that overstates collaboration. The measurement system generates evidence of relationships rather than relationships themselves. Authentic collaboration becomes impossible when collaboration is being judged because judgment introduces the adversarial dynamic that collaboration requires transcending.

**Political commitment** accepting genuine cost differs from political commitment accepting measurable cost. Politicians willing to make difficult decisions do so because they believe the outcome justifies the cost. Belief cannot be measured without changing it. Political commitment metrics would generate commitment performance: the appearance of willingness to bear cost without the substance. When the cost actually arrives, performed commitment fails.

**Capacity** enabling implementation differs from capacity satisfying assessment. States can document evaluation infrastructure without having functional evaluation. They can demonstrate procurement processes without executing timely procurement. They can show coordination mechanisms without coordinating. **The documentation and the capacity are different things.** Measurement systems that accept documentation as evidence of capacity systematically overestimate capacity because documentation is easier than function.

### The Design Alternative

Rather than measuring alignment, **design systems where alignment emerges naturally**.

This requires different questions. Not "how do we measure leadership attention?" but "what conditions create genuine leadership attention without measurement?" Not "how do we assess relationship quality?" but "what structures make collaboration rational rather than performed?" Not "how do we track capacity development?" but "what program designs match available capacity rather than requiring capacity that does not exist?"

The alternative is design thinking applied to implementation challenges. Design thinking asks what conditions produce desired outcomes, then creates those conditions. It does not ask how to measure whether desired outcomes occurred, then pressure systems toward measured performance.

**Design creates conditions. Measurement documents conditions.** When conditions are wrong, measurement documents failure without changing it. When conditions are right, measurement is unnecessary because success emerges from the conditions themselves.

The rest of this document applies design thinking to the implementation challenges Series 5 identified. Each section offers an alternative lens: a way of seeing challenges that reveals design solutions invisible to measurement orientation.

## Part II: Alternative Lenses

### Lens 1: The Authority Clarity Lens

**The standard view:** Authority fragmentation creates coordination challenges. Multiple agencies hold pieces of implementation authority. No single entity controls enough to act decisively. The solution is better coordination: mechanisms, processes, relationships that bridge authority gaps. Measure coordination effectiveness to ensure coordination occurs.

**The alternative view:** Authority fragmentation creates coordination requirements. The coordination challenge exists because the system was designed to require coordination. The solution is reducing fragmentation so coordination becomes unnecessary. **The best coordination is no coordination because the system does not require it.**

Every coordination mechanism is a design failure made visible. Someone designed a system where Agency A controls budgets, Agency B controls programs, and Agency C controls Medicaid. This design created the coordination requirement. The coordination mechanisms addressing this requirement are patches on flawed design, not solutions.

**Consolidation over coordination.** Where authority can be consolidated, consolidate it. Michigan's DHHS holds authority that Georgia distributes across DCH and DPH. Michigan requires less coordination because Michigan's design consolidated what Georgia's design fragmented. For Georgia, the solution is not better DCH-DPH coordination but authority consolidation that makes coordination unnecessary. Whether Georgia can achieve consolidation depends on political factors. The design insight remains: coordination requirements are design failures that consolidation eliminates.

**Separation over integration.** Where consolidation is politically impossible, clean separation may outperform messy integration. Distinct domains with clear boundaries require less coordination than overlapping domains with shared authority. If Agency A handles all workforce functions and Agency B handles all facility functions with no overlap, coordination requirements shrink to the interface between domains. The design question becomes: where should domain boundaries fall to minimize coordination requirements?

**Automation over negotiation.** Where decisions repeat, automate them. Every decision reduced to algorithm is a decision removed from coordination dependency. Procurement thresholds allowing direct purchase avoid procurement negotiation. Pre-approved modification authorities avoid modification negotiation. Automatic flexibility triggers avoid flexibility negotiation. The goal is not faster negotiation but eliminated negotiation because the system handles decisions automatically.

**Practical application:** Before investing in coordination mechanisms, ask why this decision requires coordination. Often the answer reveals design choices creating the coordination requirement. The design insight is not "coordinate better" but "redesign so coordination becomes unnecessary."

Georgia could invest in DCH-DPH coordination infrastructure. Or Georgia could consolidate authority so coordination infrastructure becomes unnecessary. The second approach solves the problem; the first manages it. Managing problems is necessary when solving them is impossible. But solving should be attempted before managing is accepted.

### Lens 2: The Incentive Alignment Lens

**The standard view:** State and federal interests diverge. States pursue state priorities; CMS pursues federal priorities. When these conflict, states underperform on federal objectives. The solution is federal oversight ensuring states pursue federal objectives despite state preferences. Measure compliance to ensure states do what CMS requires.

**The alternative view:** State and federal interests diverge because program design creates divergence. The incentive misalignment is not natural but designed. The solution is redesigning programs so state and federal interests naturally align. **When incentives align, compliance is unnecessary because states pursue federal objectives for state reasons.**

When states optimize for metrics rather than outcomes, the problem is not state behavior but program design. The program created metrics misaligned with outcomes. Better metrics do not solve this; they relocate the misalignment. The solution is program design where pursuing state interests automatically pursues federal interests.

**Outcome funding over process funding.** RHTP funds flow for performed processes: activities documented, reports submitted, milestones claimed. This creates incentive to perform processes regardless of outcomes. Alternative design where funds flow for achieved outcomes (rural health improvement measured, access expansion demonstrated, mortality reduction documented) would align state incentives with federal objectives without compliance measurement. States would pursue outcomes because funding depends on outcomes, not because compliance systems pressure outcome pursuit.

**Risk sharing over risk transfer.** Current design transfers implementation risk to states. States must perform or face consequences. CMS bears no risk from program design failures. This misaligns incentives: CMS designs programs without bearing design failure costs; states implement programs bearing all failure costs including design-caused failure. Alternative design sharing risk (federal funds contingent on federal support effectiveness, CMS accountability for design adequacy) would align federal and state incentives around program success rather than program compliance.

**Flexibility as default over flexibility as exception.** Current design requires states to justify deviation from federal standards. The default is federal uniformity; flexibility requires permission. This creates incentive to follow federal templates even when templates fit poorly. Alternative design where flexibility is default (federal justification required for imposing uniformity rather than state justification required for deviation) would align incentives toward state-appropriate implementation.

**Political credit alignment.** Governors seek credit for success. Current design allows CMS to claim credit for investment ("we provided $50 billion") while states bear accountability for outcomes ("the state failed to implement effectively"). This misaligns political incentives: governors gain little from RHTP success because credit flows federally while bearing substantial risk from RHTP failure because blame flows to states. Alternative design enabling gubernatorial credit-claiming for rural health improvement would align political incentives with implementation success.

**Practical application:** Before creating compliance mechanisms, ask why state incentives diverge from federal objectives. Often the answer reveals program design choices creating the divergence. The design insight is not "enforce compliance" but "redesign so compliance becomes unnecessary because state interests align with federal objectives."

The 2030 sustainability cliff illustrates incentive misalignment. States must build sustainable systems knowing funding ends. Governors investing in systems that collapse after they leave office gain little political benefit. Federal design created this sustainability disincentive; federal redesign could solve it. Automatic RHTP extension contingent on outcome achievement would align gubernatorial incentives with sustainability investment without sustainability metrics.

### Lens 3: The Capacity Reality Lens

**The standard view:** States lack capacity to implement sophisticated programs. Rural health offices are understaffed. Evaluation expertise is scarce. Procurement systems are slow. The solution is capacity building: technical assistance, training, staff development, system investment. Measure capacity development to ensure capacity grows.

**The alternative view:** Programs exceed state capacity because programs are designed without capacity constraints. The capacity gap is not a state failure but a program design failure. The solution is designing programs states can actually implement. **Design to capacity rather than designing beyond capacity and hoping capacity catches up.**

Designing programs exceeding implementer capacity guarantees implementation failure. This is obvious in principle but ignored in practice. RHTP requirements assume evaluation capacity, procurement speed, and coordination sophistication that many states demonstrably lack. The design guarantees some states will fail because the design requires capacity those states do not have.

**Design to capacity, not to ambition.** Current RHTP design reflects federal ambition: sophisticated measurement, comprehensive coordination, rapid deployment. Designing instead to actual state capacity (simpler measurement, reduced coordination requirements, realistic timelines) would improve implementation without capacity building. Capacity building takes years. Design simplification takes decisions. States lacking evaluation infrastructure cannot build it fast enough for RHTP's timeline. But RHTP could be redesigned to require evaluation infrastructure states actually have.

**Tiered programs over uniform programs.** One program design cannot fit fifty states with different capacities. Massachusetts has evaluation infrastructure Kansas lacks. California has procurement systems Wyoming cannot match. Uniform requirements guarantee that high-capacity states are under-challenged while low-capacity states are overwhelmed. Tiered design (sophisticated version for states that can implement it, simplified version for states that cannot) matches implementation requirements to implementation ability.

**Capacity as constraint, not variable.** Planning that treats capacity as constraint asks: what can we accomplish with available capacity? Planning that treats capacity as variable asks: how do we build capacity to accomplish our ambition? The first approach implements. The second approach plans to implement while waiting for capacity that may never arrive. RHTP's five-year timeline does not provide time for substantial capacity building. Treating capacity as constraint produces realistic plans; treating capacity as variable produces aspirational plans that fail.

**Outsourcing over building.** States lacking capacity can access capacity rather than build it. Universities have evaluation expertise. Consultants have procurement experience. Other states have implementation knowledge. Program design facilitating capacity access (encouraging partnerships, enabling contracts, connecting peer states) enables implementation without waiting for capacity development.

**Practical application:** Before investing in capacity building, ask why this program requires capacity the state lacks. Often the answer reveals design choices creating the capacity requirement. The design insight is not "build capacity faster" but "redesign so available capacity suffices."

The measurement sophistication RHTP requires exceeds many states' evaluation capacity. Capacity building would take years. Measurement simplification would take decisions. Fewer, simpler metrics that low-capacity states can actually produce would generate better data than complex requirements producing compliance fiction from states that cannot implement them.

### Lens 4: The Relationship Substrate Lens

**The standard view:** Relationships matter for implementation. The Montana vignette shows how strong federal-state relationships enable rapid problem-solving. The Georgia vignette shows how weak relationships produce delays harming communities. The solution is relationship investment: building trust, cultivating partnerships, developing personal connections. Assess relationship quality to ensure relationships are adequate.

**The alternative view:** Relationships matter because program design creates relationship dependency. Implementation depending on relationships depends on factors no design can control. The solution is designing programs that succeed regardless of relationship quality. **Robust design works with bad relationships while performing better with good ones.**

Relationships emerge from personal chemistry, accumulated history, institutional culture, and circumstances that programs cannot mandate. Some project officers and state directors will develop trust. Others will not. Some states enter RHTP with collaborative federal histories. Others enter with adversarial legacies. Program design cannot change these starting points. Program design can determine whether these starting points determine implementation success.

**Robustness over optimization.** Designs requiring good relationships to succeed are fragile. When relationships are strong, they succeed; when relationships are weak, they fail. Robust designs succeed adequately with weak relationships while performing better with strong ones. The design question is not "how do we ensure good relationships" but "how do we succeed regardless of relationship quality."

**Structural alignment over relational alignment.** When structures align interests, relationships matter less. When structures misalign interests, even good relationships face strain. Two agencies with conflicting mandates will struggle to collaborate regardless of personal relationships between directors. Two agencies with aligned mandates will collaborate more easily even without strong personal relationships. Structural alignment creates conditions where relationships can flourish; it also reduces dependence on relationships flourishing.

**Redundancy over dependency.** Single points of relationship failure create fragility. If implementation depends on one project officer relationship, project officer turnover threatens implementation. If implementation depends on one key partnership, partnership deterioration threatens implementation. Redundant relationships (multiple federal contacts, multiple state partnerships, multiple communication channels) survive individual relationship failures.

**Formalization as relationship insurance.** Informal relationships work until they do not. The handshake agreement holds until the parties shake hands with different people. The understanding persists until someone misunderstands. Formalization (MOUs, contracts, documented agreements, written protocols) provides insurance against relationship failure. When relationships work, formalization is unnecessary overhead. When relationships fail, formalization enables continuation despite failure. **The goal is not replacing relationships with formalization but ensuring formalization exists when relationships fail.**

**Practical application:** Before investing in relationship building, ask why implementation depends on this relationship. Often the answer reveals design choices creating the dependency. The design insight is not "build stronger relationships" but "redesign so implementation succeeds regardless of relationship strength."

Montana's rapid response to hospital closure reflected strong relationships. But implementation design could have enabled adequate response regardless of relationship quality. Automatic flexibility for defined circumstances (provider closure meets predefined criteria, modification authority triggers automatically) would remove relationship dependency from flexibility access. States with weak CMS relationships would access the same flexibility as states with strong relationships because flexibility would be structural rather than relational.

### Lens 5: The Political Economy Lens

**The standard view:** Political factors constrain implementation. Governors face electoral pressures. Legislatures control budgets. Provider interests exercise influence. These dynamics limit what agencies can accomplish. The solution is building political support: cultivating champions, creating coalitions, demonstrating benefits. Assess political commitment to ensure adequate political backing.

**The alternative view:** Political constraints exist because program design ignores political economy. Programs requiring political actors to behave against their incentives will fail. The solution is designing programs aligning with political incentives rather than fighting them. **Work with political reality rather than wishing it were different.**

Governors will pursue electoral advantage. Legislators will respond to influential interests. Providers will protect their economic position. These behaviors are not failures of political will but predictable responses to political incentives. Program design ignoring these incentives designs for a political world that does not exist.

**Credit distribution matters.** Politicians support programs generating political credit. RHTP design that enables gubernatorial credit-claiming generates gubernatorial support without requiring governors to transcend political self-interest. Design reserving credit for federal officials or diffusing credit across many actors generates less state political investment. The question is not whether governors should be more public-spirited but how program design can align credit flows with implementation needs.

**Blame avoidance matters more.** Politicians fear programs creating blame risk more than they value programs creating credit opportunity. RHTP design concentrating blame on states (state implementation failed) while diffusing credit (federal investment succeeded, outcomes varied by state) creates political incentive to minimize engagement, not maximize it. Design protecting state officials from blame for design failures (federal accountability for program adequacy, shared responsibility for outcome shortfalls) would enable implementation investment by reducing political risk.

**Interest group alignment matters.** Provider interests dominate health policy in most state capitals. Hospital associations, physician organizations, and health system lobbies exercise substantial political influence. RHTP design threatening provider interests generates provider opposition creating political cost that governors must bear or avoid. Design aligning transformation with provider interests (or at least neutralizing opposition through grandfather provisions, transition support, or alternative value propositions) reduces political constraint on implementation.

**Electoral cycle alignment matters.** Governors facing reelection invest in visible, quick wins. Programs producing visible results within electoral cycles attract gubernatorial attention. Programs requiring long-term investment before visible results attract gubernatorial neglect. RHTP's five-year timeline spans multiple electoral cycles. Design producing early visible wins (Year 1-2 achievements governors can claim) would align electoral incentives with implementation investment. Design where visible results arrive only in Year 4-5 (after many governors have moved on) misaligns electoral timing with program timeline.

**Practical application:** Before building political coalitions, ask why this program faces political opposition or neglect. Often the answer reveals design choices creating the political problem. The design insight is not "overcome political resistance" but "redesign so political incentives support implementation."

The sustainability challenge illustrates political economy failure. RHTP requires states to build systems potentially not surviving beyond 2030. Governors investing in programs that collapse after they leave office gain little political benefit and bear substantial political risk (they built something that failed). Design extending RHTP contingent on outcome achievement would align gubernatorial incentives with sustainability investment by ensuring political benefit from long-term system building.

### Lens 6: The Community Agency Lens

**The standard view:** Community engagement improves implementation. Stakeholder input helps programs fit local needs. Advisory committees surface local knowledge. Public participation builds program legitimacy. The solution is robust engagement processes: stakeholder meetings, public comment, advisory structures. Assess participation to ensure communities are engaged.

**The alternative view:** Community engagement fails because communities lack agency, not voice. Participation without power is theater. The solution is designing programs where communities hold actual authority, not programs where communities provide input that may or may not influence decisions others make. **Transfer authority, not process.**

Advisory committees that provide input ignored teach community members their time is wasted. Stakeholder processes that solicit perspectives without changing decisions produce cynicism undermining future engagement. Public participation that performs inclusion without practicing it generates the appearance of community voice without its substance.

**Authority over input.** Communities deciding implementation details engage differently than communities advising on implementation details. The difference is not participation level but authority distribution. Community members asked "what do you think we should do?" engage differently than community members asked "what have you decided to do?" The first is consultation; the second is governance. Design transferring actual authority produces different engagement than design soliciting input on decisions made elsewhere.

**Resources over process.** Community organizations with resources can act. Community organizations without resources can only advise. Transferring decision authority without transferring resources produces authority without capacity: communities that can decide but cannot implement their decisions. Design directing resources to community control enables community agency that advisory processes cannot create.

**Accountability reversal.** Current design holds states accountable to CMS for community engagement. States must document stakeholder processes, show participation evidence, demonstrate input solicitation. This accountability direction makes communities objects of engagement rather than subjects holding power. Alternative design holding states accountable to communities for implementation quality (community authority to assess state performance, community voice in determining state compliance) would reverse accountability direction and the power dynamics flowing from it.

**Exit over voice.** Communities depending on single providers have voice but not exit. They can complain about services but cannot choose alternatives. Voice without exit is weak. Design creating options (multiple providers, alternative delivery models, competitive service availability) enables community agency that voice processes cannot replace. Communities that can leave have power; communities that can only comment do not.

**Practical application:** Before designing stakeholder processes, ask what authority communities will actually hold. If the answer is "input that may or may not influence decisions," recognize the limitation. The design insight is not "improve participation quality" but "transfer actual authority so participation becomes governance."

RHTP's stakeholder requirements produce advisory committees across fifty states. Almost none transfer actual authority to communities. Community members provide input; state agencies decide. Design requiring community approval for specified decisions (subaward allocations, priority setting, vendor selection) would transfer actual authority rather than creating input processes. Whether such design is politically feasible depends on state context. The design insight remains: participation without authority is theater that design improvements cannot make genuine.

## Part III: Design Principles

The six lenses converge on principles transcending specific domains.

### Principle 1: Reduce Coordination Requirements

Every coordination requirement is a potential failure point. Systems requiring extensive coordination depend on relationships, goodwill, and alignment that may not exist. Design reducing coordination requirements reduces failure modes.

Before creating coordination mechanisms, redesign to eliminate coordination need. Consolidate authority where possible. Separate domains cleanly where consolidation fails. Automate repeated decisions. **The goal is not better coordination but less need to coordinate.**

### Principle 2: Align Incentives Structurally

When incentives misalign, behavior diverges from objectives regardless of measurement. Compliance systems attempting to force aligned behavior despite misaligned incentives face endless resistance. Design aligning incentives produces aligned behavior without compliance overhead.

Before creating compliance mechanisms, redesign to align incentives. Connect funding to outcomes. Share risk between federal and state partners. Distribute credit appropriately. Enable political benefit from implementation success. **The goal is not enforced compliance but unnecessary compliance because incentives align.**

### Principle 3: Design to Capacity

Programs exceeding implementer capacity fail. Programs scaled to capacity succeed. Capacity building takes years; design simplification takes decisions.

Before investing in capacity building, redesign to match available capacity. Simplify requirements. Create tiered program designs. Accept that different states will implement different versions. Enable capacity access through partnership rather than capacity creation through development. **The goal is not expanded capacity but reduced capacity requirements.**

### Principle 4: Reduce Relationship Dependency

Relationships emerge from factors design cannot control. Relationship-dependent implementation gambles on relationships forming. Robust design succeeds regardless of relationship quality.

Before investing in relationship building, redesign to reduce relationship dependency. Create structural alignment making relationships beneficial but not required. Build redundancy surviving individual relationship failures. Formalize as insurance when relationships fail. **The goal is not stronger relationships but reduced dependence on relationship strength.**

### Principle 5: Work With Political Incentives

Programs requiring political actors to behave against their incentives will fail. Programs aligned with political incentives succeed without political cultivation.

Before building political support, redesign to align with political incentives. Enable credit-claiming. Protect against blame. Align with provider interests where possible. Produce visible results within electoral cycles. **The goal is not overcoming political resistance but eliminating reasons for resistance.**

### Principle 6: Transfer Authority, Not Process

Community engagement without authority is theater. Communities with actual decision authority engage differently than communities invited to advise.

Before designing participation processes, determine what authority communities will actually hold. Transfer genuine authority where possible. Direct resources alongside authority. Create accountability to communities, not just accountability for engaging communities. **The goal is not better participation but actual power.**

## Part IV: What Cannot Be Redesigned

Design thinking has limits. Some constraints are fixed.

**Constitutional federalism** distributes authority between federal and state governments in ways RHTP cannot change. CMS can condition funding but cannot command state action. States retain implementation discretion that federal design cannot override. Design must work within this distribution.

**State political systems** operate according to incentives design can align with but cannot eliminate. Governors will seek electoral advantage. Legislatures will respond to constituent interests. Providers will protect economic position. Design that ignores these realities fails; design that works with them succeeds.

**RHTP statutory structure** fixes the five-year timeline, funding allocation, and basic program parameters. Design improvements must occur within existing statutory framework. Fundamental restructuring would require legislative action beyond RHTP's administrative scope.

**Prior history** shapes the context within which design operates. States enter RHTP with accumulated capacity, established relationships, and embedded political dynamics. Design cannot erase history. It can create conditions where history matters less.

**Leadership and commitment** cannot be designed into existence. Design can create conditions favorable to leadership attention and political commitment. It cannot guarantee that attention materializes or commitment develops. The factors mattering most for implementation are precisely those design cannot control.

## Part V: The Honest Assessment

### What Design Can Accomplish

Better design can reduce coordination failures by reducing coordination requirements. It can align behavior with objectives by aligning incentives structurally. It can enable implementation by matching requirements to capacity. It can reduce fragility by reducing relationship dependency. It can generate political support by aligning with political incentives. It can produce genuine engagement by transferring genuine authority.

These improvements matter. They create conditions where success becomes more likely because the system is designed for success rather than designed to be monitored toward success. They do not require measurement to achieve. They work by changing conditions rather than documenting conditions.

### What Design Cannot Accomplish

Design cannot create leadership attention where leadership does not care. It cannot build trust where history has destroyed it. It cannot generate political commitment where political will is absent. It cannot produce capacity where resources are unavailable. It cannot transfer authority where power holders refuse to relinquish control.

**The factors mattering most for implementation are precisely those design cannot control.** This is the honest conclusion of Series 5 carried forward. Better design improves the context within which leadership, relationships, and political commitment operate. It cannot substitute for their absence.

States with strong leadership, functional relationships, substantial capacity, and political commitment will succeed with almost any reasonable design. States lacking these factors will struggle regardless of design optimization. Design matters at the margins: for states in the middle, where design quality could shift implementation toward success or failure.

### The Gap Between Documentation and Delivery

Rosa brings groceries from her own kitchen because the system is not designed for Maria to receive food. The navigation infrastructure documents food insecurity. The referral system transmits the documentation. The case management platform tracks the referral. The performance measurement system reports completion rates.

None of this delivers food.

Better metrics on food insecurity referrals will not change this. Better measurement of navigation effectiveness will not change this. More sophisticated evaluation of referral completion will not change this.

Different design might. Systems where food reaches Maria because the system is built to deliver food, not systems where Maria's food insecurity is documented and referred to services that do not exist. Systems designed around the outcome (Maria eats) rather than the process (Maria's need is documented).

**The gap between documentation and delivery is a design problem.** Measurement orientation widens the gap by investing in documentation rather than delivery. Design orientation closes the gap by building systems that deliver rather than systems that document.

Series 5 examined state agencies as implementers. This companion has offered lenses for seeing implementation challenges as design problems rather than measurement problems. The lenses do not solve all problems. Some constraints will not yield to design. Some gaps will not close regardless of how cleverly systems are structured.

But seeing differently is the first step toward building differently. States that see coordination requirements as design failures may consolidate authority rather than investing in coordination mechanisms. States that see capacity gaps as program design failures may simplify requirements rather than waiting for capacity that will not arrive. States that see political constraints as incentive misalignments may redesign for political economy rather than fighting political resistance.

Whether they will see differently depends on factors this document cannot control. The lenses are offered. Whether anyone looks through them remains to be seen.

## Cross-References

**5 Synthesis (Which State Agency Structures Support Transformation?):** Analytical foundation establishing that structures matter less than leadership, relationships, capacity, and political commitment

**5A (Lead Agency Structures):** Authority gap analysis informing the Authority Clarity lens

**5B (Stakeholder Coordination):** Coordination theater analysis informing the Community Agency lens

**5C (Procurement and Contracting):** Process compliance analysis informing multiple lenses

**5D (Performance Measurement):** Measurement critique underlying Part I argument

**5E (Federal-State Relationship):** Relationship dependency analysis informing the Relationship Substrate lens

**4 Synthesis (What We Know and What We Don't):** Evidence limitations context

**4 Companion A (Better Optimization):** Parallel practical improvements within existing paradigm

**4 Companion B (Beyond Optimization):** Paradigm shifts transcending optimization entirely

## Sources

Bardach, Eugene. *A Practical Guide for Policy Analysis: The Eightfold Path to More Effective Problem Solving*. CQ Press, 4th ed., 2012.

Bovens, Mark, et al. *The Oxford Handbook of Public Accountability*. Oxford University Press, 2014.

Dorst, Kees. "The Core of 'Design Thinking' and Its Application." *Design Studies*, vol. 32, no. 6, 2011, pp. 521-532.

Hood, Christopher. "A Public Management for All Seasons?" *Public Administration*, vol. 69, no. 1, 1991, pp. 3-19.

Kettl, Donald F. *The Transformation of Governance: Public Administration for the Twenty-First Century*. Johns Hopkins University Press, 2002.

Moynihan, Donald P. *The Dynamics of Performance Management: Constructing Information and Reform*. Georgetown University Press, 2008.

Ostrom, Elinor. *Governing the Commons: The Evolution of Institutions for Collective Action*. Cambridge University Press, 1990.

Pressman, Jeffrey L., and Aaron Wildavsky. *Implementation: How Great Expectations in Washington Are Dashed in Oakland*. University of California Press, 3rd ed., 1984.

Scott, James C. *Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed*. Yale University Press, 1998.

Simon, Herbert A. *The Sciences of the Artificial*. MIT Press, 3rd ed., 1996.

Weick, Karl E. "Educational Organizations as Loosely Coupled Systems." *Administrative Science Quarterly*, vol. 21, no. 1, 1976, pp. 1-19.

*Series 5: State Agencies*
*Rural Health Transformation Project*
*Version 1: January 2026*
