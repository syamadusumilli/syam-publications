---
title: "Executive Summary: Performance Measurement: Accountability Theater"
date: 2026-05-08
author: "Syam Adusumilli"
summary: "Executive summary of Performance Measurement: Accountability Theater"
tags: ["rural-health", "rhtp", "state-policy", "series-5", "summary"]
showtoc: false
ShowReadingTime: true
ShowBreadCrumbs: true
params:
  is_summary: true
  full_article_url: "../performance-measurement-accountability-theater/"
  article_id: "RHTP-5D-Summary"
  collection: "rural-health"
weight: 999
build:
  list: never
---

# Performance Measurement: When Accountability Becomes Theater

Performance measurement in the Rural Health Transformation Program rests on unassailable logic. Taxpayers deserve evidence that $50 billion in federal investment produces results. The Centers for Medicare and Medicaid Services requires reporting to ensure states implement as promised. States need data to identify what works and adjust what does not. Yet in practice, measurement often becomes theater rather than learning. **States with limited capacity spend resources producing reports that no one reads. States with sophisticated systems may game metrics rather than improve outcomes.** The burden falls hardest on the least-resourced states, consuming energy that could fund services. Meaningful accountability, where measurement actually improves programs, remains rare.

This fundamental tension between accountability demands and capacity realities pervades RHTP implementation. CMS prescribes measurement requirements that assume capabilities many states lack. States produce compliant reports that satisfy federal oversight without generating useful information. The gap between required and meaningful measurement reveals how accountability systems can undermine the very outcomes they purport to measure, creating a performance in which all parties participate while recognizing its limitations.

The case for rigorous measurement rests on five pillars. **Public funds require accountability because federal grants represent taxpayer dollars transferred to state governments for specified purposes.** Without measurement, there is no way to verify that funds achieved their purposes. States could claim success while delivering nothing. What gets measured gets managed reflects real organizational dynamics: activities that are tracked receive attention while activities that are not tracked get neglected. Learning requires data because transformation involves trying approaches whose effectiveness is uncertain. Without measurement, states cannot distinguish approaches that work from approaches that fail. Peer comparison enables improvement through standardized measurement across states that enables identification of high performers whose practices can inform others. Federal credibility depends on demonstrated results because Congressional support for continued funding depends on evidence that investment produces outcomes.

The case for measurement restraint rests on equally compelling foundations. **Capacity varies dramatically across states.** Some have sophisticated data infrastructure, experienced evaluation staff, and measurement systems developed through prior federal programs. Others have one part-time employee managing RHTP data alongside other responsibilities. Requiring sophisticated measurement from under-resourced agencies produces compliance exercises, not useful information. Measurement burden diverts implementation resources because staff hours spent collecting data and writing reports are staff hours not spent implementing programs. Process metrics substitute for outcome metrics because outcomes are hard to measure, take years to materialize, and face contested attribution. States measure what they can rather than what matters: activities conducted, trainings delivered, technologies deployed. Gaming corrupts metrics because when measurement has consequences, organizations optimize for metrics rather than outcomes. **Low-capacity states produce low-quality data regardless of requirements** because demanding sophisticated measurement from agencies that lack measurement capability does not create capability; it creates reports that satisfy formal requirements while containing unreliable information.

Evidence that would resolve this tension largely does not exist. Rigorous research comparing program outcomes under different measurement regimes remains absent. Do programs with more extensive measurement requirements achieve better outcomes than programs with lighter requirements? Does measurement burden correlate with implementation success or failure? These questions remain unanswered. Comparative assessment of state reporting quality would clarify the capacity distribution: how many states produce reports that enable meaningful learning versus how many produce compliance documents with little analytical value. Attribution methodology remains contested because even if states collect accurate data on health outcomes, attributing changes to RHTP investment rather than other factors requires analytical sophistication that few states possess.

**The tension cannot be fully resolved because accountability and capacity exist in genuine conflict.** Requiring measurement from states that lack capacity produces low-quality data. Exempting low-capacity states from measurement requirements eliminates accountability for large portions of federal investment. Neither approach is satisfactory. The political economy of measurement reinforces compliance orientation because federal officials face criticism for programs that lack accountability mechanisms while state officials face consequences for failing to submit required reports. Both sets of officials are rewarded for the appearance of accountability, regardless of whether accountability mechanisms improve outcomes. Learning and accountability serve different purposes that sometimes conflict: learning requires honest acknowledgment of failure while accountability systems penalize failure acknowledgment. States that truthfully report that an approach did not work risk funding reductions, enhanced monitoring, and political criticism.

CMS performance measurement requirements specify extensive reporting obligations. States must submit quarterly progress reports documenting activity against plan milestones, financial expenditures, emerging challenges, and course corrections. Annual performance reviews assess whether states meet stated objectives and maintain policy alignment. Poor performance triggers enhanced monitoring, technical assistance requirements, or funding adjustments. **Standard metrics span process (number of telehealth consultations provided, community health workers trained), outputs (technology platforms deployed, training programs established), and outcomes (emergency department utilization, preventable hospitalizations, maternal mortality).** The Office of Rural Health Transformation reviews performance annually, with states that underperform facing consequences including funding reductions or cooperative agreement termination.

Federal requirements intensify tensions by assuming data infrastructure that many states lack. Calculating emergency department utilization rates for rural populations requires linked claims data, geographic identifiers, and analytical capability. States without all-payer claims databases or geographic information systems cannot produce these calculations with the precision federal templates assume. Quarterly reporting timelines compress data collection windows into periods too short for meaningful quality review. Standardized templates obscure important variation because state circumstances differ dramatically: a 15 percent increase in telehealth utilization means something different in Alaska, where telehealth is essential, than in New Jersey, where alternative access exists.

The gap between required and feasible measurement varies dramatically by state. High-capacity states (approximately 10 to 12 states) have robust data infrastructure from prior federal programs, experienced evaluation staff, and leadership that values data-informed decision making. These states can meet CMS requirements and potentially use measurement for genuine learning. Moderate-capacity states (approximately 15 to 20 states) have basic data systems and some analytical staff but lack the sophistication CMS requirements assume. They can produce compliant reports but struggle to use measurement for program improvement. Low-capacity states (approximately 10 to 15 states) have minimal data infrastructure and no dedicated evaluation staff. They produce reports primarily through heroic individual effort, often by staff for whom RHTP reporting is one of many responsibilities. **Very-low-capacity states (approximately 5 to 8 states) lack the basic prerequisites for meaningful measurement.** CMS requirements do not adjust for this variation. All states receive the same templates, the same deadlines, and the same expectations.

States have adopted varied approaches to performance measurement reflecting different assessments of compliance burden, learning value, and capacity constraints. The minimal compliance approach collects only what CMS requires, investing no additional resources in measurement beyond federal mandates. Several smaller states with limited administrative capacity have adopted this approach, recognizing that their resources cannot support sophisticated measurement while simultaneously implementing programs. They prioritize implementation over documentation, accepting that their reports will lack analytical depth. The approach has clear advantages in that resources flow to services rather than measurement, but significant limitations in that states learn nothing from their own experience and operate blind.

The administrative data approach leverages existing all-payer claims databases and robust Medicaid data systems as the primary measurement source. Ohio, Pennsylvania, and Florida exemplify this approach, using their existing data infrastructure to analyze claims data that reveal utilization patterns without requiring new data collection. **The approach works well for utilization metrics but struggles with patient experience, access to care, and outcome measures that claims data cannot capture.** A patient who cannot get an appointment generates no claim. Gaming risk is moderate because providers can influence claims through coding practices.

States with sophisticated data infrastructure have implemented real-time monitoring dashboards providing rapid feedback on key indicators. California's RHTP monitoring system integrates data from Medicaid claims, hospital discharge data, vital statistics, provider surveys, and subawardee reports. North Carolina similarly invested in monitoring infrastructure, building on systems developed for Medicaid transformation. The approach requires substantial investment in dashboard development, data integration, and ongoing maintenance. Learning value is high when capacity exists because rapid feedback enables course correction. States identify implementation challenges while adjustments remain possible rather than discovering problems after years of ineffective operation.

Some states incorporate community-defined measurement alongside federal metrics. Community advisory bodies identify what outcomes matter most to rural residents. New Mexico's RHTP measurement includes indicators developed through tribal consultation: traditional healing access, cultural competency of services, community health worker engagement in tribal communities. These metrics do not appear on CMS templates but matter deeply to communities served. **The approach recognizes that federal metrics may miss what communities value.** Gaming risk is potentially lower because community members observe their own communities and cannot be fooled by inflated metrics.

Prior infrastructure investment largely determines current measurement options. States that invested in data systems during ACA implementation, Medicaid expansion, or previous federal grants have capacity that states lacking such investment cannot quickly develop. Measurement approach reflects accumulated capability more than current strategic choice. Political context shapes measurement investment: governors and legislators who value evidence-based policy support measurement infrastructure while those skeptical of government programs may resist data collection as bureaucratic overhead.

Two states illustrate how identical measurement requirements produce radically different realities. State A has a robust Office of Health Analytics with 12 full-time evaluation staff developed through a decade of federal grants requiring sophisticated measurement. Staff have graduate training in epidemiology, health services research, and biostatistics. The state maintains an all-payer claims database, integrated vital