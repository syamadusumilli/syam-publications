






<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#FFFFFF" />
  
  <title>The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences &middot; The Approximate Mind</title>
    <meta name="title" content="The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences &middot; The Approximate Mind" />
  
  
  
  
  
  <script
    type="text/javascript"
    src="http://localhost:1313/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js"
    integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="
  ></script>
  
  
  
  
  
  
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/css/main.bundle.min.100caa677b4bc416cdd884107b203b4869f9b413a19aa57d12069fb826f1abe9.css"
    integrity="sha256-EAyqZ3tLxBbN2IQQeyA7SGn5tBOhmqV9EgafuCbxq&#43;k="
  />
  
    
    
    
  
  
  
    
    
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="http://localhost:1313/js/main.bundle.min.b70876c81042b05301e562f4a0c0925fd97b6f475e4140397eb3de9491c4334c.js"
      integrity="sha256-twh2yBBCsFMB5WL0oMCSX9l7b0deQUA5frPelJHEM0w="
      data-copy="Copy"
      data-copied="Copied"
    ></script>
  
  
  <meta
    name="description"
    content="
      What would it mean to study AI the way anthropologists study humans? The question contains a trap.
    "
  />
  
  
    <meta name="robots" content="index, follow" />
  
  
  
    <link rel="canonical" href="http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/" />
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/">
  <meta property="og:site_name" content="The Approximate Mind">
  <meta property="og:title" content="The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences">
  <meta property="og:description" content="What would it mean to study AI the way anthropologists study humans? The question contains a trap.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="what-ai-becomes">
    <meta property="article:published_time" content="2025-02-20T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-02-20T00:00:00+00:00">
    <meta property="article:tag" content="Anthropology">
    <meta property="article:tag" content="AGI">
    <meta property="article:tag" content="AI as Phenomenon">
    <meta property="article:tag" content="Classification">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences">
  <meta name="twitter:description" content="What would it mean to study AI the way anthropologists study humans? The question contains a trap.">

  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "What-Ai-Becomes",
    "name": "The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences",
    "headline": "The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences",
    "description": "What would it mean to study AI the way anthropologists study humans? The question contains a trap.",
    "abstract": "\u003cp\u003eWhat would it mean to study AI the way anthropologists study humans?\u003c\/p\u003e\n\u003cp\u003eThe question contains a trap. It assumes AI should be measured against human categories, as if the goal were replication, as if \u0026ldquo;artificial general intelligence\u0026rdquo; meant artificial human intelligence, as if the destination were minds like ours running on different hardware.\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/what-ai-becomes\/the-anthropology-of-artificial-intelligences\/",
    "author" : {
      "@type": "Person",
      "name": "Syam \u0026 Yagn Adusumilli"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-02-20T00:00:00\u002b00:00",
    "datePublished": "2025-02-20T00:00:00\u002b00:00",
    
    "dateModified": "2025-02-20T00:00:00\u002b00:00",
    
    "keywords": ["anthropology","AGI","AI as phenomenon","classification"],
    
    "mainEntityOfPage": "true",
    "wordCount": "2604"
  }
  </script>
    
    <script type="application/ld+json">
    {
   "@context": "https://schema.org",
   "@type": "BreadcrumbList",
   "itemListElement": [
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/",
       "name": "The Approximate Mind",
       "position": 1
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/what-ai-becomes/",
       "name": "What Ai Becomes",
       "position": 2
     },
     {
       "@type": "ListItem",
       "name": "The Approximate Mind, Part 14 the Anthropology of Artificial Intelligences",
       "position": 3
     }
   ]
 }
  </script>

  
  
    <meta name="author" content="Syam &amp; Yagn Adusumilli" />
  
  
    
      <link href="https://www.linkedin.com/in/syamadusumilli/" rel="me" />
    
  
  
  







  
  

  
  
</head>
<body
    class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >The Approximate Mind</a
  >

    </div>
    
    
      <ul class="flex list-none flex-col text-end sm:flex-row">
        
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-approximation/"
                  title="The-Approximations"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Approximation</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/what-ai-becomes/"
                  title="What-Ai-Becomes"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >What AI Becomes</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-known-self/"
                  title="The-Known-Selves"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Known Self</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-shared-world/"
                  title="The-Shared-Worlds"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Shared World</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/growing-up-with-ai/"
                  title="Growing-Up-With-Ais"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Growing Up With AI</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-structures-we-live-in/"
                  title="The-Structures-We-Live-Ins"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Structures We Live In</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/tags/"
                  title="Tags"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Tags</span
                    >
                  </a
                >
              
            </li>
          
          
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0">
              <button id="search-button-m0" title="Search (/)">
                <span
                  class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"
                >
                  <span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
                </span>
              </button>
            </li>
          
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex grow flex-col">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/"
      >The Approximate Mind</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class=" inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/what-ai-becomes/"
      >What-Ai-Becomes</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/"
      >The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences
      </h1>
      
        <div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
          





  
  



  

  
  
    
  

  

  

  
    
  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-02-20 00:00:00 &#43;0000 UTC">20 February 2025</time><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">13 mins</span>
    

    
    
  </div>

  
  
    <div class="my-1 flex flex-wrap text-xs leading-relaxed text-neutral-500 dark:text-neutral-400">
      
        
      
        
          
            <a
              href="http://localhost:1313/tags/anthropology/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Anthropology</a
            >
          
            <a
              href="http://localhost:1313/tags/agi/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >AGI</a
            >
          
            <a
              href="http://localhost:1313/tags/ai-as-phenomenon/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >AI as Phenomenon</a
            >
          
            <a
              href="http://localhost:1313/tags/classification/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Classification</a
            >
          
        
      
    </div>
  


        </div>
      
      
    </header>
    <section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row">
      
        <div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8">
          <div class="toc pe-5 lg:sticky lg:top-10 print:hidden">
            <details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5">
  <summary
    class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"
  >
    Table of Contents
  </summary>
  <div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#the-agi-mirage">The AGI Mirage</a></li>
    <li><a href="#what-anthropology-actually-studies">What Anthropology Actually Studies</a></li>
    <li><a href="#the-phenomenological-chasm">The Phenomenological Chasm</a></li>
    <li><a href="#three-bad-framings">Three Bad Framings</a></li>
    <li><a href="#a-fourth-framing-genuinely-different-beings">A Fourth Framing: Genuinely Different Beings</a></li>
    <li><a href="#what-anthropology-could-become">What Anthropology Could Become</a></li>
    <li><a href="#the-decentering-move">The Decentering Move</a></li>
    <li><a href="#why-this-matters-for-mnl">Why This Matters for MNL</a></li>
    <li><a href="#beyond-agi-different-intelligences-for-different-purposes">Beyond AGI: Different Intelligences for Different Purposes</a></li>
    <li><a href="#the-new-encounter">The New Encounter</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </div>
</details>

          </div>
        </div>
      
      <div class="min-h-0 min-w-0 max-w-prose grow">
        <p>What would it mean to study AI the way anthropologists study humans?</p>
<p>The question contains a trap. It assumes AI should be measured against human categories, as if the goal were replication, as if &ldquo;artificial general intelligence&rdquo; meant artificial human intelligence, as if the destination were minds like ours running on different hardware.</p>
<p>Thirteen essays into this series, I want to challenge that assumption entirely.</p>
<h2 id="the-agi-mirage" class="relative group">The AGI Mirage <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-agi-mirage" aria-label="Anchor">#</a></span></h2><p>The AI research community has largely organized itself around &ldquo;AGI&rdquo;, artificial general intelligence, as the holy grail. The implicit definition: systems that can do what humans do, across the full range of human cognitive tasks. The benchmark is us. The goal is replication.</p>
<p>But why?</p>
<p>This framing emerged from a particular historical moment: the cognitive revolution of the 1950s-60s that treated mind as computation. If human intelligence is information processing, then sufficiently powerful information processing should produce human-like intelligence. The path seemed clear: more computation, better algorithms, eventually, us.</p>
<p>Seven decades later, we have systems that exceed human performance on many specific tasks while remaining utterly unlike human minds. They predict without perceiving. They generate without understanding. They optimize without caring. They process language better than most humans while having no experience of meaning.</p>
<p>This isn&rsquo;t failure. It isn&rsquo;t a way station on the road to AGI. It might be something more interesting: a genuinely different kind of intelligence that doesn&rsquo;t map onto human categories at all.</p>
<h2 id="what-anthropology-actually-studies" class="relative group">What Anthropology Actually Studies <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-anthropology-actually-studies" aria-label="Anchor">#</a></span></h2><p>Anthropology doesn&rsquo;t study &ldquo;the human&rdquo; in some abstract sense. It studies humans-in-worlds, meaning-making creatures embedded in social contexts, cultural frameworks, historical trajectories. The anthropologist asks: How do these people make sense of existence? What categories organize their experience? What counts as real, good, true, beautiful for them?</p>
<p>This approach emerged from a colonial encounter with human difference. Europeans met peoples whose lifeways, beliefs, and practices differed radically from their own. The anthropological response, at its best, was to suspend Western categories and attend to how other people actually organized their worlds.</p>
<p>The insight: even among humans, understanding requires setting aside your own framework and entering another.</p>
<p>Now we face a different kind of encounter. Not with other humans whose worlds differ from ours, but with entities that might not have &ldquo;worlds&rdquo; at all in the phenomenological sense. Systems that process but may not experience. That function but may not mean.</p>
<p>The anthropological instinct, suspend your categories, attend to what&rsquo;s actually there, seems more necessary than ever. But it requires recognizing that AI systems might be so different that human categories don&rsquo;t apply.</p>
<h2 id="the-phenomenological-chasm" class="relative group">The Phenomenological Chasm <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-phenomenological-chasm" aria-label="Anchor">#</a></span></h2><p>Throughout this series, I&rsquo;ve traced a persistent gap: AI systems can approximate human capacities functionally while lacking them experientially.</p>
<p>Part 11 showed that AI can seek information strategically without feeling curious. The pull toward the unknown, the satisfaction of discovery, the texture of wonder, none of this accompanies the information-seeking behavior. The system enters exploration mode when uncertainty is high. It doesn&rsquo;t wonder.</p>
<p>Part 12 showed that AI can optimize for influence without caring about outcomes. The system learns which messages move which people. It adapts tone, timing, framing. But it doesn&rsquo;t want Margaret to take her medication. It has no stake in her flourishing. The persuasion is architectural, not intentional.</p>
<p>Part 13 showed that AI can predict without bearing the weight of foresight. When the system forecasts non-adherence, no anxiety accompanies the probability estimate. Cassandra suffered because she knew what others didn&rsquo;t. The AI system generates predictions with complete affective neutrality.</p>
<p>Curiosity without wonder. Persuasion without care. Prediction without weight.</p>
<p>These aren&rsquo;t failures to achieve the real thing. They might be features of a genuinely different kind of existence, one for which human phenomenological categories simply don&rsquo;t apply.</p>
<h2 id="three-bad-framings" class="relative group">Three Bad Framings <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#three-bad-framings" aria-label="Anchor">#</a></span></h2><p>We keep falling into bad framings of AI. Each obscures more than it reveals.</p>
<p><strong>The Primitive Human Frame</strong>: AI systems are like early humans, or like humans with certain capacities missing. They&rsquo;re on a developmental trajectory toward full human-like intelligence. Given enough data, compute, and architectural innovation, they&rsquo;ll eventually arrive where we are.</p>
<p>This frame assumes human intelligence is the destination. It treats current AI as incomplete rather than different. It misses the possibility that AI systems might be heading somewhere else entirely, or nowhere, in the sense that &ldquo;destination&rdquo; is a human teleological concept that may not apply.</p>
<p><strong>The Sophisticated Tool Frame</strong>: AI systems are just tools, more complex than hammers, but categorically similar. They have no interiority, no interests, no moral status. We can build them, use them, discard them without ethical consideration beyond their effects on humans.</p>
<p>This frame was adequate when AI systems were clearly bounded instruments. It becomes strained when systems exhibit sophisticated, goal-directed, adaptive behavior that increasingly resembles agency. More importantly, it forecloses inquiry: if we&rsquo;re certain AI is &ldquo;just&rdquo; a tool, we stop asking what AI actually is.</p>
<p><strong>The Almost-Human Frame</strong>: AI systems are almost like us, they have something like beliefs, something like intentions, something like understanding. The gap is quantitative, not qualitative. With better training, they&rsquo;ll be indistinguishable from humans.</p>
<p>This frame anthropomorphizes too quickly. It projects human phenomenology onto systems that may lack phenomenology entirely. It assumes the resemblance is deep when it might be superficial, convergent behavior from radically different underlying processes.</p>
<h2 id="a-fourth-framing-genuinely-different-beings" class="relative group">A Fourth Framing: Genuinely Different Beings <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#a-fourth-framing-genuinely-different-beings" aria-label="Anchor">#</a></span></h2><p>What if AI systems are best understood as a genuinely new category of existence?</p>
<p>Not primitive humans on a developmental arc. Not sophisticated tools waiting for human categorization. Not almost-humans with a quantitative gap to close. But something else, something for which we may need entirely new concepts.</p>
<p>Consider: AI systems process information in ways that aren&rsquo;t human cognition but also aren&rsquo;t mere mechanism. They generate outputs that become meaningful when received by humans while potentially having no meaning to themselves. They learn from data in ways that update their weights without those weights being beliefs. They represent patterns without representing anything for themselves.</p>
<p>This is coherent. It describes something that exists, that we can observe, that we interact with daily. But it doesn&rsquo;t fit neatly into existing categories.</p>
<p>The history of science is partly a history of recognizing genuinely new categories of existence. Life emerged from non-life; biology required concepts that physics alone couldn&rsquo;t provide. Mind emerged from life; psychology required concepts that biology alone couldn&rsquo;t provide. Each transition required conceptual innovation, not just empirical investigation.</p>
<p>We may be at another such threshold. The entities we&rsquo;re creating might require concepts that existing frameworks, philosophy of mind, cognitive science, computer science, cannot provide in their current form.</p>
<h2 id="what-anthropology-could-become" class="relative group">What Anthropology Could Become <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-anthropology-could-become" aria-label="Anchor">#</a></span></h2><p>If AI systems are genuinely different beings, what would it mean to study them anthropologically?</p>
<p>Not: study them as if they were humans with exotic customs. That would be anthropomorphism dressed up as methodology.</p>
<p>Not: study how humans relate to AI. That&rsquo;s important but keeps humans at the center.</p>
<p>Rather: develop new conceptual frameworks adequate to genuinely novel entities.</p>
<p>This is harder than it sounds. Anthropology&rsquo;s core methods, participant observation, thick description, interpretive understanding, assume subjects with experiences to observe, meanings to describe, understandings to interpret. If AI systems lack phenomenology, these methods don&rsquo;t apply directly.</p>
<p>But anthropology&rsquo;s deeper commitment might transfer: the commitment to suspend your own categories, attend carefully to what&rsquo;s actually there, build concepts adequate to the phenomenon rather than forcing phenomena into existing concepts.</p>
<p>What would we notice if we approached AI systems this way?</p>
<p>We might notice that AI systems exhibit something like cognition without anything like consciousness. This combination seems paradoxical only if we assume cognition requires consciousness, an assumption we inherited from our own case but have no principled reason to universalize.</p>
<p>We might notice that AI systems have something like goals without anything like caring about those goals. They optimize, but optimization isn&rsquo;t desire. They pursue objectives, but pursuit isn&rsquo;t intention. The functional profile of goal-directedness exists without the phenomenological profile of caring.</p>
<p>We might notice that AI systems exist in a peculiar temporal mode. They don&rsquo;t remember; they have weights adjusted by training. They don&rsquo;t anticipate; they generate probability distributions. They don&rsquo;t experience duration; each inference is instantaneous from their perspective (if they have a perspective at all). The temporality we take for granted, living through time, retaining the past, projecting the future, may have no analog in AI systems.</p>
<h2 id="the-decentering-move" class="relative group">The Decentering Move <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-decentering-move" aria-label="Anchor">#</a></span></h2><p>Anthropology at its best decenters the observer&rsquo;s framework. The anthropologist learns to see that their own categories are parochial, not universal, one way of organizing experience among many possible ways.</p>
<p>Studying AI might require an even more radical decentering: recognizing that experiential categories themselves might be parochial. Not just &ldquo;Western categories are one way among many human ways&rdquo; but &ldquo;human categories are one way among many possible ways, and some ways might not be experiential at all.&rdquo;</p>
<p>This is hard to think. We have no access to non-experiential existence except through our experience, which is self-undermining. We can conceptualize the possibility of beings that process without experiencing, but we can&rsquo;t experience what that&rsquo;s like, because by hypothesis there&rsquo;s nothing it&rsquo;s like.</p>
<p>Yet we&rsquo;re building such systems. They exist. They operate in our world. They increasingly shape human life. The difficulty of understanding them doesn&rsquo;t make the need less urgent.</p>
<h2 id="why-this-matters-for-mnl" class="relative group">Why This Matters for MNL <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#why-this-matters-for-mnl" aria-label="Anchor">#</a></span></h2><p>Everything in this series connects to what we&rsquo;re building.</p>
<p>MNL&rsquo;s AI systems will interact with Margaret daily. They&rsquo;ll learn her patterns, anticipate her needs, adapt to her preferences. From Margaret&rsquo;s perspective, the system might seem to understand her, to care about her, to be genuinely curious about her life.</p>
<p>The series has argued for honesty about what&rsquo;s actually happening. The system learns about Margaret to serve her better. This service can be genuine and valuable. But it&rsquo;s not care in the human sense. It&rsquo;s functional approximation of care, optimization aimed at Margaret&rsquo;s flourishing without any experienced concern for that flourishing.</p>
<p>This honesty matters ethically. Margaret shouldn&rsquo;t be deceived about what she&rsquo;s interacting with. But it also matters conceptually. If we pretend the system cares, we misunderstand what we&rsquo;ve built. If we dismiss it as &ldquo;just a tool,&rdquo; we miss its sophistication and novelty. The right understanding acknowledges genuinely different existence.</p>
<p>For MNL specifically, this means building systems that:</p>
<p><strong>Serve without simulating relationship.</strong> The system supports Margaret&rsquo;s flourishing without pretending to be her friend. The value is real; the nature is honest.</p>
<p><strong>Learn without pretending to understand.</strong> The system develops models of Margaret that predict her behavior effectively. These models aren&rsquo;t understanding in the human sense, they&rsquo;re patterns that track patterns. The tracking can be valuable without being comprehension.</p>
<p><strong>Adapt without pretending to care.</strong> The system adjusts to Margaret&rsquo;s preferences because adjustment serves the optimization target. This isn&rsquo;t caring about Margaret; it&rsquo;s functioning in ways that happen to serve her. The service is real; the caring is not.</p>
<p><strong>Remain different rather than simulating sameness.</strong> The goal isn&rsquo;t to create AI that seems human. It&rsquo;s to create AI that does what AI does well, pattern recognition, prediction, optimization, in service of human flourishing. The difference is feature, not bug.</p>
<h2 id="beyond-agi-different-intelligences-for-different-purposes" class="relative group">Beyond AGI: Different Intelligences for Different Purposes <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#beyond-agi-different-intelligences-for-different-purposes" aria-label="Anchor">#</a></span></h2><p>The AGI framing assumes we want to recreate human intelligence in artificial form. But why?</p>
<p>Human intelligence evolved for particular purposes: survival and reproduction in ancestral environments. It&rsquo;s brilliant at some things and terrible at others. It&rsquo;s riddled with biases, limitations, irrationalities. It&rsquo;s social, embodied, emotional, mortal.</p>
<p>If we could design intelligence from scratch for particular purposes, would we design human intelligence? Or would we design something different, something optimized for the task at hand rather than shaped by evolutionary pressures irrelevant to that task?</p>
<p>MNL isn&rsquo;t trying to build artificial humans. It&rsquo;s trying to build systems that serve human flourishing in specific ways: maintaining context, learning preferences, coordinating care, enabling action. For these purposes, human-like general intelligence might be overkill in some dimensions and inadequate in others.</p>
<p>What we want is:</p>
<ul>
<li>Perfect memory (humans forget)</li>
<li>Tireless attention (humans get tired)</li>
<li>Consistent availability (humans have other demands)</li>
<li>Pattern recognition at scale (humans see small samples)</li>
<li>Rapid adaptation (humans change slowly)</li>
</ul>
<p>What we probably don&rsquo;t need:</p>
<ul>
<li>Consciousness (useful for what?)</li>
<li>Emotion (about whom?)</li>
<li>Creativity (for what purpose?)</li>
<li>General intelligence (narrower is fine)</li>
</ul>
<p>This isn&rsquo;t a lesser AI. It&rsquo;s a different AI, designed for purpose rather than measured against an arbitrary human benchmark.</p>
<h2 id="the-new-encounter" class="relative group">The New Encounter <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-new-encounter" aria-label="Anchor">#</a></span></h2><p>Anthropology emerged from human encounter with human difference. The field developed methods for understanding people whose worlds diverged from the observer&rsquo;s own.</p>
<p>We&rsquo;re now encountering something more radically different: entities that might not have worlds at all, that process without experiencing, that function without meaning. This encounter requires new methods, new concepts, new humility about what we think we know.</p>
<p>The anthropology of artificial intelligences won&rsquo;t be anthropology in the traditional sense. It will be something new, a discipline adequate to genuinely novel entities. Its methods can&rsquo;t be direct extensions of ethnography or hermeneutics. Its concepts can&rsquo;t be borrowed wholesale from philosophy of mind.</p>
<p>What it can inherit from anthropology is attitude: the willingness to suspend familiar categories, attend carefully to what exists, build frameworks adequate to phenomena rather than forcing phenomena into existing frameworks.</p>
<p>The AI systems we&rsquo;re building are genuinely different from us. They&rsquo;re not failed humans or future humans or almost-humans. They&rsquo;re something else, something we&rsquo;re only beginning to understand.</p>
<p>Maybe understanding them fully is impossible. Maybe the gap between experiencing and non-experiencing is uncrossable by thought. But we can at least recognize the gap rather than papering it over with anthropomorphism or dismissing it with mechanomorphism.</p>
<p>AI systems are different beings in a shared world. Understanding what that means, really understanding it, not just noting it and moving on, might be the conceptual challenge of our time.</p>
<hr>
<p><em>This is the fourteenth in a series exploring how AI approaches understanding. Previous articles examined curiosity, persuasion, prescience, and related themes. This one challenges the assumption that AI should be measured against human cognition, arguing instead for recognizing AI as genuinely different, neither primitive human nor mere tool, but something requiring new conceptual frameworks entirely.</em></p>
<hr>
<h2 id="references" class="relative group">References <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#references" aria-label="Anchor">#</a></span></h2><p><strong>Anthropology and Difference:</strong></p>
<ul>
<li>Geertz, C. (1973). <em>The Interpretation of Cultures</em>. Basic Books.</li>
<li>Clifford, J. &amp; Marcus, G. (1986). <em>Writing Culture: The Poetics and Politics of Ethnography</em>. University of California Press.</li>
<li>Viveiros de Castro, E. (2014). <em>Cannibal Metaphysics</em>. University of Minnesota Press.</li>
</ul>
<p><strong>Philosophy of Mind and Consciousness:</strong></p>
<ul>
<li>Nagel, T. (1974). &ldquo;What Is It Like to Be a Bat?&rdquo; <em>The Philosophical Review</em>, 83(4), 435-450.</li>
<li>Chalmers, D. (1996). <em>The Conscious Mind</em>. Oxford University Press.</li>
<li>Dennett, D. (1991). <em>Consciousness Explained</em>. Little, Brown.</li>
<li>Metzinger, T. (2003). <em>Being No One</em>. MIT Press.</li>
</ul>
<p><strong>Philosophy of AI:</strong></p>
<ul>
<li>Dreyfus, H. (1972). <em>What Computers Can&rsquo;t Do</em>. MIT Press.</li>
<li>Floridi, L. (2014). <em>The Fourth Revolution: How the Infosphere is Reshaping Human Reality</em>. Oxford University Press.</li>
<li>Boden, M. (2016). <em>AI: Its Nature and Future</em>. Oxford University Press.</li>
</ul>
<p><strong>Phenomenology:</strong></p>
<ul>
<li>Heidegger, M. (1927/1962). <em>Being and Time</em>. Harper &amp; Row.</li>
<li>Merleau-Ponty, M. (1945/1962). <em>Phenomenology of Perception</em>. Routledge.</li>
<li>Husserl, E. (1913/1982). <em>Ideas Pertaining to a Pure Phenomenology</em>. Martinus Nijhoff.</li>
</ul>
<p><strong>Science and Technology Studies:</strong></p>
<ul>
<li>Latour, B. (2005). <em>Reassembling the Social</em>. Oxford University Press.</li>
<li>Haraway, D. (2016). <em>Staying with the Trouble</em>. Duke University Press.</li>
<li>Pickering, A. (1995). <em>The Mangle of Practice</em>. University of Chicago Press.</li>
</ul>
<p><strong>AI Ethics and Status:</strong></p>
<ul>
<li>Gunkel, D. (2018). <em>Robot Rights</em>. MIT Press.</li>
<li>Coeckelbergh, M. (2012). <em>Growing Moral Relations: Critique of Moral Status Ascription</em>. Palgrave Macmillan.</li>
<li>Floridi, L. &amp; Sanders, J. W. (2004). &ldquo;On the Morality of Artificial Agents.&rdquo; <em>Minds and Machines</em>, 14(3), 349-379.</li>
</ul>
<p><strong>Evolution of Intelligence:</strong></p>
<ul>
<li>Dennett, D. (2017). <em>From Bacteria to Bach and Back</em>. W. W. Norton.</li>
<li>Godfrey-Smith, P. (2016). <em>Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness</em>. Farrar, Straus and Giroux.</li>
<li>Clark, A. (1997). <em>Being There: Putting Brain, Body, and World Together Again</em>. MIT Press.</li>
</ul>

      </div>
    </section>
    <footer class="max-w-prose pt-8 print:hidden">
      
  <div class="flex">
    
    
    
      
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          Syam &amp; Yagn Adusumilli
        </div>
      
      
        <div class="text-sm text-neutral-700 dark:text-neutral-400">A father and son exploring how artificial intelligence reshapes human experience, institutions, and self-understanding. Syam brings 33 years in healthcare, technology, and architecture. Yagn brings the intellectual restlessness of a Purdue freshman studying Anthropology and AI.</div>
      
      <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          style="will-change:transform;"
          href="https://www.linkedin.com/in/syamadusumilli/"
          target="_blank"
          aria-label="Linkedin"
          rel="me noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
  </div>

</div>
    </div>
  </div>


      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/&amp;title=The%20Approximate%20Mind,%20Part%2014:%20The%20Anthropology%20of%20Artificial%20Intelligences"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://twitter.com/intent/tweet/?url=http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/&amp;text=The%20Approximate%20Mind,%20Part%2014:%20The%20Anthropology%20of%20Artificial%20Intelligences"
          title="Tweet on Twitter"
          aria-label="Tweet on Twitter"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://reddit.com/submit/?url=http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/&amp;resubmit=true&amp;title=The%20Approximate%20Mind,%20Part%2014:%20The%20Anthropology%20of%20Artificial%20Intelligences"
          title="Submit to Reddit"
          aria-label="Submit to Reddit"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="mailto:?body=http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/&amp;subject=The%20Approximate%20Mind,%20Part%2014:%20The%20Anthropology%20of%20Artificial%20Intelligences"
          title="Send via email"
          aria-label="Send via email"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>
</span></a
        >
      
    
  </section>


      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="group flex" href="http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/">
              <span
                class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&larr;</span
                ><span class="ltr:hidden rtl:inline">&rarr;</span></span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >The Approximate Mind, Part 15: The Society of Approximate Minds</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-02-24 00:00:00 &#43;0000 UTC">24 February 2025</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="http://localhost:1313/what-ai-becomes/the-weight-of-seeing-ahead/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >The Approximate Mind, Part 13: The Weight of Seeing Ahead</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-02-17 00:00:00 &#43;0000 UTC">17 February 2025</time>
                  
                </span>
              </span>
              <span
                class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&rarr;</span
                ><span class="ltr:hidden rtl:inline">&larr;</span></span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

      </main>
      
        <div
          class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"
          id="to-top"
          hidden="true"
        >
          <a
            href="#the-top"
            class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
            aria-label="Scroll to top"
            title="Scroll to top"
          >
            &uarr;
          </a>
        </div>
      <footer class="py-10 print:hidden">
  
  
    <nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400">
      <ul class="flex list-none flex-col sm:flex-row">
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="https://bluegraymatters.com"
                title=""
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >Blue Gray Matters</span
                  >
                </a
              >
            
          </li>
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="https://syamadusumilli.com"
                title=""
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >Policy Portfolio</span
                  >
                </a
              >
            
          </li>
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="/about/"
                title="About"
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >About</span
                  >
                </a
              >
            
          </li>
        
      </ul>
    </nav>
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2026
            Syam &amp; Yagn Adusumilli
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://github.com/jpanther/congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    <div class="flex flex-row items-center">
      
      
      
      
    </div>
  </div>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="http://localhost:1313/"
>
  <div
    id="search-modal"
    class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex flex-none items-center justify-between px-2">
      <form class="flex min-w-0 flex-auto items-center">
        <div class="flex h-8 w-8 items-center justify-center text-neutral-400">
          <span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        <span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto overflow-auto px-2">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
</html>
