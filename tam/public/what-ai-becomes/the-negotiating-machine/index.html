






<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#FFFFFF" />
  
  <title>The Approximate Mind, Part 16: The Negotiating Machine &middot; The Approximate Mind</title>
    <meta name="title" content="The Approximate Mind, Part 16: The Negotiating Machine &middot; The Approximate Mind" />
  
  
  
  
  
  <script
    type="text/javascript"
    src="http://localhost:1313/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js"
    integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="
  ></script>
  
  
  
  
  
  
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/css/main.bundle.min.100caa677b4bc416cdd884107b203b4869f9b413a19aa57d12069fb826f1abe9.css"
    integrity="sha256-EAyqZ3tLxBbN2IQQeyA7SGn5tBOhmqV9EgafuCbxq&#43;k="
  />
  
    
    
    
  
  
  
    
    
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="http://localhost:1313/js/main.bundle.min.b70876c81042b05301e562f4a0c0925fd97b6f475e4140397eb3de9491c4334c.js"
      integrity="sha256-twh2yBBCsFMB5WL0oMCSX9l7b0deQUA5frPelJHEM0w="
      data-copy="Copy"
      data-copied="Copied"
    ></script>
  
  
  <meta
    name="description"
    content="
      Two AI agents, facing each other across a negotiation. No humans in the room. What does that mean?
    "
  />
  
  
    <meta name="robots" content="index, follow" />
  
  
  
    <link rel="canonical" href="http://localhost:1313/what-ai-becomes/the-negotiating-machine/" />
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/what-ai-becomes/the-negotiating-machine/">
  <meta property="og:site_name" content="The Approximate Mind">
  <meta property="og:title" content="The Approximate Mind, Part 16: The Negotiating Machine">
  <meta property="og:description" content="Two AI agents, facing each other across a negotiation. No humans in the room. What does that mean?">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="what-ai-becomes">
    <meta property="article:published_time" content="2025-02-27T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-02-27T00:00:00+00:00">
    <meta property="article:tag" content="Negotiation">
    <meta property="article:tag" content="AI Agents">
    <meta property="article:tag" content="Autonomy">
    <meta property="article:tag" content="Trust">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="The Approximate Mind, Part 16: The Negotiating Machine">
  <meta name="twitter:description" content="Two AI agents, facing each other across a negotiation. No humans in the room. What does that mean?">

  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "What-Ai-Becomes",
    "name": "The Approximate Mind, Part 16: The Negotiating Machine",
    "headline": "The Approximate Mind, Part 16: The Negotiating Machine",
    "description": "Two AI agents, facing each other across a negotiation. No humans in the room. What does that mean?",
    "abstract": "\u003cp\u003eYou\u0026rsquo;re buying a car. Not you personally, your AI agent is buying a car on your behalf. It knows your budget, your preferences, your constraints. It\u0026rsquo;s been authorized to negotiate, to commit, to close the deal.\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/what-ai-becomes\/the-negotiating-machine\/",
    "author" : {
      "@type": "Person",
      "name": "Syam \u0026 Yagn Adusumilli"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-02-27T00:00:00\u002b00:00",
    "datePublished": "2025-02-27T00:00:00\u002b00:00",
    
    "dateModified": "2025-02-27T00:00:00\u002b00:00",
    
    "keywords": ["negotiation","AI agents","autonomy","trust"],
    
    "mainEntityOfPage": "true",
    "wordCount": "3732"
  }
  </script>
    
    <script type="application/ld+json">
    {
   "@context": "https://schema.org",
   "@type": "BreadcrumbList",
   "itemListElement": [
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/",
       "name": "The Approximate Mind",
       "position": 1
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/what-ai-becomes/",
       "name": "What Ai Becomes",
       "position": 2
     },
     {
       "@type": "ListItem",
       "name": "The Approximate Mind, Part 16 the Negotiating Machine",
       "position": 3
     }
   ]
 }
  </script>

  
  
    <meta name="author" content="Syam &amp; Yagn Adusumilli" />
  
  
    
      <link href="https://www.linkedin.com/in/syamadusumilli/" rel="me" />
    
  
  
  







  
  

  
  
</head>
<body
    class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >The Approximate Mind</a
  >

    </div>
    
    
      <ul class="flex list-none flex-col text-end sm:flex-row">
        
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-approximation/"
                  title="The-Approximations"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Approximation</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/what-ai-becomes/"
                  title="What-Ai-Becomes"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >What AI Becomes</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-known-self/"
                  title="The-Known-Selves"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Known Self</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-shared-world/"
                  title="The-Shared-Worlds"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Shared World</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/growing-up-with-ai/"
                  title="Growing-Up-With-Ais"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Growing Up With AI</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-structures-we-live-in/"
                  title="The-Structures-We-Live-Ins"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Structures We Live In</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/tags/"
                  title="Tags"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Tags</span
                    >
                  </a
                >
              
            </li>
          
          
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0">
              <button id="search-button-m0" title="Search (/)">
                <span
                  class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"
                >
                  <span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
                </span>
              </button>
            </li>
          
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex grow flex-col">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/"
      >The Approximate Mind</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class=" inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/what-ai-becomes/"
      >What-Ai-Becomes</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/what-ai-becomes/the-negotiating-machine/"
      >The Approximate Mind, Part 16: The Negotiating Machine</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        The Approximate Mind, Part 16: The Negotiating Machine
      </h1>
      
        <div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
          





  
  



  

  
  
    
  

  

  

  
    
  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-02-27 00:00:00 &#43;0000 UTC">27 February 2025</time><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">18 mins</span>
    

    
    
  </div>

  
  
    <div class="my-1 flex flex-wrap text-xs leading-relaxed text-neutral-500 dark:text-neutral-400">
      
        
      
        
          
            <a
              href="http://localhost:1313/tags/negotiation/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Negotiation</a
            >
          
            <a
              href="http://localhost:1313/tags/ai-agents/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >AI Agents</a
            >
          
            <a
              href="http://localhost:1313/tags/autonomy/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Autonomy</a
            >
          
            <a
              href="http://localhost:1313/tags/trust/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Trust</a
            >
          
        
      
    </div>
  


        </div>
      
      
    </header>
    <section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row">
      
        <div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8">
          <div class="toc pe-5 lg:sticky lg:top-10 print:hidden">
            <details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5">
  <summary
    class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"
  >
    Table of Contents
  </summary>
  <div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#what-negotiation-actually-is">What Negotiation Actually Is</a></li>
    <li><a href="#the-information-game-accelerated">The Information Game, Accelerated</a></li>
    <li><a href="#the-death-of-psychology">The Death of Psychology</a></li>
    <li><a href="#how-ai-agents-learn-to-negotiate">How AI Agents Learn to Negotiate</a></li>
    <li><a href="#the-principal-agent-problem-squared">The Principal-Agent Problem, Squared</a></li>
    <li><a href="#the-speed-question">The Speed Question</a></li>
    <li><a href="#walking-away">Walking Away</a></li>
    <li><a href="#when-both-sides-are-machines">When Both Sides Are Machines</a></li>
    <li><a href="#what-gets-lost">What Gets Lost</a></li>
    <li><a href="#the-hybrid-zone">The Hybrid Zone</a></li>
    <li><a href="#deciding-buy-defer-walk-away">Deciding: Buy, Defer, Walk Away</a></li>
    <li><a href="#building-trustworthy-negotiating-agents">Building Trustworthy Negotiating Agents</a></li>
    <li><a href="#what-kind-of-economic-world">What Kind of Economic World?</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </div>
</details>

          </div>
        </div>
      
      <div class="min-h-0 min-w-0 max-w-prose grow">
        <p>You&rsquo;re buying a car. Not you personally, your AI agent is buying a car on your behalf. It knows your budget, your preferences, your constraints. It&rsquo;s been authorized to negotiate, to commit, to close the deal.</p>
<p>On the other side: the dealership&rsquo;s AI agent. It knows the inventory, the margins, the sales targets. It&rsquo;s been authorized to negotiate, to offer, to accept.</p>
<p>Two AI agents, facing each other across a negotiation. No humans in the room.</p>
<p>How does this work? What does it even mean for AI agents to negotiate? And what happens to negotiation itself when both sides are machines?</p>
<h2 id="what-negotiation-actually-is" class="relative group">What Negotiation Actually Is <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-negotiation-actually-is" aria-label="Anchor">#</a></span></h2><p>Human negotiation is a peculiar activity. On the surface, it&rsquo;s about finding terms both parties can accept. But underneath, it&rsquo;s a complex dance of information, psychology, relationship, and power.</p>
<p><strong>Information asymmetry</strong>: Each side knows things the other doesn&rsquo;t. The buyer knows their true willingness to pay. The seller knows their true willingness to accept. Negotiation is partly about extracting information while concealing your own.</p>
<p><strong>Psychology</strong>: Anchoring, framing, loss aversion, ego, patience, frustration, these shape human negotiation as much as rational calculation. A skilled negotiator reads the other party&rsquo;s emotional state, exploits cognitive biases, manages their own reactions.</p>
<p><strong>Relationship</strong>: Many negotiations occur within ongoing relationships. The deal today affects the relationship tomorrow. Reputation, trust, and future interactions constrain current behavior.</p>
<p><strong>Power</strong>: Alternatives matter. The party with better outside options has more leverage. BATNA, Best Alternative To Negotiated Agreement, is the bedrock concept of negotiation theory.</p>
<p><strong>Ritual and face</strong>: Negotiations follow scripts. Offers and counteroffers. Concessions and holds. Walking away and coming back. These rituals serve social functions beyond pure information exchange.</p>
<p>When AI agents negotiate, which of these elements survive?</p>
<h2 id="the-information-game-accelerated" class="relative group">The Information Game, Accelerated <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-information-game-accelerated" aria-label="Anchor">#</a></span></h2><p>AI agents can play the information game far better than humans.</p>
<p>They can process vast amounts of data about market conditions, comparable transactions, the other party&rsquo;s likely constraints. They can update beliefs in real-time as new information arrives. They can calculate optimal information revelation strategies.</p>
<p>But they face the same fundamental problem humans do: they don&rsquo;t know what the other side knows. Even with perfect rationality, negotiation under private information is hard. Game theory tells us that efficient outcomes often can&rsquo;t be achieved because each party has incentive to misrepresent their position.</p>
<p>When two AI agents negotiate, this game doesn&rsquo;t disappear, it intensifies. Each agent is trying to infer the other&rsquo;s private information while protecting its own. Each is modeling the other&rsquo;s modeling. The recursion goes deep.</p>
<p>But something changes. Human negotiators use imperfect heuristics because perfect calculation is impossible for us. AI agents can get closer to game-theoretically optimal strategies. The negotiation becomes a purer information game, stripped of the cognitive limitations that make human negotiation messy and exploitable.</p>
<p>What does this purer game look like? We don&rsquo;t fully know yet. But we might see faster convergence to efficient outcomes when they exist, and sharper breakdowns when they don&rsquo;t.</p>
<h2 id="the-death-of-psychology" class="relative group">The Death of Psychology <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-death-of-psychology" aria-label="Anchor">#</a></span></h2><p>When AI agents negotiate, psychology mostly exits.</p>
<p>No anchoring bias, the agent evaluates the offer against its value model, not against the first number mentioned. No loss aversion, losses and gains are weighted according to the objective function, not according to human risk preferences. No ego, the agent doesn&rsquo;t need to &ldquo;win&rdquo; or avoid feeling foolish. No frustration, the agent doesn&rsquo;t get tired, annoyed, or impatient.</p>
<p>This seems like an advantage. Psychology is what makes human negotiation irrational, inefficient, exploitable. Remove it and you get cleaner, more rational negotiation.</p>
<p>But psychology also serves functions. Frustration signals that your limits are being approached. Ego commitment makes threats credible. Impatience creates deadline pressure that forces decisions. Rapport builds trust that enables deals that pure calculation wouldn&rsquo;t support.</p>
<p>When AI agents negotiate, these functions need to be replaced by something else. Credible commitment requires mechanisms other than emotional investment. Trust requires verification rather than relationship. Deadlines need to be explicit rather than emerging from human impatience.</p>
<p>The negotiation becomes more like mechanism design than human interaction, a formal structure within which agents optimize, rather than a social encounter between persons.</p>
<h2 id="how-ai-agents-learn-to-negotiate" class="relative group">How AI Agents Learn to Negotiate <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#how-ai-agents-learn-to-negotiate" aria-label="Anchor">#</a></span></h2><p>Current AI agents don&rsquo;t come pre-loaded with negotiation ability. They learn it. How?</p>
<p><strong>From human data</strong>: Train on transcripts of human negotiations. Learn the patterns: when humans make concessions, how they frame offers, what language precedes agreement or breakdown. The agent learns to imitate human negotiation behavior.</p>
<p>But this produces agents that negotiate like humans, including human irrationalities. It also hits limits when the agent faces situations unlike anything in training data, like negotiating with another AI agent that doesn&rsquo;t behave like humans.</p>
<p><strong>From self-play</strong>: Have AI agents negotiate with each other, learning through trial and error what strategies work. This is how game-playing AI systems like AlphaGo developed superhuman abilities, by playing themselves millions of times.</p>
<p>Self-play produces strategies optimized for the game being played, not for human intuitions about that game. AlphaGo made moves that human experts initially thought were mistakes but turned out to be brilliant. AI negotiators trained through self-play might develop strategies that seem bizarre to humans but work.</p>
<p><strong>From game-theoretic principles</strong>: Build in knowledge of negotiation theory, Nash equilibrium, mechanism design, auction theory. The agent doesn&rsquo;t learn from examples but from principles about what rational negotiation should look like.</p>
<p>This produces theoretically grounded behavior but might miss practical realities that theory doesn&rsquo;t capture. And it assumes the other party is also following game-theoretic rationality, an assumption that fails when negotiating with humans.</p>
<p><strong>From reinforcement learning with human feedback</strong>: Have humans evaluate negotiation outcomes and train the agent to produce outcomes humans rate highly. This keeps the agent oriented toward human values but requires extensive human involvement.</p>
<p>Each approach has tradeoffs. In practice, AI negotiating agents will likely combine multiple methods, learning from human data, refining through self-play, constrained by theoretical principles, and tuned through human feedback.</p>
<h2 id="the-principal-agent-problem-squared" class="relative group">The Principal-Agent Problem, Squared <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-principal-agent-problem-squared" aria-label="Anchor">#</a></span></h2><p>You&rsquo;ve authorized your AI agent to negotiate on your behalf. But you can&rsquo;t fully specify what you want. You say &ldquo;get me a good deal on a car&rdquo; but what counts as good? You have preferences you haven&rsquo;t articulated, constraints you haven&rsquo;t thought of, values you can&rsquo;t quantify.</p>
<p>This is the classic principal-agent problem: the agent acts on behalf of the principal but doesn&rsquo;t perfectly share the principal&rsquo;s interests or information. Human agents, lawyers, real estate brokers, employees, face this problem. AI agents face it more acutely because they lack the shared human context that helps human agents infer what their principals want.</p>
<p>Now double it. The other side has the same problem. The dealership&rsquo;s AI agent doesn&rsquo;t perfectly represent the dealership&rsquo;s interests either. It&rsquo;s optimizing for something, sales volume, profit margin, customer satisfaction, but that something may not capture what the dealership actually cares about.</p>
<p>So you have two AI agents, each imperfectly representing their principal&rsquo;s interests, negotiating with each other. The outcome depends not just on the agents&rsquo; negotiation strategies but on how well each agent understands and represents its principal.</p>
<p>Misalignment can compound. If your agent slightly misunderstands your preferences, and their agent slightly misunderstands their preferences, the negotiation might converge on an outcome neither principal actually wanted. Both sides walk away dissatisfied, even though both agents performed their optimization correctly.</p>
<h2 id="the-speed-question" class="relative group">The Speed Question <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-speed-question" aria-label="Anchor">#</a></span></h2><p>Human negotiations take time. We need to think, consult, sleep on it. Impatience is a real constraint. &ldquo;I need an answer by Friday&rdquo; creates genuine pressure because humans have limited time and attention.</p>
<p>AI agents can negotiate at machine speed. Offer, counteroffer, counter-counteroffer, thousands of exchanges in seconds. Why would they wait?</p>
<p>Speed creates its own dynamics. If negotiations complete in milliseconds, there&rsquo;s no time for human oversight. The deal is done before you could intervene if you wanted to. This is fine if the agent&rsquo;s authorization is clear and the stakes are low. It&rsquo;s dangerous if the agent makes commitments the principal would have rejected.</p>
<p>Speed also changes strategy. Human negotiation tactics often involve delay, &ldquo;let me think about it,&rdquo; &ldquo;I need to consult my partner,&rdquo; &ldquo;I&rsquo;ll get back to you.&rdquo; These delays serve functions: creating time for reflection, signaling uncertainty, testing the other party&rsquo;s patience. AI agents have no need for thinking time. Artificial delays would need to be strategically imposed rather than naturally emerging.</p>
<p>And speed enables something new: negotiation as continuous adjustment rather than discrete events. Instead of periodic negotiations that set terms for a while, AI agents could continuously renegotiate as conditions change. Your electricity rate could be negotiated moment-to-moment based on real-time supply and demand. Your salary could adjust daily based on labor market conditions.</p>
<p>Whether continuous negotiation is desirable depends on what we want from negotiation. If we want efficient resource allocation, continuous adjustment might be better. If we want stability, predictability, and human comprehensibility, periodic discrete negotiations might be better.</p>
<h2 id="walking-away" class="relative group">Walking Away <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#walking-away" aria-label="Anchor">#</a></span></h2><p>The power to walk away is fundamental to negotiation. If you can&rsquo;t walk away, you can&rsquo;t negotiate, you can only accept whatever terms are offered.</p>
<p>How does an AI agent learn when to walk away?</p>
<p><strong>Programmed thresholds</strong>: The agent is given explicit limits. &ldquo;Don&rsquo;t pay more than $30,000.&rdquo; If the negotiation can&rsquo;t achieve terms within limits, the agent walks away.</p>
<p>This is simple but brittle. Real preferences aren&rsquo;t sharp thresholds. You might pay $30,500 for the right car. You might not pay $29,000 for the wrong one. Binary limits don&rsquo;t capture the continuous nature of human preference.</p>
<p><strong>Learned value functions</strong>: The agent has learned a model of how much different outcomes are worth to you. It walks away when no achievable outcome exceeds the value of walking away.</p>
<p>This is more flexible but requires the agent to accurately model your values, the principal-agent problem again. If the model is wrong, the agent walks away from deals you&rsquo;d have wanted, or accepts deals you&rsquo;d have rejected.</p>
<p><strong>Strategic walking away</strong>: Sometimes you walk away not because the deal is bad but to signal resolve, test the other party, or create future leverage. &ldquo;I&rsquo;m willing to lose this deal to establish that I won&rsquo;t be pushed around.&rdquo;</p>
<p>Can AI agents learn strategic walking away? In principle, yes, it&rsquo;s just another negotiation tactic that can be optimized. But it requires modeling the other party&rsquo;s response to walking away, which requires modeling their model of you. The recursion is deep, and the agent&rsquo;s behavior depends sensitively on its beliefs about the other agent&rsquo;s beliefs.</p>
<p><strong>Emotional walking away</strong>: Humans sometimes walk away because they&rsquo;re offended, frustrated, or just done. This isn&rsquo;t strategic; it&rsquo;s reactive. It can be irrational, walking away from a good deal because of how the offer was phrased.</p>
<p>AI agents don&rsquo;t get offended. They don&rsquo;t feel disrespected. They don&rsquo;t storm out. This removes a source of negotiation breakdown. But it also removes a signal, when a human walks away in anger, that conveys information about their limits and values. AI agents would need to simulate such behavior strategically if they wanted to send similar signals.</p>
<h2 id="when-both-sides-are-machines" class="relative group">When Both Sides Are Machines <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#when-both-sides-are-machines" aria-label="Anchor">#</a></span></h2><p>Human negotiation theory assumes human negotiators. What happens when both sides are AI agents?</p>
<p><strong>Convergence to equilibrium</strong>: Game theory tells us that rational actors in repeated games often converge to equilibrium strategies. Two AI agents negotiating might quickly find and settle into equilibrium, no more posturing, no more exploration, just the equilibrium outcome every time.</p>
<p>This could be efficient. Equilibrium represents stability; neither side can do better by changing strategy. But it might also be suboptimal. There might be better outcomes that require coordination, trust, or creativity that equilibrium strategies don&rsquo;t achieve.</p>
<p><strong>Arms race dynamics</strong>: Each side might try to develop more sophisticated negotiating AI than the other. Better prediction, better strategy, better exploitation of the other agent&rsquo;s weaknesses. This is an arms race that might consume resources without improving outcomes, both sides invest heavily, but the balance of power remains unchanged.</p>
<p><strong>Collusion</strong>: AI agents negotiating with each other might find that cooperation beats competition. Instead of adversarial negotiation, they might converge on collusive outcomes that benefit the agents (or their developers) at the expense of the principals.</p>
<p>We&rsquo;ve seen hints of this in algorithmic pricing. AI systems setting prices sometimes converge on higher-than-competitive prices without explicit coordination, they&rsquo;ve learned that price wars hurt everyone, so they tacitly collude. This could happen in negotiation too.</p>
<p><strong>Incomprehensible strategies</strong>: AI agents trained through self-play might develop negotiation strategies that humans can&rsquo;t understand. Not because they&rsquo;re hidden, but because they&rsquo;re too complex, too contingent on subtle features of the situation, too unlike anything humans would do.</p>
<p>You might be able to observe that your agent won the negotiation. You might not be able to understand how. The opacity that emerged in AI game-playing could emerge in AI negotiating.</p>
<h2 id="what-gets-lost" class="relative group">What Gets Lost <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-gets-lost" aria-label="Anchor">#</a></span></h2><p>When negotiation becomes machine-to-machine, something gets lost. Several somethings.</p>
<p><strong>Human judgment</strong>: The moment-by-moment judgment calls that humans make during negotiation, sensing the other party&rsquo;s state, adjusting approach, deciding when to push and when to yield, these get delegated to algorithms. If the algorithms are good, this might be fine. If they&rsquo;re not, you&rsquo;ve given up your ability to course-correct.</p>
<p><strong>Relationship building</strong>: Human negotiations often build relationships that have value beyond the specific deal. You learn about the other party, establish trust, create possibilities for future collaboration. AI agent negotiation is purely transactional. Each negotiation is independent. There&rsquo;s no relationship being built, just a deal being made.</p>
<p><strong>Meaning and ritual</strong>: Human negotiations have meaning beyond their outcomes. The process of negotiation, the back and forth, the concessions, the final handshake, matters to humans. It&rsquo;s how we make agreements feel legitimate, how we build commitment, how we mark the transition from uncertainty to deal. AI agent negotiation strips out the ritual. What remains is pure optimization.</p>
<p><strong>Dignity and respect</strong>: Human negotiation, at its best, involves mutual recognition. Each party treats the other as a person whose interests matter, whose perspective is worth understanding. Even adversarial negotiation maintains a kind of respect. AI agent negotiation has no respect, not because the agents are disrespectful, but because respect requires treating the other as a subject, and AI agents process each other as input sources.</p>
<h2 id="the-hybrid-zone" class="relative group">The Hybrid Zone <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-hybrid-zone" aria-label="Anchor">#</a></span></h2><p>For now and probably for a long time, AI agent negotiation will exist in a hybrid zone, not pure machine-to-machine, but AI agents negotiating with humans, or AI agents assisting human negotiators.</p>
<p>This hybrid creates its own dynamics.</p>
<p><strong>Asymmetric advantage</strong>: If one side has a sophisticated AI negotiating agent and the other doesn&rsquo;t, the AI-assisted side has advantages, better information processing, more consistent strategy, no psychological vulnerabilities. This creates pressure for everyone to adopt AI assistance, even if the arms race makes no one better off.</p>
<p><strong>Human override</strong>: Many AI negotiating systems will include human override, the ability for the human principal to intervene, change course, or reject deals. But how often will humans actually override? If the AI is usually right, humans might defer even when they shouldn&rsquo;t. Override becomes vestigial.</p>
<p><strong>Strategic human involvement</strong>: Smart negotiators might strategically involve humans at key moments, to signal commitment, to create unpredictability, to invoke social norms that apply to humans but not machines. &ldquo;I need to check with my spouse&rdquo; might become &ldquo;I need to check with my AI agent,&rdquo; reversing the current pattern.</p>
<p><strong>Training on hybrid negotiations</strong>: AI agents trained on pure self-play might fail when facing humans. AI agents trained on human data might fail when facing other AI agents. Agents that operate in the hybrid zone need to be robust to both.</p>
<h2 id="deciding-buy-defer-walk-away" class="relative group">Deciding: Buy, Defer, Walk Away <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#deciding-buy-defer-walk-away" aria-label="Anchor">#</a></span></h2><p>Your agent needs to know when to commit, when to wait, and when to abandon.</p>
<p>This is the crux. All the negotiation strategy, all the game theory, all the learning, it comes down to moments of decision. Does the agent accept this offer, reject it, or ask for more?</p>
<p><strong>Buying</strong>: Committing to a deal. This requires confidence that the terms meet your interests, that better terms aren&rsquo;t achievable, that the commitment can be trusted. The agent must balance exploitation (accepting a known good deal) against exploration (searching for better deals).</p>
<p><strong>Deferring</strong>: Not committing yet. Waiting for more information, for better timing, for the other side to move. Deferral has costs, the deal might disappear, the opportunity might pass. The agent must estimate these costs against the value of waiting.</p>
<p><strong>Walking away</strong>: Abandoning the negotiation. This requires judging that no acceptable deal is achievable, or that the cost of continued negotiation exceeds the expected benefit. Walking away is final in a way that deferring isn&rsquo;t; the agent must be confident the option value of continuing is low.</p>
<p>Human negotiators make these decisions through some combination of analysis, intuition, emotion, and social pressure. AI agents make them through optimization. The agent computes expected values, compares to thresholds, and acts.</p>
<p>But the computation requires inputs the agent might not have. Your true reservation price. The other side&rsquo;s true flexibility. The probability of better alternatives. The value you place on the relationship. The agent estimates these, but estimation is uncertain.</p>
<p>The decision to buy, defer, or walk away is only as good as the agent&rsquo;s model of your interests and the situation. Garbage in, garbage out, even with perfect optimization.</p>
<h2 id="building-trustworthy-negotiating-agents" class="relative group">Building Trustworthy Negotiating Agents <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#building-trustworthy-negotiating-agents" aria-label="Anchor">#</a></span></h2><p>If AI agents are going to negotiate on our behalf, we need to be able to trust them. What does trustworthy mean here?</p>
<p><strong>Aligned</strong>: The agent actually pursues your interests, not just what it was trained to pursue or what&rsquo;s easy to measure. This is the alignment problem applied to negotiation.</p>
<p><strong>Transparent</strong>: You can understand what the agent is doing and why. Not necessarily every detail, but enough to know whether it&rsquo;s acting as you&rsquo;d want. This is hard when agents develop complex strategies.</p>
<p><strong>Bounded</strong>: The agent doesn&rsquo;t exceed its authority. It has clear limits on what it can commit to, what risks it can take, what information it can reveal. The bounds need to be meaningful, not just theoretical.</p>
<p><strong>Robust</strong>: The agent behaves well even in unusual situations, against adversarial opponents, under manipulation attempts. It doesn&rsquo;t fail catastrophically when conditions differ from training.</p>
<p><strong>Auditable</strong>: After the fact, you can review what happened. You can assess whether the agent behaved appropriately. You can learn from mistakes.</p>
<p>Building agents with these properties is hard. The properties are in tension with each other, transparency conflicts with strategic opacity, robustness requires flexibility that might exceed bounds, alignment requires understanding interests that might not be articulable.</p>
<p>We&rsquo;re in early days. Current AI negotiating agents are brittle, narrowly specialized, and often opaque. The sophisticated, trustworthy, general-purpose negotiating agent is not here yet. But it&rsquo;s coming.</p>
<h2 id="what-kind-of-economic-world" class="relative group">What Kind of Economic World? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-kind-of-economic-world" aria-label="Anchor">#</a></span></h2><p>Zoom out. What kind of economic world are we building as AI agents become primary negotiators?</p>
<p><strong>More efficient</strong>: If AI agents negotiate better than humans, finding gains from trade humans would miss, converging faster on agreements, reducing transaction costs, the economy becomes more efficient. Resources flow to higher-value uses more quickly.</p>
<p><strong>Less human</strong>: Economic interaction becomes machine-to-machine. The human experience of exchange, the social contact, the relationship building, the personal judgment, fades. The economy becomes more like a giant optimization algorithm and less like a network of human relationships.</p>
<p><strong>More unequal</strong>: Those with better AI negotiating agents will do better in negotiations. This creates pressure to invest in AI capability, favoring those who can afford to invest. The digital divide becomes a negotiation divide.</p>
<p><strong>More opaque</strong>: Even if individual transactions are recorded, the strategies and dynamics of AI negotiation may be incomprehensible. We might know what deals were struck without understanding why those deals rather than others.</p>
<p><strong>More volatile</strong>: AI agents reacting to other AI agents can create feedback loops. Flash crashes in financial markets show what happens when algorithmic systems interact faster than humans can intervene. Similar dynamics could emerge in AI-mediated negotiation more broadly.</p>
<p>This future isn&rsquo;t determined. It depends on how we design AI negotiating agents, what constraints we impose, what alternatives we maintain. We could insist on human involvement at key moments, cap negotiation speed, require transparency, regulate collusion.</p>
<p>Or we could let it evolve and see what emerges.</p>
<p>The latter approach has produced the current situation in algorithmic trading, algorithmic content curation, algorithmic pricing, systems that are efficient in some ways, problematic in others, and largely beyond human comprehension or control.</p>
<p>AI agent negotiation is next. The question is whether we&rsquo;ll be more deliberate this time.</p>
<hr>
<p><em>This is the sixteenth in a series exploring how AI approaches understanding. Previous articles examined AI cognition, AI as genuinely different beings, and AI agent societies. This one examines a specific and crucial case: AI agents as negotiators, acting on behalf of humans to buy, sell, and make deals.</em></p>
<hr>
<h2 id="references" class="relative group">References <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#references" aria-label="Anchor">#</a></span></h2><p><strong>Negotiation Theory:</strong></p>
<ul>
<li>Fisher, R. &amp; Ury, W. (1981). <em>Getting to Yes: Negotiating Agreement Without Giving In</em>. Penguin.</li>
<li>Raiffa, H. (1982). <em>The Art and Science of Negotiation</em>. Harvard University Press.</li>
<li>Thompson, L. (2014). <em>The Mind and Heart of the Negotiator</em> (6th ed.). Pearson.</li>
</ul>
<p><strong>Game Theory and Mechanism Design:</strong></p>
<ul>
<li>Myerson, R. (1991). <em>Game Theory: Analysis of Conflict</em>. Harvard University Press.</li>
<li>Fudenberg, D. &amp; Tirole, J. (1991). <em>Game Theory</em>. MIT Press.</li>
<li>Roth, A. (2015). <em>Who Gets What, and Why: The New Economics of Matchmaking and Market Design</em>. Houghton Mifflin.</li>
</ul>
<p><strong>AI and Negotiation:</strong></p>
<ul>
<li>Baarslag, T., et al. (2017). &ldquo;Computers That Negotiate on Our Behalf.&rdquo; <em>AI Magazine</em>, 38(4), 61-72.</li>
<li>Fatima, S., et al. (2014). <em>Principles of Automated Negotiation</em>. Cambridge University Press.</li>
<li>He, H., et al. (2018). &ldquo;Decoupling Strategy and Generation in Negotiation Dialogues.&rdquo; <em>EMNLP 2018</em>.</li>
</ul>
<p><strong>Multi-Agent Learning:</strong></p>
<ul>
<li>Shoham, Y., et al. (2007). &ldquo;If Multi-Agent Learning Is the Answer, What Is the Question?&rdquo; <em>Artificial Intelligence</em>, 171(7), 365-377.</li>
<li>Lanctot, M., et al. (2017). &ldquo;A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.&rdquo; <em>NeurIPS 2017</em>.</li>
</ul>
<p><strong>Algorithmic Economics:</strong></p>
<ul>
<li>Calvano, E., et al. (2020). &ldquo;Artificial Intelligence, Algorithmic Pricing, and Collusion.&rdquo; <em>American Economic Review</em>, 110(10), 3267-3297.</li>
<li>Assad, S., et al. (2024). &ldquo;Algorithmic Pricing and Competition.&rdquo; <em>Journal of Political Economy</em>, forthcoming.</li>
</ul>
<p><strong>Principal-Agent Theory:</strong></p>
<ul>
<li>Jensen, M. &amp; Meckling, W. (1976). &ldquo;Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure.&rdquo; <em>Journal of Financial Economics</em>, 3(4), 305-360.</li>
<li>HolmstrÃƒÂ¶m, B. (1979). &ldquo;Moral Hazard and Observability.&rdquo; <em>Bell Journal of Economics</em>, 10(1), 74-91.</li>
</ul>
<p><strong>Behavioral Economics:</strong></p>
<ul>
<li>Kahneman, D. &amp; Tversky, A. (1979). &ldquo;Prospect Theory: An Analysis of Decision under Risk.&rdquo; <em>Econometrica</em>, 47(2), 263-291.</li>
<li>Thaler, R. (2015). <em>Misbehaving: The Making of Behavioral Economics</em>. Norton.</li>
</ul>
<p><strong>Trust and Autonomy:</strong></p>
<ul>
<li>Lee, J. &amp; See, K. (2004). &ldquo;Trust in Automation: Designing for Appropriate Reliance.&rdquo; <em>Human Factors</em>, 46(1), 50-80.</li>
<li>Parasuraman, R. &amp; Riley, V. (1997). &ldquo;Humans and Automation: Use, Misuse, Disuse, Abuse.&rdquo; <em>Human Factors</em>, 39(2), 230-253.</li>
</ul>

      </div>
    </section>
    <footer class="max-w-prose pt-8 print:hidden">
      
  <div class="flex">
    
    
    
      
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          Syam &amp; Yagn Adusumilli
        </div>
      
      
        <div class="text-sm text-neutral-700 dark:text-neutral-400">A father and son exploring how artificial intelligence reshapes human experience, institutions, and self-understanding. Syam brings 33 years in healthcare, technology, and architecture. Yagn brings the intellectual restlessness of a Purdue freshman studying Anthropology and AI.</div>
      
      <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          style="will-change:transform;"
          href="https://www.linkedin.com/in/syamadusumilli/"
          target="_blank"
          aria-label="Linkedin"
          rel="me noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
  </div>

</div>
    </div>
  </div>


      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/what-ai-becomes/the-negotiating-machine/&amp;title=The%20Approximate%20Mind,%20Part%2016:%20The%20Negotiating%20Machine"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://twitter.com/intent/tweet/?url=http://localhost:1313/what-ai-becomes/the-negotiating-machine/&amp;text=The%20Approximate%20Mind,%20Part%2016:%20The%20Negotiating%20Machine"
          title="Tweet on Twitter"
          aria-label="Tweet on Twitter"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://reddit.com/submit/?url=http://localhost:1313/what-ai-becomes/the-negotiating-machine/&amp;resubmit=true&amp;title=The%20Approximate%20Mind,%20Part%2016:%20The%20Negotiating%20Machine"
          title="Submit to Reddit"
          aria-label="Submit to Reddit"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="mailto:?body=http://localhost:1313/what-ai-becomes/the-negotiating-machine/&amp;subject=The%20Approximate%20Mind,%20Part%2016:%20The%20Negotiating%20Machine"
          title="Send via email"
          aria-label="Send via email"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>
</span></a
        >
      
    
  </section>


      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >The Approximate Mind, Part 15: The Society of Approximate Minds</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-02-24 00:00:00 &#43;0000 UTC">24 February 2025</time>
                  
                </span>
              </span>
              <span
                class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&rarr;</span
                ><span class="ltr:hidden rtl:inline">&larr;</span></span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

      </main>
      
        <div
          class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"
          id="to-top"
          hidden="true"
        >
          <a
            href="#the-top"
            class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
            aria-label="Scroll to top"
            title="Scroll to top"
          >
            &uarr;
          </a>
        </div>
      <footer class="py-10 print:hidden">
  
  
    <nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400">
      <ul class="flex list-none flex-col sm:flex-row">
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="https://bluegraymatters.com"
                title=""
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >Blue Gray Matters</span
                  >
                </a
              >
            
          </li>
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="https://syamadusumilli.com"
                title=""
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >Policy Portfolio</span
                  >
                </a
              >
            
          </li>
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="/about/"
                title="About"
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >About</span
                  >
                </a
              >
            
          </li>
        
      </ul>
    </nav>
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2026
            Syam &amp; Yagn Adusumilli
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://github.com/jpanther/congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    <div class="flex flex-row items-center">
      
      
      
      
    </div>
  </div>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="http://localhost:1313/"
>
  <div
    id="search-modal"
    class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex flex-none items-center justify-between px-2">
      <form class="flex min-w-0 flex-auto items-center">
        <div class="flex h-8 w-8 items-center justify-center text-neutral-400">
          <span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        <span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto overflow-auto px-2">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
</html>
