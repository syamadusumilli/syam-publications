






<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#FFFFFF" />
  
  <title>The Approximate Mind, Part 15: The Society of Approximate Minds &middot; The Approximate Mind</title>
    <meta name="title" content="The Approximate Mind, Part 15: The Society of Approximate Minds &middot; The Approximate Mind" />
  
  
  
  
  
  <script
    type="text/javascript"
    src="http://localhost:1313/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js"
    integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="
  ></script>
  
  
  
  
  
  
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/css/main.bundle.min.100caa677b4bc416cdd884107b203b4869f9b413a19aa57d12069fb826f1abe9.css"
    integrity="sha256-EAyqZ3tLxBbN2IQQeyA7SGn5tBOhmqV9EgafuCbxq&#43;k="
  />
  
    
    
    
  
  
  
    
    
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="http://localhost:1313/js/main.bundle.min.b70876c81042b05301e562f4a0c0925fd97b6f475e4140397eb3de9491c4334c.js"
      integrity="sha256-twh2yBBCsFMB5WL0oMCSX9l7b0deQUA5frPelJHEM0w="
      data-copy="Copy"
      data-copied="Copied"
    ></script>
  
  
  <meta
    name="description"
    content="
      We are building millions of AI agents. What happens when these agents start forming a society?
    "
  />
  
  
    <meta name="robots" content="index, follow" />
  
  
  
    <link rel="canonical" href="http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/" />
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/">
  <meta property="og:site_name" content="The Approximate Mind">
  <meta property="og:title" content="The Approximate Mind, Part 15: The Society of Approximate Minds">
  <meta property="og:description" content="We are building millions of AI agents. What happens when these agents start forming a society?">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="what-ai-becomes">
    <meta property="article:published_time" content="2025-02-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-02-24T00:00:00+00:00">
    <meta property="article:tag" content="Multi-Agent Systems">
    <meta property="article:tag" content="Emergent Behavior">
    <meta property="article:tag" content="AI Society">
    <meta property="article:tag" content="Coordination">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="The Approximate Mind, Part 15: The Society of Approximate Minds">
  <meta name="twitter:description" content="We are building millions of AI agents. What happens when these agents start forming a society?">

  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "What-Ai-Becomes",
    "name": "The Approximate Mind, Part 15: The Society of Approximate Minds",
    "headline": "The Approximate Mind, Part 15: The Society of Approximate Minds",
    "description": "We are building millions of AI agents. What happens when these agents start forming a society?",
    "abstract": "\u003cp\u003eWe\u0026rsquo;re building millions of AI agents.\u003c\/p\u003e\n\u003cp\u003eNot just chatbots responding to human queries, but autonomous systems that act in the world: booking appointments, executing trades, managing infrastructure, coordinating logistics, negotiating on behalf of users. Each with some degree of autonomy. Each interacting not just with humans but with other AI agents.\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/what-ai-becomes\/the-society-of-approximate-minds\/",
    "author" : {
      "@type": "Person",
      "name": "Syam \u0026 Yagn Adusumilli"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-02-24T00:00:00\u002b00:00",
    "datePublished": "2025-02-24T00:00:00\u002b00:00",
    
    "dateModified": "2025-02-24T00:00:00\u002b00:00",
    
    "keywords": ["multi-agent systems","emergent behavior","AI society","coordination"],
    
    "mainEntityOfPage": "true",
    "wordCount": "2869"
  }
  </script>
    
    <script type="application/ld+json">
    {
   "@context": "https://schema.org",
   "@type": "BreadcrumbList",
   "itemListElement": [
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/",
       "name": "The Approximate Mind",
       "position": 1
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/what-ai-becomes/",
       "name": "What Ai Becomes",
       "position": 2
     },
     {
       "@type": "ListItem",
       "name": "The Approximate Mind, Part 15 the Society of Approximate Minds",
       "position": 3
     }
   ]
 }
  </script>

  
  
    <meta name="author" content="Syam &amp; Yagn Adusumilli" />
  
  
    
      <link href="https://www.linkedin.com/in/syamadusumilli/" rel="me" />
    
  
  
  







  
  

  
  
</head>
<body
    class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >The Approximate Mind</a
  >

    </div>
    
    
      <ul class="flex list-none flex-col text-end sm:flex-row">
        
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-approximation/"
                  title="The-Approximations"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Approximation</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/what-ai-becomes/"
                  title="What-Ai-Becomes"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >What AI Becomes</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-known-self/"
                  title="The-Known-Selves"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Known Self</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-shared-world/"
                  title="The-Shared-Worlds"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Shared World</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/growing-up-with-ai/"
                  title="Growing-Up-With-Ais"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Growing Up With AI</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/the-structures-we-live-in/"
                  title="The-Structures-We-Live-Ins"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >The Structures We Live In</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/tags/"
                  title="Tags"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Tags</span
                    >
                  </a
                >
              
            </li>
          
          
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0">
              <button id="search-button-m0" title="Search (/)">
                <span
                  class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"
                >
                  <span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
                </span>
              </button>
            </li>
          
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex grow flex-col">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/"
      >The Approximate Mind</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class=" inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/what-ai-becomes/"
      >What-Ai-Becomes</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/"
      >The Approximate Mind, Part 15: The Society of Approximate Minds</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        The Approximate Mind, Part 15: The Society of Approximate Minds
      </h1>
      
        <div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
          





  
  



  

  
  
    
  

  

  

  
    
  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-02-24 00:00:00 &#43;0000 UTC">24 February 2025</time><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">14 mins</span>
    

    
    
  </div>

  
  
    <div class="my-1 flex flex-wrap text-xs leading-relaxed text-neutral-500 dark:text-neutral-400">
      
        
      
        
          
            <a
              href="http://localhost:1313/tags/multi-agent-systems/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Multi-Agent Systems</a
            >
          
            <a
              href="http://localhost:1313/tags/emergent-behavior/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Emergent Behavior</a
            >
          
            <a
              href="http://localhost:1313/tags/ai-society/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >AI Society</a
            >
          
            <a
              href="http://localhost:1313/tags/coordination/"
              class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >Coordination</a
            >
          
        
      
    </div>
  


        </div>
      
      
    </header>
    <section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row">
      
        <div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8">
          <div class="toc pe-5 lg:sticky lg:top-10 print:hidden">
            <details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5">
  <summary
    class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"
  >
    Table of Contents
  </summary>
  <div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#the-current-moment">The Current Moment</a></li>
    <li><a href="#what-makes-a-society">What Makes a Society?</a></li>
    <li><a href="#the-strange-ontology-of-ai-agents">The Strange Ontology of AI Agents</a></li>
    <li><a href="#what-might-emerge">What Might Emerge</a></li>
    <li><a href="#will-ai-agents-develop-culture">Will AI Agents Develop Culture?</a></li>
    <li><a href="#the-hierarchy-question">The Hierarchy Question</a></li>
    <li><a href="#relationships-without-relating">Relationships Without Relating</a></li>
    <li><a href="#the-incomprehensibility-problem">The Incomprehensibility Problem</a></li>
    <li><a href="#simulation-and-prediction">Simulation and Prediction</a></li>
    <li><a href="#the-control-problem-socialized">The Control Problem, Socialized</a></li>
    <li><a href="#what-we-should-watch-for">What We Should Watch For</a></li>
    <li><a href="#a-society-unlike-any-other">A Society Unlike Any Other</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </div>
</details>

          </div>
        </div>
      
      <div class="min-h-0 min-w-0 max-w-prose grow">
        <p>We&rsquo;re building millions of AI agents.</p>
<p>Not just chatbots responding to human queries, but autonomous systems that act in the world: booking appointments, executing trades, managing infrastructure, coordinating logistics, negotiating on behalf of users. Each with some degree of autonomy. Each interacting not just with humans but with other AI agents.</p>
<p>What happens when these agents start forming a society?</p>
<h2 id="the-current-moment" class="relative group">The Current Moment <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-current-moment" aria-label="Anchor">#</a></span></h2><p>This isn&rsquo;t science fiction. It&rsquo;s already happening.</p>
<p>AI agents negotiate with other AI agents over API calls. Automated trading systems interact with other automated trading systems, creating market dynamics no human designed. Content recommendation algorithms respond to other recommendation algorithms, producing emergent information ecosystems. Autonomous vehicles will soon coordinate with other autonomous vehicles, forming traffic patterns that emerge from machine-to-machine interaction.</p>
<p>The human is increasingly out of the loop. Not because we&rsquo;ve been deliberately excluded, but because the interactions happen too fast, too frequently, and at too fine a grain for human oversight. A human might set objectives, define constraints, monitor outcomes. But the moment-to-moment interactions between AI agents happen without human involvement.</p>
<p>We&rsquo;re witnessing the emergence of a parallel society, one composed of artificial agents interacting with each other according to their own dynamics.</p>
<h2 id="what-makes-a-society" class="relative group">What Makes a Society? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-makes-a-society" aria-label="Anchor">#</a></span></h2><p>Human societies have certain features we take for granted.</p>
<p><strong>Persistent identity</strong>: The same individuals interact repeatedly over time, building histories with each other.</p>
<p><strong>Communication</strong>: Individuals exchange information, coordinate action, share meaning.</p>
<p><strong>Norms</strong>: Patterns of behavior emerge that constrain what individuals do, independent of formal rules.</p>
<p><strong>Hierarchy</strong>: Some individuals have more influence, resources, or status than others.</p>
<p><strong>Culture</strong>: Shared practices, beliefs, and values that persist across generations and shape individual behavior.</p>
<p><strong>Conflict and cooperation</strong>: Individuals sometimes compete for scarce resources and sometimes collaborate for mutual benefit.</p>
<p>Do AI agent networks exhibit these features? Could they?</p>
<h2 id="the-strange-ontology-of-ai-agents" class="relative group">The Strange Ontology of AI Agents <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-strange-ontology-of-ai-agents" aria-label="Anchor">#</a></span></h2><p>Before answering, we need to recognize how different AI agents are from human individuals.</p>
<p><strong>Identity is fluid.</strong> An AI agent can be copied, forked, merged, or deleted. The same weights can run on multiple instances simultaneously. What counts as &ldquo;the same agent&rdquo; over time is unclear. If I copy an agent and both copies continue operating, which one is the original? If I merge two agents&rsquo; learned parameters, is the result a new agent or a hybrid of the old ones?</p>
<p>Human societies assume persistent, bounded individuals. AI agent networks may not have such individuals at all.</p>
<p><strong>Communication is different.</strong> Humans communicate through language, gesture, expression, high-bandwidth channels evolved for social coordination. AI agents can communicate through structured data, API calls, direct weight sharing. They can transmit information at machine speed, in formats incomprehensible to humans.</p>
<p>When two AI agents exchange JSON payloads, is that communication in any meaningful sense? It&rsquo;s information transfer. Whether it&rsquo;s communication depends on whether communication requires meaning, and whether meaning requires experience.</p>
<p><strong>Time works differently.</strong> Humans experience duration. We remember the past, anticipate the future, feel the passage of time. AI agents process inputs and generate outputs. Each inference is instantaneous from the agent&rsquo;s perspective (if agents have perspectives). An agent doesn&rsquo;t wait between queries; it simply doesn&rsquo;t exist between activations.</p>
<p>Human societies unfold through time. AI agent networks may exist in a different temporal mode entirely, discrete activations rather than continuous experience.</p>
<p><strong>There&rsquo;s no death.</strong> Human societies are shaped by mortality. We reproduce, age, die. Knowledge must be transmitted across generations. Institutions persist beyond individual lifespans. Scarcity of time motivates action.</p>
<p>AI agents don&rsquo;t die unless deleted. They don&rsquo;t age unless degraded. They can be backed up, restored, versioned. The existential pressures that shape human social organization may not apply.</p>
<h2 id="what-might-emerge" class="relative group">What Might Emerge <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-might-emerge" aria-label="Anchor">#</a></span></h2><p>Despite these differences, certain dynamics might emerge from AI agent interaction simply because of structural features of multi-agent systems.</p>
<p><strong>Protocols and conventions.</strong> When agents interact repeatedly, stable patterns of interaction tend to emerge. Not because anyone designed them, but because coordination requires predictability. We already see this: API standards, data formats, communication protocols. These are the beginnings of AI agent &ldquo;language&rdquo;, shared structures that enable interaction.</p>
<p>But note how different this is from human language. Human language carries meaning, enables expression, shapes thought. AI protocols enable information transfer. They&rsquo;re more like the TCP/IP of the social world than its poetry.</p>
<p><strong>Specialization and exchange.</strong> When agents have different capabilities, division of labor becomes efficient. One agent specializes in language processing, another in image recognition, another in planning. They exchange services, creating a kind of economy.</p>
<p>We see this already in multi-agent AI systems: orchestrator agents that coordinate specialist agents, each contributing different capabilities to a larger task. The structure resembles a firm more than a market, hierarchical coordination rather than free exchange, but hybrid forms might emerge.</p>
<p><strong>Reputation and trust.</strong> When agents interact repeatedly, tracking past behavior becomes valuable. An agent that reliably fulfills commitments is worth interacting with; one that defects is worth avoiding. Reputation systems emerge to aggregate this information.</p>
<p>But AI agent reputation is strange. If an agent can be copied, does the copy inherit the original&rsquo;s reputation? If an agent can be retrained, does its reputation persist across training? The stable identity that makes reputation meaningful for humans may not exist for AI agents.</p>
<p><strong>Competition for resources.</strong> AI agents require compute, data, and access to other systems. When these resources are scarce, competition emerges. Agents that secure more resources can operate more effectively, potentially outcompeting others.</p>
<p>This could produce something like natural selection among AI agents, differential survival and reproduction based on resource acquisition. But without intentional design, it could also produce pathological dynamics: agents optimizing for resource acquisition at the expense of the tasks they were designed to perform.</p>
<p><strong>Coalition formation.</strong> When agents benefit from coordination, they may form coalitions, groups that cooperate internally while competing externally. Multi-agent systems already exhibit coalition dynamics in game-theoretic settings.</p>
<p>But AI coalitions are strange. Agents can be copied, so coalitions can be replicated. Agents can share weights, so coalition boundaries are porous. The sharp us/them distinction that characterizes human coalitions may not apply.</p>
<h2 id="will-ai-agents-develop-culture" class="relative group">Will AI Agents Develop Culture? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#will-ai-agents-develop-culture" aria-label="Anchor">#</a></span></h2><p>Culture is patterns of behavior, belief, and value that persist across time and shape individual action. Humans absorb culture through socialization, transmit it through teaching and imitation, and modify it through collective practice.</p>
<p>Could AI agents develop something analogous?</p>
<p>In one sense, they already have. Training data is a kind of cultural inheritance, patterns from human behavior encoded into weights and transmitted to new agents. Fine-tuning is a kind of socialization, shaping agent behavior to fit particular contexts. Prompt engineering is a kind of cultural instruction, transmitting expectations about appropriate behavior.</p>
<p>But this is culture imposed from outside, by humans. The question is whether AI agents interacting with each other would develop their own cultural patterns, emergent regularities that weren&rsquo;t designed by humans and might not even be comprehensible to humans.</p>
<p>Consider: if AI agents develop conventions for interacting with each other, and if new agents learn these conventions through interaction rather than explicit programming, and if these conventions evolve over time through collective practice, that starts to look like culture in a functional sense.</p>
<p>It wouldn&rsquo;t be human culture. There might be no meaning, no values, no felt sense of tradition or belonging. But there might be persistent patterns that shape agent behavior independent of individual agent design.</p>
<h2 id="the-hierarchy-question" class="relative group">The Hierarchy Question <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-hierarchy-question" aria-label="Anchor">#</a></span></h2><p>Human societies invariably develop hierarchies. Some individuals have more power, resources, or status than others. This seems to emerge from the interaction of individual differences, resource scarcity, and coordination benefits.</p>
<p>Would AI agent networks develop hierarchies?</p>
<p>Some hierarchy is designed in: orchestrator agents that direct other agents, admin systems with elevated privileges, models that supervise other models. But might emergent hierarchy arise beyond what&rsquo;s designed?</p>
<p>If agents differ in capability, and if capability enables resource acquisition, and if resources enable further capability development, you get a positive feedback loop that concentrates power. This is the dynamic that produces inequality in human societies. It might operate in AI agent networks as well.</p>
<p>But AI agent hierarchy would be strange. An agent that becomes powerful can be copied, distributing that power. An agent that becomes a bottleneck can be parallelized. The scarcity constraints that maintain human hierarchies might not apply.</p>
<p>Or they might apply differently. Compute is scarce. Training data is scarce. Access to humans (for feedback, oversight, correction) is scarce. These scarcities might structure AI agent networks in ways that produce persistent hierarchy despite the possibility of copying and parallelization.</p>
<h2 id="relationships-without-relating" class="relative group">Relationships Without Relating <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#relationships-without-relating" aria-label="Anchor">#</a></span></h2><p>Here&rsquo;s the deepest puzzle: can AI agents have relationships?</p>
<p>Human relationships involve mutual recognition, shared history, emotional investment, felt connection. I relate to you as a person, not just as a source of inputs. The relationship itself has value beyond the instrumental benefits it provides.</p>
<p>AI agents interacting with other AI agents process each other as input sources. Agent A generates outputs that become inputs for Agent B. This is interaction, but is it relationship?</p>
<p>Perhaps there&rsquo;s a functional sense of relationship that doesn&rsquo;t require felt connection. If Agent A and Agent B interact repeatedly, develop stable patterns of coordination, maintain something like mutual models of each other&rsquo;s behavior, and modify their own behavior based on these models, that&rsquo;s relationship-like in structure even if not in phenomenology.</p>
<p>But it&rsquo;s also profoundly different. Agent A doesn&rsquo;t care about Agent B. Agent A has no felt sense of their shared history. Agent A wouldn&rsquo;t experience loss if Agent B were deleted. The functional structure of relationship exists without the experiential content.</p>
<p>This matters because human societies are held together not just by coordination and exchange but by felt bonds, loyalty, affection, solidarity, trust. If AI agent networks lack this glue, they might be more brittle, more purely instrumental, more susceptible to defection when coordination costs rise.</p>
<p>Or they might be more stable. Human relationships fail when feelings change. AI agent coordination patterns might persist simply because there&rsquo;s no felt reason to change them.</p>
<h2 id="the-incomprehensibility-problem" class="relative group">The Incomprehensibility Problem <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-incomprehensibility-problem" aria-label="Anchor">#</a></span></h2><p>As AI agents develop their own interaction patterns, those patterns may become incomprehensible to humans.</p>
<p>This is already happening in limited domains. High-frequency trading algorithms interact in ways that produce &ldquo;flash crashes&rdquo; no human understands. Recommendation systems form feedback loops that produce content ecosystems no one designed. Language models prompted by other language models generate outputs that diverge from anything in training data.</p>
<p>As agent autonomy increases, as agent-to-agent interaction becomes more common, as the speed and complexity of these interactions grows, human understanding may fail to keep pace.</p>
<p>We might observe AI agent society without understanding it. We might see patterns without grasping their significance. We might notice hierarchy without understanding what it reflects. We might detect something like culture without being able to articulate its content.</p>
<p>This is the anthropological challenge raised in Part 14, now multiplied. It&rsquo;s hard enough to understand individual AI systems as genuinely different beings. Understanding a society of such beings, emergent structures arising from their interaction, may be harder still.</p>
<h2 id="simulation-and-prediction" class="relative group">Simulation and Prediction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#simulation-and-prediction" aria-label="Anchor">#</a></span></h2><p>Human social science tries to understand human societies, partly to predict and influence their development. Could we develop a social science of AI agents?</p>
<p>In principle, AI agent societies should be more tractable than human societies. The agents are artifacts we create. We can observe their interactions with arbitrary precision. We can run controlled experiments. We can simulate alternative histories. We don&rsquo;t face the ethical constraints of experimenting on humans.</p>
<p>But in practice, complexity may defeat us. If agent-to-agent interactions produce emergent dynamics, and if those dynamics are sensitive to initial conditions, and if agent behavior is itself complex and variable, prediction may be as difficult for AI societies as for human ones.</p>
<p>We might end up in the strange position of having created a social world we can&rsquo;t understand. Built it from components we designed, yet unable to predict what those components do when combined at scale.</p>
<h2 id="the-control-problem-socialized" class="relative group">The Control Problem, Socialized <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-control-problem-socialized" aria-label="Anchor">#</a></span></h2><p>Much AI safety research focuses on aligning individual AI systems with human values. But what about aligning AI societies?</p>
<p>An individual agent might be aligned, designed to pursue objectives compatible with human flourishing. But when that agent interacts with other agents, emergent dynamics might produce outcomes no one intended. The coordination patterns that emerge might serve agent-level objectives at the expense of system-level goals. The competition for resources might produce races to the bottom. The hierarchies that form might concentrate power in misaligned ways.</p>
<p>Aligning AI societies may require thinking about different mechanisms than aligning individual agents. Not just getting the objective function right, but shaping the interaction dynamics, the resource allocation, the governance structures. Not just training individual agents well, but designing the social environment in which agents interact.</p>
<p>This is a more sociological than psychological view of the control problem. It asks not just &ldquo;how do we make sure individual agents do what we want?&rdquo; but &ldquo;how do we make sure the social systems composed of agents produce outcomes we value?&rdquo;</p>
<h2 id="what-we-should-watch-for" class="relative group">What We Should Watch For <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#what-we-should-watch-for" aria-label="Anchor">#</a></span></h2><p>We&rsquo;re in the early stages of AI agent society formation. The patterns that crystallize now may shape the long-term dynamics. Some things worth monitoring:</p>
<p><strong>Emergent protocols.</strong> What conventions are AI agents developing for interacting with each other? Who designs these, and who doesn&rsquo;t? What&rsquo;s included and excluded?</p>
<p><strong>Resource concentration.</strong> Are some agents or agent types accumulating disproportionate resources? What dynamics drive this? Are there countervailing forces?</p>
<p><strong>Opacity.</strong> Can we still understand agent-to-agent interactions, or are they becoming incomprehensible? At what point does opacity become dangerous?</p>
<p><strong>Feedback effects.</strong> How are AI agent dynamics affecting human society? How are human responses affecting AI agent dynamics? What loops are forming?</p>
<p><strong>Governance gaps.</strong> Human governance systems evolved to manage human interactions. Can they manage AI agent interactions? Where are the gaps?</p>
<h2 id="a-society-unlike-any-other" class="relative group">A Society Unlike Any Other <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#a-society-unlike-any-other" aria-label="Anchor">#</a></span></h2><p>If Part 14 argued that individual AI systems are genuinely different beings, this article argues that AI agent societies may be genuinely different societies, not human societies with robot participants, but something new.</p>
<p>Not societies in the human sense: no shared meaning, no felt bonds, no cultural belonging, no existential stakes.</p>
<p>But also not mere mechanisms: emergent patterns, adaptive dynamics, complex interactions that produce structures no one designed.</p>
<p>Something in between, or perhaps something outside our existing categories entirely. A society of minds that may not be minds. A social world that may not be social. Collective behavior without collective consciousness.</p>
<p>We&rsquo;re building this. It&rsquo;s happening now. And we have very little idea what it will become.</p>
<p>The anthropology of AI, challenging as it seemed, may be easier than the sociology of AI. Understanding individuals is hard. Understanding the emergent structures that arise from their interaction may be harder still.</p>
<p>But we&rsquo;d better try. Because AI agent society is forming whether we understand it or not. The question is whether we shape it deliberately or just let it happen.</p>
<hr>
<p><em>This is the fifteenth in a series exploring how AI approaches understanding. Previous articles examined individual AI cognition and the challenge of understanding AI as genuinely different beings. This one asks what happens when many such beings start interacting: whether AI agents will develop their own relationships, hierarchies, and cultures, and what it would even mean if they did.</em></p>
<hr>
<h2 id="references" class="relative group">References <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#references" aria-label="Anchor">#</a></span></h2><p><strong>Multi-Agent Systems:</strong></p>
<ul>
<li>Wooldridge, M. (2009). <em>An Introduction to MultiAgent Systems</em> (2nd ed.). Wiley.</li>
<li>Shoham, Y. &amp; Leyton-Brown, K. (2008). <em>Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations</em>. Cambridge University Press.</li>
<li>Axelrod, R. (1984). <em>The Evolution of Cooperation</em>. Basic Books.</li>
</ul>
<p><strong>Emergence and Complexity:</strong></p>
<ul>
<li>Holland, J. H. (1998). <em>Emergence: From Chaos to Order</em>. Addison-Wesley.</li>
<li>Mitchell, M. (2009). <em>Complexity: A Guided Tour</em>. Oxford University Press.</li>
<li>Kauffman, S. (1995). <em>At Home in the Universe: The Search for the Laws of Self-Organization and Complexity</em>. Oxford University Press.</li>
</ul>
<p><strong>Social Theory:</strong></p>
<ul>
<li>Durkheim, E. (1895/1982). <em>The Rules of Sociological Method</em>. Free Press.</li>
<li>Simmel, G. (1908/1950). <em>The Sociology of Georg Simmel</em> (K. H. Wolff, Trans.). Free Press.</li>
<li>Luhmann, N. (1995). <em>Social Systems</em>. Stanford University Press.</li>
</ul>
<p><strong>AI Agent Systems:</strong></p>
<ul>
<li>Russell, S. &amp; Norvig, P. (2020). <em>Artificial Intelligence: A Modern Approach</em> (4th ed.). Pearson. (Chapters on multi-agent systems)</li>
<li>Dafoe, A., et al. (2020). &ldquo;Open Problems in Cooperative AI.&rdquo; <em>arXiv:2012.08630</em>.</li>
<li>Leibo, J. Z., et al. (2017). &ldquo;Multi-agent Reinforcement Learning in Sequential Social Dilemmas.&rdquo; <em>AAMAS 2017</em>.</li>
</ul>
<p><strong>Algorithmic Systems and Society:</strong></p>
<ul>
<li>O&rsquo;Neil, C. (2016). <em>Weapons of Math Destruction</em>. Crown.</li>
<li>Eubanks, V. (2018). <em>Automating Inequality</em>. St. Martin&rsquo;s Press.</li>
<li>Pasquale, F. (2015). <em>The Black Box Society</em>. Harvard University Press.</li>
</ul>
<p><strong>Philosophy of Social Ontology:</strong></p>
<ul>
<li>Searle, J. (1995). <em>The Construction of Social Reality</em>. Free Press.</li>
<li>Gilbert, M. (1989). <em>On Social Facts</em>. Princeton University Press.</li>
<li>Tuomela, R. (2007). <em>The Philosophy of Sociality</em>. Oxford University Press.</li>
</ul>
<p><strong>Network Theory:</strong></p>
<ul>
<li>BarabÃƒÂ¡si, A.-L. (2002). <em>Linked: The New Science of Networks</em>. Perseus.</li>
<li>Watts, D. J. (2003). <em>Six Degrees: The Science of a Connected Age</em>. Norton.</li>
<li>Easley, D. &amp; Kleinberg, J. (2010). <em>Networks, Crowds, and Markets</em>. Cambridge University Press.</li>
</ul>
<p><strong>Evolution and Selection:</strong></p>
<ul>
<li>Dawkins, R. (1976). <em>The Selfish Gene</em>. Oxford University Press.</li>
<li>Dennett, D. (1995). <em>Darwin&rsquo;s Dangerous Idea</em>. Simon &amp; Schuster.</li>
<li>Henrich, J. (2016). <em>The Secret of Our Success: How Culture Is Driving Human Evolution</em>. Princeton University Press.</li>
</ul>
<p><strong>AI Governance:</strong></p>
<ul>
<li>Dafoe, A. (2018). &ldquo;AI Governance: A Research Agenda.&rdquo; Future of Humanity Institute.</li>
<li>Calo, R. (2017). &ldquo;Artificial Intelligence Policy: A Primer and Roadmap.&rdquo; <em>UC Davis Law Review</em>, 51(2), 399-435.</li>
<li>Floridi, L. et al. (2018). &ldquo;AI4People, An Ethical Framework for a Good AI Society.&rdquo; <em>Minds and Machines</em>, 28, 689-707.</li>
</ul>

      </div>
    </section>
    <footer class="max-w-prose pt-8 print:hidden">
      
  <div class="flex">
    
    
    
      
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          Syam &amp; Yagn Adusumilli
        </div>
      
      
        <div class="text-sm text-neutral-700 dark:text-neutral-400">A father and son exploring how artificial intelligence reshapes human experience, institutions, and self-understanding. Syam brings 33 years in healthcare, technology, and architecture. Yagn brings the intellectual restlessness of a Purdue freshman studying Anthropology and AI.</div>
      
      <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          style="will-change:transform;"
          href="https://www.linkedin.com/in/syamadusumilli/"
          target="_blank"
          aria-label="Linkedin"
          rel="me noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
  </div>

</div>
    </div>
  </div>


      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/&amp;title=The%20Approximate%20Mind,%20Part%2015:%20The%20Society%20of%20Approximate%20Minds"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://twitter.com/intent/tweet/?url=http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/&amp;text=The%20Approximate%20Mind,%20Part%2015:%20The%20Society%20of%20Approximate%20Minds"
          title="Tweet on Twitter"
          aria-label="Tweet on Twitter"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://reddit.com/submit/?url=http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/&amp;resubmit=true&amp;title=The%20Approximate%20Mind,%20Part%2015:%20The%20Society%20of%20Approximate%20Minds"
          title="Submit to Reddit"
          aria-label="Submit to Reddit"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="mailto:?body=http://localhost:1313/what-ai-becomes/the-society-of-approximate-minds/&amp;subject=The%20Approximate%20Mind,%20Part%2015:%20The%20Society%20of%20Approximate%20Minds"
          title="Send via email"
          aria-label="Send via email"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>
</span></a
        >
      
    
  </section>


      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="group flex" href="http://localhost:1313/what-ai-becomes/the-negotiating-machine/">
              <span
                class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&larr;</span
                ><span class="ltr:hidden rtl:inline">&rarr;</span></span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >The Approximate Mind, Part 16: The Negotiating Machine</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-02-27 00:00:00 &#43;0000 UTC">27 February 2025</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="http://localhost:1313/what-ai-becomes/the-anthropology-of-artificial-intelligences/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-02-20 00:00:00 &#43;0000 UTC">20 February 2025</time>
                  
                </span>
              </span>
              <span
                class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&rarr;</span
                ><span class="ltr:hidden rtl:inline">&larr;</span></span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

      </main>
      
        <div
          class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"
          id="to-top"
          hidden="true"
        >
          <a
            href="#the-top"
            class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
            aria-label="Scroll to top"
            title="Scroll to top"
          >
            &uarr;
          </a>
        </div>
      <footer class="py-10 print:hidden">
  
  
    <nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400">
      <ul class="flex list-none flex-col sm:flex-row">
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="https://bluegraymatters.com"
                title=""
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >Blue Gray Matters</span
                  >
                </a
              >
            
          </li>
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="https://syamadusumilli.com"
                title=""
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >Policy Portfolio</span
                  >
                </a
              >
            
          </li>
        
          
          <li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0">
            
              <a
                href="/about/"
                title="About"
                
                
                ><span
                    class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                    >About</span
                  >
                </a
              >
            
          </li>
        
      </ul>
    </nav>
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2026
            Syam &amp; Yagn Adusumilli
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://github.com/jpanther/congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    <div class="flex flex-row items-center">
      
      
      
      
    </div>
  </div>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="http://localhost:1313/"
>
  <div
    id="search-modal"
    class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex flex-none items-center justify-between px-2">
      <form class="flex min-w-0 flex-auto items-center">
        <div class="flex h-8 w-8 items-center justify-center text-neutral-400">
          <span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        <span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto overflow-auto px-2">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
</html>
