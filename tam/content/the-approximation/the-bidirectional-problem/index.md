---
title: "The Approximate Mind, Part 8: When the Approximated Becomes the Approximator"
date: 2025-01-30
draft: false
weight: 8
description: "Humans adapt to AI. We change how we communicate to be better understood by algorithms. The target we are aiming to approximate is shifting as we approximate it."
slug: "the-bidirectional-problem"
tags: ["bidirectional problem", "feedback loops", "human adaptation", "co-evolution"]
series: ["The Approximate Mind"]
series_order: 8
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

We're building AI that approximates human understanding. But something strange is happening: the approximation is changing what it approximates.

Humans adapt to AI. We change how we communicate to be better understood by algorithms. We modify our behavior to work with recommendation systems. We reshape our preferences based on what AI surfaces.

This creates a feedback loop. AI learns from humans. Humans adapt to AI. AI learns from the adapted humans. The target we're aiming to approximate is shifting as we approximate it.

## The Adaptation We Don't Notice

Consider how you've already adapted:

**Search behavior.** You've learned what kinds of queries get good results. You phrase things for algorithms, not for other humans. Your search vocabulary is shaped by years of reinforcement learning, you searching, the algorithm responding, you adjusting.

**Social media presentation.** You've learned what content gets engagement. Even without consciously gaming algorithms, you've internalized patterns that work. Your self-presentation is partly algorithmic optimization.

**Communication style.** Autocomplete suggestions shape what you type. Grammar checkers reshape your prose. Voice assistants train you to speak in ways they understand.

These adaptations happen gradually, often unconsciously. We don't notice we're changing because the changes feel like choices.

## The Cognitive Ecology

This isn't new. Humans have always adapted to their tools. Writing changed how we remember. Printing changed how we organize knowledge. Calculators changed how we do math.

As Andy Clark argues, cognition extends into the environment. Our tools are part of our minds, not just external aids. If AI becomes a significant cognitive partner, it will shape how we think, not just what we think about.

But AI might be different in degree if not in kind. Previous tools didn't adapt back. A hammer doesn't learn from how you use it. AI does. The feedback loop is tighter, faster, more pervasive.

## What Happens to the Target?

If AI approximates human understanding, but humans adapt to AI, then:

**The training data becomes outdated.** AI trained on pre-AI humans might not work well for post-AI humans.

**The target keeps moving.** Every improvement in approximation changes what needs to be approximated.

**The equilibrium is unknown.** We're not approximating a fixed target. We're co-evolving toward some equilibrium that doesn't exist yet.

This is a Heisenberg problem: measuring changes the measured. Approximating understanding changes understanding.

## The Dangers

**Preference distortion.** If AI surfaces certain preferences more than others, we might come to prefer what AI can satisfy. Our preferences would be shaped by AI's capabilities rather than our own needs.

**Cognitive offloading.** If AI handles certain cognitive tasks, we might lose the ability to do them ourselves. The tools become crutches.

**Homogenization.** If everyone adapts to the same AI, diversity might decrease. We'd all converge toward what AI can handle well.

**Manipulation.** If AI learns what influences us, and we adapt to AI, the potential for manipulation increases. We might become more predictable, more nudgeable, more controllable.

Sherry Turkle warned about "alone together", how technology can create the illusion of connection while actually isolating us. The bidirectional problem extends this: technology might create the illusion of being understood while actually reshaping us to fit its understanding.

## The Opportunities

But the feedback loop isn't necessarily bad:

**Enhanced capabilities.** If AI augments our cognition well, we might become more capable, not less. The cognitive ecosystem could be symbiotic.

**Better self-understanding.** AI that reflects our patterns back to us might help us understand ourselves better. The approximation could be a mirror.

**Expanded expression.** AI that understands nuance might help us express things we couldn't before. Our communication could become richer, not poorer.

**Adaptive institutions.** If AI learns from us and we learn from AI, institutions built on this feedback could be more responsive, more flexible, more human.

The question isn't whether the feedback loop happens, it's already happening. The question is whether we steer it deliberately or let it steer us.

## Design Implications

If humans adapt to AI, then:

**Monitor for distortion.** Watch for signs that AI is shaping preferences rather than serving them. Build in checks against manipulation.

**Preserve human capability.** Design AI that augments rather than replaces. Ensure humans can still function without the tool.

**Support diversity.** Resist homogenization. Ensure AI works for diverse humans, not just the majority that adapts fastest.

**Make adaptation visible.** Help people notice how they're changing. Transparency about the feedback loop is the first step to steering it.

**Design for the equilibrium you want.** Since co-evolution is happening, decide what equilibrium would be good and design toward it.

## The Strange Equilibrium

We're moving toward something unprecedented: a cognitive ecosystem where human and artificial intelligence co-evolve. Neither will be understandable without the other. Human cognition will be partly shaped by AI interaction. AI behavior will be trained on AI-adapted humans.

The question isn't whether this is good or bad. It's already happening. The question is whether we steer toward an equilibrium that serves human flourishing or just system efficiency.

Approximating human understanding isn't just a technical challenge. It's the beginning of a feedback loop that changes what "human understanding" means. We're not just building AI to match humans. We're creating conditions for humans and AI to co-evolve toward something neither is alone.

That's either exciting or terrifying, depending on whether we navigate it deliberately or drift into it blindly.

---

*This is the eighth in a series exploring how AI approaches understanding. Previous articles examined the challenge from AI's side. This one examines how humans adapt to AI, creating a bidirectional influence that changes the target we're aiming to approximate.*

---

## References

**Extended Cognition:**
- Clark, A. (2008). *Supersizing the Mind*. Oxford University Press.
- Clark, A. & Chalmers, D. (1998). "The Extended Mind." *Analysis*, 58(1), 7-19.

**Technology and Self:**
- Turkle, S. (2011). *Alone Together*. Basic Books.
- Carr, N. (2010). *The Shallows*. W.W. Norton.

**Co-evolution:**
- Sterelny, K. (2012). *The Evolved Apprentice*. MIT Press.

**Feedback Loops:**
- Wiener, N. (1948). *Cybernetics*. MIT Press.
