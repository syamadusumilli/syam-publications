---
title: "The Approximate Mind, Part 7: Good Enough for Whom?"
date: 2025-01-27
draft: false
weight: 7
description: "Good enough is not universal. It depends on who is judging, their resources and constraints, and what is at stake."
slug: "good-enough-for-whom"
tags: ["sufficiency", "equity", "I AM NOT AVERAGE", "dignity"]
series: ["The Approximate Mind"]
series_order: 7
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

We've been asking "can AI approximate human understanding?" But this question hides another: good enough for what purpose, judged by whose standards, serving whose interests?

"Good enough" isn't universal. It depends on who's judging, their resources and constraints, and what's at stake.

## The Margaret Problem

Consider Margaret again. She's 78, managing diabetes, living independently. What counts as "good enough" AI support for her?

**For Margaret:** Good enough means maintaining independence, feeling understood, not being overwhelmed with technology. 85% medication adherence might be acceptable if 95% adherence would require interventions she finds burdensome.

**For her physician:** Good enough means A1C under 7, reduced hospitalizations, clinical metrics optimized. 95% adherence is the goal, and Margaret's preferences about how to get there are secondary to outcomes.

**For her daughter:** Good enough means no emergencies, no crises, peace of mind. She wants the system to catch problems early, even if Margaret finds the monitoring intrusive.

**For the insurance company:** Good enough means reduced costs. Whatever prevents expensive hospitalizations, regardless of Margaret's subjective experience.

**For the AI developer:** Good enough means engagement metrics, successful predictions, minimal complaints. Whether Margaret's life is genuinely better might be harder to measure than whether she uses the app.

These definitions conflict. Optimizing for one might undermine another. "Good enough" is always relative to someone's standards, and whose standards dominate determines what the system actually does.

## The Justice Question

There's a deeper problem. When we deploy AI that's "good enough," we're making implicit choices about who gets helped and who doesn't.

AI trained on wealthy, educated, WEIRD populations works best for people like the training data. Deploy it to everyone and you've created a system that's great for some and mediocre for others. The gap follows existing lines of privilege.

Is that acceptable? It depends on what you're comparing to:

**Perfection standard:** The AI should work equally well for everyone. By this standard, deployment is unjust because quality is unequal.

**Improvement standard:** The AI should help everyone more than they were helped before. By this standard, deployment might be just even if quality is unequal, if everyone is better off than before, inequality in improvement might be acceptable.

**Opportunity standard:** The AI should provide equal opportunity for benefit, even if outcomes differ. By this standard, the question is access, not outcomes.

The improvement standard tends to dominate in practice: something is better than nothing. But it obscures a moral choice. When we deploy imperfect AI to underserved populations, we're choosing between "some help that's imperfect" and "waiting for perfect help that may never come."

## The Bias Complication

What if AI is 90% accurate for white patients, 75% accurate for Black patients?

**By the perfection standard:** Don't deploy. Unequal accuracy means unequal care.

**By the improvement standard:** Deploy anyway. 75% accuracy exceeds the 0% they had before.

**But there's a catch:** Deploying unequal AI normalizes the inequality. It becomes the baseline. The urgency to fix the disparity decreases because "at least they have something."

My answer: Deploy while working urgently to fix bias, but acknowledge that "better than nothing" can still be unjust. The question isn't just "does this help?" but "does this help in ways that reduce or entrench inequality?"

## Whose Definition Wins?

In practice, the definition of "good enough" that wins is usually the definition held by whoever has power:

**Developers** define good enough in terms of what they can measure and optimize.

**Funders** define good enough in terms of ROI and market capture.

**Regulators** define good enough in terms of safety and compliance.

**Users** define good enough in terms of their actual experience, but users often have least power in shaping product design.

The people most affected by AI, patients, seniors, marginalized communities, rarely get to define what "good enough" means for them. The definition is imposed from above.

This is a problem of justice, not just technology. Building AI that truly serves people requires giving affected communities voice in defining success.

## A Framework for "Good Enough"

Let me propose four criteria:

**1. Minimum adequacy threshold.** Basic safety, sufficient accuracy, bounded bias. Below this threshold, don't deploy regardless of other considerations.

**2. Comparative justice.** Does deployment reduce or increase inequality? If deployment widens gaps between well-served and poorly-served populations, that's a cost that needs justification.

**3. Progressive improvement.** Accept imperfection while requiring improvement. Deploy imperfect systems only with binding commitments to reduce known problems.

**4. Community voice.** Let affected populations participate in defining success. Not just consulting communities, but giving them genuine decision-making power.

## The Uncomfortable Truth

When we apply perfection standards to wealthy hospitals and improvement standards to poor clinics, we're making a moral choice about acceptable inequality. Often this choice is made by people who'll never experience the "good enough" systems they're deploying.

The question isn't just "is AI good enough?" but "is this AI system part of a just distribution, or does it perpetuate existing injustice?"

That's not a technical question. It's a political one. And answering it honestly requires acknowledging that "good enough" is always good enough for someone, and asking who benefits and who loses when we deploy imperfect systems.

---

*This is the seventh in a series exploring how AI approaches understanding. Previous articles examined capabilities and limitations. This one examines who gets to define success and why that matters for justice.*

---

## References

**Justice and Distribution:**
- Rawls, J. (1971). *A Theory of Justice*. Harvard University Press.
- Sen, A. (1999). *Development as Freedom*. Oxford University Press.
- Nussbaum, M. (2006). *Frontiers of Justice*. Harvard University Press.

**Technology and Inequality:**
- Benjamin, R. (2019). *Race After Technology*. Polity Press.
- Eubanks, V. (2018). *Automating Inequality*. St. Martin's Press.
- O'Neil, C. (2016). *Weapons of Math Destruction*. Crown.

**Participatory Design:**
- Costanza-Chock, S. (2020). *Design Justice*. MIT Press.
