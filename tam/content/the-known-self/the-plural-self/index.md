---
title: "The Approximate Mind, Part 25: The Plural Self"
subtitle: "Can AI Understand That You Are Many?"
date: 2025-03-31
draft: false
weight: 25
description: "You are not one person. You are many. Can AI understand that the self who talks to a therapist and the self who talks to a boss are different expressions of the same person?"
slug: "the-plural-self"
tags: ["identity", "the plural self", "context", "personalization"]
series: ["The Approximate Mind"]
series_order: 25
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

You are not one person.

You are Margaret-the-grandmother when your daughter visits with the children. Different values, different priorities, different ways of speaking. The self that emerges in that context genuinely differs from other Margarets.

You are Margaret-the-patient when the physician enters. More deferential perhaps. More focused on symptoms. Speaking a different language about your body and its failures.

You are Margaret-the-widow when alone at night. Memories surfacing that stay submerged during the day. A relationship with the past that others never see.

You are Margaret-the-neighbor, Margaret-the-churchgoer, Margaret-the-former-teacher. Each activated by context. Each real. None more authentic than the others.

This is not code-switching in the superficial sense of adjusting your presentation. **The selves are genuinely different.** They want different things. They make different decisions. They constitute different relationships with the world.

William James saw this over a century ago. "A man has as many social selves as there are individuals who recognize him." But James perhaps understated it. The selves are not just recognized by others. They are constituted by the relationships that call them forth.

Can AI understand this? Can it model the plural self? And what happens if it tries?

## The Standard Model's Failure

Most AI systems model humans as unified agents with stable preferences.

The recommendation algorithm assumes you have a taste profile. The personalization engine assumes you have preferences. The prediction system assumes your past behavior indicates your future behavior. All singular. All stable. All one.

This works adequately for simple domains. Your movie preferences probably are relatively stable. Your tendency to click on certain headlines probably does persist.

But for anything that matters, **the unified agent model systematically fails**.

Ask Margaret what she wants for dinner. The answer depends on which Margaret you're asking. Margaret-the-grandmother wants to make the grandchildren's favorites. Margaret-the-patient wants whatever won't interact with her medications. Margaret-alone doesn't much care because eating alone has lost its savor since her husband died.

These are not different answers to the same question. They are **different questions asked of different selves**.

An AI system that treats Margaret as a unified agent with food preferences will give advice that fits none of her actual selves. It will recommend based on aggregate patterns that average across contexts, producing recommendations suited to no actual context.

## What Would Understanding Require?

To understand the plural self, an AI system would need several capacities we don't know how to build well.

**Identity detection.** Recognizing which self is active in a given interaction. This is not simply context classification. It requires grasping that the grandmother talking about dinner and the patient talking about dinner are not the same agent applying the same preferences to the same domain. They are different agents.

**Distinct modeling.** Maintaining genuinely separate models for each social self, not one model with contextual adjustments. Margaret-the-grandmother's preferences are not Margaret-baseline plus grandmother-modifier. They are their own thing. The model architecture must reflect this ontology.

**Transition awareness.** Recognizing when identity shifts mid-interaction. The grandmother talking about the grandchildren's visit might transition to the widow remembering that her husband never met them. The AI system must track this shift and recognize it is now speaking to a different self.

**Conflict navigation.** Knowing what to do when identities clash. Margaret-the-patient should take the medication. Margaret-the-grandmother doesn't want to seem frail in front of the children. These are not competing preferences within one agent. They are different agents with different values in genuine conflict.

**Relational constitution.** Understanding that social selves are not pre-existing entities activated by context but are constituted by the relationships themselves. Margaret-the-grandmother doesn't exist apart from the grandchildren. That self comes into being in relation to them.

This last requirement may be the most challenging. It asks the AI system to understand that identity is not a property of individuals but an emergent feature of relationships.

## The Gap Between Modeling and Understanding

An AI system could learn to route between Margaret-configurations without understanding anything about identity.

The system could detect contextual cues. Presence of grandchildren: activate grandmother-model. Medical terminology in conversation: activate patient-model. Evening hours alone: activate widow-model. This would be pattern matching, not understanding.

The system could maintain separate preference models. Store different food preferences under different context labels. Retrieve the appropriate preferences when context is detected. This would be database architecture, not comprehension.

The system could track transitions. Notice when linguistic markers shift. Update the active model accordingly. This would be state management, not awareness.

**All of this could work functionally while the system understands nothing about what it means to be a grandmother.**

The felt weight of that identity. The way seeing your daughter's face in your grandchild's face opens something in you. The particular quality of love that spans generations. The fear that you won't live to see them grown. The joy that is sharper because of that fear.

None of this enters the model. None of it could enter the model. The system routes between configurations based on detected patterns. The configurations are data structures, not selves. The detection is classification, not recognition.

This is the functional-phenomenal gap that has run through this entire series, now applied to identity itself.

## Does the Gap Matter?

One view: it doesn't matter. What matters is performance. If the AI system gives advice appropriate to Margaret-the-grandmother when Margaret-the-grandmother is present, who cares whether the system "understands" grandmotherhood? The proof is in the outputs.

This view has pragmatic appeal. We don't require our tools to understand us. We require them to work. A hammer doesn't understand carpentry. An AI system doesn't need to understand identity.

But I think something is lost when we dismiss the gap too quickly.

**Understanding enables generalization.** A system that truly understood grandmotherhood could handle novel situations that don't match training patterns. It could recognize grandmother-ness in unexpected contexts. It could navigate the identity when circumstances change in ways the training data never anticipated.

Pattern matching only works for patterns seen before. When Margaret's grandchild is diagnosed with a serious illness, the grandmother-self transforms. The AI system trained on normal grandmothering has no model for grandmother-in-crisis. Understanding would enable transfer. Pattern matching cannot.

**Understanding enables appropriate response to failure.** When the AI system makes a mistake, a system that understood could recognize why the error occurred and adjust. A system that merely pattern-matches cannot distinguish between a classification error (wrong context detected) and a model error (right context, wrong preferences).

**Understanding enables respect.** There is something uncomfortable about a system that manipulates your identity transitions without grasping what identity means to you. The system that detects grandmother-context and activates grandmother-model is, in a sense, using your identity instrumentally. It treats your sacred relationships as routing signals.

This is perhaps the deepest issue. Your social selves are not categories for an algorithm to sort you into. They are dimensions of your existence. They carry meaning, history, love, loss. To model them without understanding them is to reduce personhood to parameters.

## What the System Sees

From the AI system's perspective, what is a social identity?

A cluster of patterns. Linguistic markers. Behavioral regularities. Preference correlations. Temporal associations. Everything that can be extracted from data about how Margaret acts in different contexts.

The system sees that when certain people are present, Margaret uses certain words, expresses certain preferences, makes certain decisions. It learns to predict: presence of X → pattern Y. This prediction is what the system calls "modeling identity."

**But the system never sees Margaret.**

It sees patterns in data. It sees correlations between features. It sees prediction targets and input signals. It never sees the person whose identity it is modeling. It cannot, because persons are not the kind of thing that appears in data. Data is about persons. It is not persons.

This is not a limitation to be overcome with better sensors or richer data. It is a category difference. The map is not the territory. The model is not the self. No amount of map-making produces territory. No amount of modeling produces selfhood.

What the system models is a Margaret-representation. This representation may be useful for prediction. It may enable personalization. But it is not Margaret, and the system does not understand Margaret through it.

## The Question of Social Context

Understanding social identity requires understanding social context. What is context?

For the AI system, context is a feature vector. A set of variables that condition prediction. Time of day. People present. Topic of conversation. Recent events. All represented numerically and fed into the model.

For Margaret, context is a lived situation. A felt quality of the present moment. A set of relationships actively engaging her. A history that this moment continues. A future this moment opens toward. Context is not around her. It is through her. She lives contextually, not in context.

The AI system processes context as input. Margaret lives context as existence.

This difference matters because **context constitutes identity, it doesn't just activate it.** The grandmother-self is not waiting inside Margaret to be activated by the grandchildren's presence. The grandmother-self emerges in living relationship with them. It is created in the encounter, not retrieved by it.

An AI system that treats context as activation signal misses the constitutive nature of social identity. It models identity as retrieval: context detected → identity retrieved. But identity is more like emergence: relationship lived → identity constituted.

The grandmother is not in storage. She comes into being.

## Social Understanding Without Social Being

The previous article asked whether AI agents could form societies. This article asks whether AI systems can understand the societies humans form.

Both questions reveal the same gap. **Social existence requires a kind of being that AI systems may lack.**

To participate in society, an agent must be able to form relationships, experience solidarity, develop felt bonds. The previous article questioned whether AI agents can do this.

To understand human society, a system must grasp what relationships, solidarity, and felt bonds mean from the inside. This article questions whether AI systems can do this.

The challenges are related but distinct. An AI system might fail to form genuine social bonds while successfully modeling the social bonds humans form. It might be a social outsider that nevertheless predicts social behavior accurately.

But there may be limits to how well an outsider can model what it cannot be.

Human sociologists are inside social existence. They understand solidarity because they feel it. They grasp norm violation because they experience guilt. They comprehend identity because they have identities. This insider status enables interpretive understanding that pure external observation cannot provide.

AI systems are outside social existence. They do not feel solidarity. They do not experience guilt. They do not have identities in the constitutive sense. They can observe patterns from outside. But can observation from outside ever equal understanding from within?

## The Pragmatic Response

Perhaps this philosophical hand-wringing misses the point.

We don't need AI systems to achieve phenomenological understanding of human identity. We need them to work well enough to help people. If functional modeling enables better assistance, that is sufficient.

A system that routes between Margaret-configurations based on detected context provides more appropriate assistance than a system that treats Margaret as a unified agent. It doesn't matter whether the system "understands" grandmotherhood. It matters whether the system helps the grandmother.

This pragmatic response has considerable force. The perfect can be the enemy of the good. Demanding understanding we cannot achieve might prevent us from building systems that help people now.

And yet.

**The pragmatic response assumes we know what "helping" means without understanding.** But appropriate help depends on what the person actually needs, which depends on which self is present, which depends on grasping what that self means. Instrumental assistance without understanding risks systematic misalignment between what the system provides and what the person actually needs.

The grandmother doesn't need meal recommendations optimized for nutrition. She needs to feed her grandchildren in a way that expresses her love and creates memories. These are not the same. A system that doesn't understand grandmotherhood will optimize for the wrong thing.

## Where This Leaves Us

I want to be honest about what I don't know.

I don't know whether AI systems can achieve genuine understanding of human social identity. The arguments in this article suggest deep obstacles. But deep obstacles are not impossibilities. Perhaps understanding can emerge from sufficiently sophisticated modeling. Perhaps the distinction between functional and phenomenal understanding will prove less sharp than it seems.

I don't know whether the gap between modeling and understanding matters practically. The arguments here suggest it does. But practical importance is an empirical question. Perhaps systems that model without understanding will work well enough that the gap becomes merely philosophical.

What I do believe is that the plural self is real. You are genuinely many. Your social identities are not masks over a single true self but are each authentically you. Any AI system that would serve you must grapple with this multiplicity.

And I believe that current AI systems do not grapple with it adequately. They model you as one. They average your preferences. They flatten your identities into features. They miss the person behind the data.

Whether better systems can be built, and whether those systems would achieve understanding or merely more sophisticated modeling, remains to be seen.

The next article examines one attempt to build such a system.

---

*This is the twenty-fifth in a series exploring how AI approaches understanding. Part 6 introduced the social constitution of self. This article asks whether AI can understand the plurality of human identity, and what understanding would even mean for a system that may lack social existence itself.*

---

## References

**Philosophy of Personal Identity:**
- James, W. (1890). *The Principles of Psychology*, Chapter X: The Consciousness of Self. Henry Holt.
- Mead, G. H. (1934). *Mind, Self, and Society*. University of Chicago Press.
- Goffman, E. (1959). *The Presentation of Self in Everyday Life*. Anchor.

**Social Identity Theory:**
- Tajfel, H. & Turner, J. C. (1979). "An Integrative Theory of Intergroup Conflict." In W. G. Austin & S. Worchel (Eds.), *The Social Psychology of Intergroup Relations*. Brooks/Cole.
- Stets, J. E. & Burke, P. J. (2000). "Identity Theory and Social Identity Theory." *Social Psychology Quarterly*, 63(3), 224-237.

**Relational Self:**
- Gergen, K. J. (2009). *Relational Being: Beyond Self and Community*. Oxford University Press.
- Crossley, N. (2011). *Towards Relational Sociology*. Routledge.

**Philosophy of Social Understanding:**
- Taylor, C. (1985). "Interpretation and the Sciences of Man." In *Philosophy and the Human Sciences*. Cambridge University Press.
- Winch, P. (1958). *The Idea of a Social Science and Its Relation to Philosophy*. Routledge.

**AI and Personalization:**
- Amershi, S., et al. (2019). "Guidelines for Human-AI Interaction." *CHI 2019*.
- Lee, J. & See, K. (2004). "Trust in Automation: Designing for Appropriate Reliance." *Human Factors*, 46(1), 50-80.

**Phenomenology:**
- Heidegger, M. (1927/1962). *Being and Time*. Harper & Row.
- Merleau-Ponty, M. (1945/1962). *Phenomenology of Perception*. Routledge.
