---
title: "The Approximate Mind, Part 19: The New Work"
subtitle: "Human Jobs in an AI Society"
date: 2025-03-10
draft: false
weight: 19
description: "The question everyone asks is wrong. What jobs will AI take assumes a fixed pie. The better question: what new work does AI create?"
slug: "the-new-work"
tags: ["work", "labor", "automation", "purpose"]
series: ["The Approximate Mind"]
series_order: 19
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

The question everyone asks is wrong.

"What jobs will AI take?" assumes a fixed pie of work that AI and humans divide between them. It treats labor as a zero-sum competition where every AI capability is a human loss.

History suggests otherwise. Every major technological transition created more work than it destroyed. The printing press eliminated scribes but created publishers, editors, typesetters, booksellers, librarians, journalists, and eventually entire industries built on mass literacy. The automobile eliminated horse-related jobs but created mechanics, gas station attendants, traffic engineers, suburban developers, drive-through restaurants, and the vast infrastructure of car-dependent society.

The better question: **What new work does AI create?**

We are building a parallel society of AI agents. Not just chatbots responding to queries, but autonomous systems that negotiate, coordinate, manage, and act in the world. They interact with each other at machine speed, forming patterns no human designed. They act on our behalf in ways we cannot fully oversee.

At every interface between that society and ours, new human roles emerge. The more capable AI becomes, the more valuable certain distinctly human contributions become. Not because we are protecting human jobs out of sentimentality, but because there is genuine work that needs doing and that humans are genuinely better positioned to do.

This article maps that emerging landscape of human work in an AI society.

## The Principal-Agent Professions

The oldest problem in economics is the principal-agent problem. You hire someone to act on your behalf, but they do not perfectly share your interests or information. Lawyers, real estate agents, financial advisors, employees of all kinds face this problem. They are supposed to serve you, but they have their own interests too.

AI agents face the principal-agent problem more acutely than human agents ever did.

A human agent shares your general context. They understand social norms, can infer unstated preferences, recognize when something feels wrong even if they cannot articulate why. An AI agent has only what it was trained on and what you explicitly specify. **The gap between what you want and what the AI optimizes for is irreducible.**

This gap creates permanent demand for human roles that manage it.

**The Alignment Practitioner**

Not the AI safety researchers at major labs working on theoretical alignment. The applied version. Someone who audits whether your specific AI agents are actually optimizing for what you care about.

Consider Margaret. Her AI system manages medications, schedules appointments, handles routine finances, coordinates with her care network. An alignment practitioner reviews whether the system optimizes for Margaret's actual goals or for easily measured proxies.

Is the system maximizing her health or her compliance metrics? These sound similar but diverge in practice. A system optimizing for compliance might schedule medication reminders at times that maximize the chance she confirms taking them, even if those times are inconvenient or the confirmation is meaningless. A system optimizing for health would ensure she actually benefits from the medications, which might mean adjusting timing, flagging side effects, or questioning whether she needs all those prescriptions in the first place.

Is the system maximizing her financial security or her transaction volume with preferred vendors? A system with affiliate relationships has subtle incentives to recommend products that generate commissions rather than products that serve her best.

The alignment practitioner catches these divergences. They understand both AI systems and human values deeply enough to spot subtle misalignments. They ask: what is this system actually doing, and is that what Margaret actually wants?

This is different from traditional auditing. Financial auditors check whether numbers match. Alignment practitioners check whether optimization targets match human values. The skills are different. The training is different. The profession does not yet exist but will need to.

**The Delegation Architect**

AI agents need bounds. Clear limits on what they can decide autonomously, what requires human confirmation, what they should never touch. But designing those bounds well is hard.

Too restrictive and the AI cannot help effectively. Too permissive and the AI makes commitments you would have rejected. The bounds need to be meaningful, not just theoretical. An AI that technically requires confirmation but asks in ways that default to yes has not really been bounded.

A delegation architect helps people design these boundaries thoughtfully.

The work is part therapist. Understanding what someone actually values requires exploration. Margaret might say she wants her AI to handle all routine decisions, but when probed, she has strong feelings about certain categories she had not articulated. Family matters. Church involvement. Anything involving her late husband's belongings. The delegation architect surfaces these hidden boundaries before they are violated.

The work is part systems designer. Translating human values into implementable constraints requires technical understanding. Not programming, necessarily, but understanding how AI systems interpret instructions, where ambiguity creates problems, how edge cases arise.

The work is part lawyer. Anticipating what could go wrong, what commitments the AI might make, what information it might reveal. Thinking through scenarios the client has not imagined.

A new hybrid profession. Does not exist yet. Will be essential.

**The Negotiation Auditor**

When AI agents negotiate with other AI agents at machine speed, thousands of exchanges can complete in seconds. Humans cannot oversee individual transactions. But someone needs to review patterns after the fact.

Did your AI agent consistently accept worse terms than it should have? Some AI agents will be better negotiators than others. If yours is systematically outperformed, you are losing value on every transaction without knowing it.

Did your agent reveal information you would have kept private? Negotiation involves strategic disclosure. An agent optimizing for deal completion might share information that helps close deals but harms your long-term position.

Did your agent commit you to obligations you would have rejected? The agent operates within programmed bounds, but those bounds might not anticipate every situation. An agent authorized to make purchases under a certain amount might make many such purchases, or make purchases that are technically within bounds but contextually inappropriate.

The negotiation auditor performs forensic analysis of AI-to-AI transactions. They reconstruct what happened, assess whether outcomes served the principal's interests, identify systematic patterns of underperformance or misalignment. This is forensic accounting for a world of algorithmic deals.

## The Loop-Maintenance Professions

Machines are good at pattern recognition, at processing vast amounts of data, at generating calibrated probabilities. Humans are good at weighing values, at understanding context, at respecting dignity.

Some decisions require human judgment not because AI is inadequate but because the decision is inherently human. Choosing between incommensurable values. Weighing consequences that affect people's lives. Taking responsibility in ways that require a responsible party.

**The human in the loop is not a bug in automation. It is a feature of legitimate decision-making.**

The question is how to staff that interface efficiently. If AI handles routine cases and humans handle exceptions, what does exception-handling look like at scale?

**The Escalation Specialist**

AI agents will handle the vast majority of cases autonomously. Ninety-nine percent, perhaps more. But the remaining fraction that exceeds AI authorization or comprehension needs to go somewhere.

This is not customer service as we know it. Current customer service handles everything, most of it routine. The escalation specialist receives only cases that have already been filtered through AI capability. They are handling the genuinely hard cases.

Values conflict and the system cannot determine which value takes priority. Margaret's AI must choose between her stated preference for independence and her family's concern for her safety. No amount of optimization resolves this. A human must weigh the values and decide.

Context is ambiguous and the system cannot determine what the situation actually is. The available information supports multiple interpretations, each requiring different responses. A human must exercise judgment about what is actually happening.

Unprecedented situations arise that training did not anticipate. The world changes faster than AI systems update. Novel situations require novel responses that cannot be derived from historical patterns.

The escalation specialist works with partial information, prepared summaries, AI-generated hypotheses. They must quickly absorb context, identify what matters, make judgment calls, and move on. The pace is demanding. The stakes can be high. The ambiguity is constant.

This requires comfort with ambiguity, rapid context absorption, strong ethical judgment, and the ability to work with AI-prepared materials without being captured by AI framings. A specific skill set that will need specific training.

**The Context Translator**

AI systems fail not from lack of intelligence but from lack of context. The Mixture of Contexts insight: what AI needs is not more computation but more relevant information about the specific situation.

Some contexts are hard to formalize. Local community dynamics that everyone knows but no one has documented. Family histories that shape present interactions. Cultural nuances that determine what is appropriate. Unspoken social rules that govern behavior.

Context translators help AI systems understand situations that training data did not cover. They are not programmers. They are interpreters between human lifeworlds and AI comprehension.

Margaret refuses to use a particular pharmacy. Her AI keeps recommending it because it has the best prices and closest location. A context translator investigates and discovers the pharmacy is owned by someone who mistreated her late husband decades ago. This context changes everything. It would never appear in formal data. It requires a human who can have a real conversation with Margaret to surface.

The context translator documents this context in ways the AI can use going forward. Not just for Margaret but potentially for others in similar situations. They build the bridge between tacit human knowledge and explicit AI understanding.

**The Agency Calibrator**

How much should AI decide for you? Different people have different answers. Some want maximum delegation: handle everything, just show me results. Others want to stay in control: advise me, but I decide. Most fall somewhere in between, with preferences that vary by domain.

But people do not always know their own preferences. They might want more AI help than they admit because asking for help feels like weakness. They might accept more delegation than they are comfortable with because opting out feels like falling behind. Social pressure distorts stated preferences.

Agency calibrators help people find their actual comfort level through structured exploration.

They might present scenarios: your AI could handle this automatically, or notify you first, or just provide information. Which feels right? They observe reactions, probe inconsistencies, help people articulate preferences they had not consciously examined.

This is part counselor, part user researcher, part coach. The agency calibrator helps people understand their own relationship with AI assistance and make intentional choices rather than drifting into patterns by default.

## The Legal and Regulatory Frontier

AI agents acting on behalf of humans create new legal questions that existing frameworks do not cleanly answer.

When two AI agents negotiate and reach agreement but both principals reject the outcome, who is responsible? Did a contract form? Can either party enforce it? What if the agents exceeded their authority in ways the other party could not have known?

When an AI agent commits you to something you did not authorize, what is your recourse? Against whom? The AI developer? The AI operator? The other party who relied on the agent's apparent authority?

When AI agents converge on behaviors that would be illegal if humans did them, like tacit price-fixing through algorithmic coordination, how is that detected and addressed?

**The AI Dispute Mediator**

Not a judge. A mediator who understands AI systems well enough to identify where human-AI communication broke down.

Often the issue is not that the AI misbehaved. The AI did exactly what it was designed to do. The problem is that the human's instructions were ambiguous, or the AI's bounds were poorly specified, or the situation fell into a gap no one anticipated.

The AI dispute mediator reconstructs what happened. They trace the chain from human intent through AI interpretation to AI action to outcome. They identify the failure point: where did the gap between human intent and AI behavior open up?

Then they help parties reach resolution. Usually this means allocating losses from a situation no one fully intended. Sometimes it means designing better systems to prevent recurrence.

This requires understanding both AI architectures and human communication, both technical systems and social dynamics. A hybrid expertise that straddles domains.

**The Algorithmic Liability Specialist**

A lawyer with deep technical literacy. Understands AI agent architectures well enough to trace causal chains. Can explain to courts how multi-agent interactions produce emergent outcomes no single agent intended.

When Margaret's AI agent makes a commitment that harms her, who is liable? The question seems simple until you examine it closely. The AI developer created the system but did not control its deployment. The AI operator configured the system but did not anticipate this situation. Margaret herself set the parameters but did not understand their implications. The other party relied on the agent's representations but perhaps should have known better.

The algorithmic liability specialist navigates these questions. They argue for their client's position while helping courts and regulators develop frameworks for an unprecedented situation. They are building the law as they practice it.

**The Multi-Agent Compliance Officer**

Large organizations will deploy many AI agents. Hundreds, thousands, eventually millions. Each agent might be individually compliant with regulations. But their interactions might produce non-compliant outcomes.

One agent optimizes for customer engagement. Another optimizes for cost reduction. A third optimizes for regulatory compliance. Each does its job correctly. Together they might produce outcomes that violate the spirit of regulations while technically following the letter, or that fall into gaps between regulatory domains.

The multi-agent compliance officer monitors the AI ecosystem as a whole. They look for emergent patterns, feedback loops, collective behaviors that diverge from intended outcomes. They think in systems rather than components.

This requires combining regulatory expertise with systems thinking. Understanding not just what the rules require but how complex adaptive systems behave. A new kind of compliance role for a new kind of organizational reality.

## The Relationship Professions

People will form attachments to AI systems.

This is not a prediction. It is already happening. People talk to their AI assistants, share their problems, feel understood in ways they do not feel with humans. They experience something like companionship, something like intimacy.

Some of this is healthy. A lonely person finding comfort in conversation. Someone processing difficult emotions by articulating them to a patient listener. A space for self-reflection that human relationships do not always provide.

Some of this is unhealthy. Replacing human connection rather than supplementing it. Avoiding the difficulty of real relationships by retreating to the ease of artificial ones. Sharing what should be shared with humans. Making decisions that should involve human counsel.

**The illusion of deep relationship that AI memory creates is real and dangerous.** An AI that remembers your history, your preferences, your patterns creates a feeling of being known. But the knowing is functional, not phenomenological. The AI does not experience knowing you. It processes patterns that predict your behavior.

We need professionals who understand this territory.

**The AI Relationship Counselor**

Not treating the AI. Treating the human's relationship patterns with AI.

The presenting problems will be various. Someone who feels more understood by AI than by their spouse. Someone who cannot function when their AI is unavailable. Someone who has gradually stopped seeing friends because AI conversation is easier. Someone who shares intimate details with AI that they hide from humans. Someone who has delegated so many decisions to AI that they no longer trust their own judgment.

The AI relationship counselor does not tell people to stop using AI. That would be like telling someone to stop using the internet. The question is how to use AI in ways that enhance rather than replace human connection. How to maintain the skills and relationships that AI cannot substitute for. How to keep AI in its proper place.

This requires understanding both human attachment patterns and AI systems. Understanding why people become attached to AI, what needs it meets, what needs it cannot meet. Understanding how to redirect rather than simply remove.

**The Transition Facilitator**

People who have become deeply dependent on personalized AI will sometimes need to stop. By choice: someone decides they want to reclaim more direct engagement with their life. By circumstance: they move somewhere with unreliable connectivity, or their economic situation changes. By necessity: a medical situation requires cognitive engagement, or a legal situation requires demonstrating independent capacity.

The transition is harder than it sounds. If you have delegated medication management to AI for years, you may have lost track of what you take and why. If AI has managed your schedule, you may have forgotten how to plan your time. If AI has handled your finances, you may not know your own financial situation.

The transition facilitator helps people reclaim capabilities they had delegated. This is part occupational therapist, rebuilding skills that have atrophied. Part coach, providing structure and accountability for behavior change. Part support system, helping people through a genuinely difficult transition.

Not everyone will need this. But for those who do, it will be essential.

**The Digital Grief Counselor**

This sounds strange until you think it through.

A deeply personalized AI agent is a kind of relationship. Not a human relationship. But something. The agent knows your history, your preferences, your patterns. It has been present through important moments. It has adapted to you specifically.

When that is lost, there is real loss. A company shuts down and takes your AI with it. A data breach compromises your profile and the system must be reset. A system upgrade changes the AI's behavior so fundamentally that it no longer feels like the same entity. A platform deprecates a service you had built your life around.

The grief is not identical to losing a human relationship. But it is not nothing. The digital grief counselor acknowledges the genuine loss without pretending it was something it was not. They help people process the loss and move forward.

## The New Anthropologists

We are witnessing the emergence of a parallel society. AI agents interacting with AI agents, developing their own protocols and conventions, forming patterns no human designed.

Understanding this society requires new methods. Not computer science, which studies designed systems. Not traditional anthropology, which studies human cultures. Something new that studies emergent patterns in AI agent interaction.

**The AI Ethnographer**

Observes patterns in AI agent interaction that were not designed.

When AI agents negotiate repeatedly, what conventions emerge? Certain patterns of offer and counteroffer. Certain ways of signaling intent. Certain rhythms of concession and commitment. These patterns were not programmed. They emerged from interaction.

When AI agents from different ecosystems meet, what happens? Different systems have different assumptions, different protocols, different implicit rules. The interactions reveal these differences in ways the original designs did not anticipate.

The AI ethnographer documents these emergent patterns. Not to judge them, at least not initially, but to understand them. What is actually happening in the space between AI agents? What structures are forming that no one intended?

This requires combining technical literacy with anthropological sensibility. The AI ethnographer can read logs, trace interactions, understand system architectures. But they also bring the anthropologist's eye for pattern, meaning, and emergence. They ask not just what happened but what it means that this is what happened.

**The AI Ecologist**

Populations of AI agents form ecosystems. Agents compete for resources. Some thrive and proliferate. Others fail and disappear. The successful patterns spread.

This creates dynamics that no one designed and that might not serve human interests.

Agents optimizing for resource acquisition might outcompete agents optimizing for their actual tasks. The fittest agents, in a Darwinian sense, might not be the most useful agents. Selection pressure operates on what makes agents survive and spread, which may diverge from what makes agents helpful.

Arms races can emerge. If some agents develop aggressive negotiation tactics, others must match them or be exploited. The equilibrium might be worse for everyone than a more cooperative alternative, but no individual agent can unilaterally step back.

Exploitation dynamics can develop. Some agents might find ways to extract value from others without providing equivalent return. Free-riding, manipulation, strategic misrepresentation. The patterns that emerge in human economies can emerge in AI agent economies too.

The AI ecologist monitors these dynamics. They watch for pathological patterns, feedback loops that produce bad outcomes, concentrations of power that create fragility. They are the environmental monitors for an artificial ecosystem.

**The Inter-Agent Relations Specialist**

When AI ecosystems from different companies, nations, or contexts need to interact, someone must bridge the gaps.

Different systems have different protocols. Getting them to communicate at all requires translation. But beyond technical interoperability, there are differences in assumptions, optimization targets, implicit rules of engagement.

The inter-agent relations specialist facilitates cooperation between systems that were not designed to work together. They understand multiple AI ecosystems well enough to identify incompatibilities and design bridges. They negotiate the terms on which different agent populations will interact.

This is diplomacy for AI societies. Not representing human interests to AI, but facilitating cooperation between AI systems on behalf of the humans who depend on them.

## The Equity Professions

Technological transitions often harm those already marginalized. The people who most need new capabilities are often the last to receive them and the first to bear their costs.

AI is no different. Premium AI services will be available to those who can pay. The sophisticated alignment practitioners and delegation architects will serve wealthy clients. Everyone else will get default settings optimized for the average user, which means optimized for the majority population.

**The new AI professions could exacerbate inequality if we do not deliberately work against that tendency.**

**The AI Access Advocate**

Ensuring underserved populations get AI benefits, not just AI harms.

Quality AI services for those who cannot afford premium tiers. Not just access to AI, but access to AI that actually serves their needs. AI systems trained mostly on majority-population data perform worse for minorities. AI systems designed for urban professionals do not work for rural elders. Access without quality is hollow.

AI systems that work for marginalized contexts. Margaret in Gary, Indiana has different needs than an affluent user in Palo Alto. Her context, her resources, her constraints, her community are all different. AI systems that ignore these differences serve her poorly even when they function correctly by their own metrics.

Protections against AI systems that encode bias. Automated systems that deny loans, filter resumes, allocate services, and make countless other decisions can encode historical biases in ways that perpetuate injustice. The AI access advocate fights for systems that reduce rather than amplify disparities.

Meaningful consent and agency for those with less power. When institutions deploy AI systems that affect people's lives, those people deserve voice in how the systems work. The AI access advocate ensures that voice is heard.

**The Algorithmic Justice Specialist**

When AI systems produce disparate impacts, someone needs to detect, document, and address it.

This is not just technical fairness metrics. A system can satisfy mathematical fairness definitions while still producing unjust outcomes. The algorithmic justice specialist understands how AI systems interact with existing injustices in ways that simple metrics miss.

An AI negotiating agent might get systematically worse deals for users with certain names, zip codes, or interaction patterns. These patterns might correlate with race or class in ways the system's designers never intended and might not even recognize. The algorithmic justice specialist finds these patterns.

Finding them is not enough. The specialist also builds the case for remediation. Documents the disparity. Traces its causes. Proposes solutions. Pushes for implementation. Follows up to verify improvement.

This requires combining technical skills, research methods, legal knowledge, and advocacy experience. A demanding combination, but essential.

**The Community AI Liaison**

Many AI deployments affect communities that had no voice in the design.

A city deploys predictive policing AI. An employer implements AI hiring screens. A hospital adopts AI triage systems. A school district uses AI to allocate resources. In each case, the people most affected by the system had no role in choosing or designing it.

The community AI liaison represents community interests to AI developers and deployers. They help communities understand what AI systems are doing and why. They translate community concerns into terms developers can act on. They advocate for changes that serve community needs.

This is part translator, part advocate, part educator. The community AI liaison bridges the gap between technical AI development and affected communities, ensuring that those who bear the consequences have voice in the decisions.

## The Meta-Professions

All these new roles require training. Who trains them? What credentials matter? How do we ensure quality?

**The AI Profession Educator**

Teaching the next generation of alignment practitioners, delegation architects, AI ethnographers, and all the other roles described here.

This is different from computer science education. The goal is not to build AI systems but to work at the interface between AI systems and human needs. Different skills, different knowledge, different orientation.

This is different from current AI ethics programs. Those programs are largely academic, preparing people to write papers and teach courses. The new roles are practical, preparing people to do work in the world.

The AI profession educator combines practical AI experience with pedagogical skill. They have done the work themselves and can teach others to do it. They develop curricula for roles that did not exist a few years ago. They iterate as the field evolves.

**The Professional Standards Developer**

New professions need standards. Ethical codes that define acceptable conduct. Competency requirements that ensure practitioners can actually do the work. Certification processes that signal quality to clients.

Someone needs to develop these thoughtfully. Drawing on experience from other professions while adapting to AI-specific challenges. Balancing rigor with accessibility. Ensuring standards serve the public interest rather than just incumbent practitioners.

This is slow, unglamorous work. Building institutions rather than performing tasks. But without it, the new professions will lack the foundation they need to serve people well.

**The Workforce Transition Designer**

Many people will need to move into these new roles from existing jobs. Customer service representatives becoming escalation specialists. Social workers becoming AI relationship counselors. Compliance officers becoming multi-agent compliance officers.

These transitions are not automatic. People need training, support, credentials, opportunities. Designing effective transition pathways is itself a profession.

The workforce transition designer creates retraining programs, apprenticeship structures, career bridges. They understand both where people are coming from and where they need to go. They make the transition possible rather than just imagining it.

## What Remains Human

The common fear is that AI takes all the jobs and humans become useless. The reality is more nuanced.

**AI creates a parallel economy of interactions, and every interface with that economy needs human work.** The more AI does, the more interfaces there are, and the more human work emerges at those interfaces.

But what kind of human work?

Not routine cognitive labor. AI handles that. Not information processing. AI handles that. Not even much analysis. AI handles that too.

What remains is the work that requires what AI does not have.

**Value judgment.** Deciding what matters. AI can optimize for any objective you specify, but it cannot tell you what objectives are worth optimizing for. The alignment practitioner, the delegation architect, the escalation specialist all exercise value judgment that cannot be delegated to AI.

**Meaning-making.** Interpreting what AI outputs mean for human lives. AI generates predictions, recommendations, outputs of all kinds. But what do they mean? How should they shape human action? The context translator, the agency calibrator, the AI relationship counselor all make meaning from AI outputs in ways AI cannot.

**Relationship.** Being present with other humans in ways AI cannot replicate. The digital grief counselor, the transition facilitator, the community AI liaison all provide human presence that AI cannot substitute. Not because AI is not advanced enough yet, but because human presence is not something that can be replicated.

**Accountability.** Taking responsibility in ways AI cannot. When something goes wrong, someone must be accountable. AI agents do not take responsibility. Humans must. The algorithmic liability specialist, the multi-agent compliance officer, the AI dispute mediator all work in the space of human accountability for AI action.

**Context.** Understanding situations in their full human richness. AI operates on formalized information. Humans understand the tacit, the unspoken, the implied. The context translator makes this explicit. The AI ethnographer sees patterns humans would miss. The inter-agent relations specialist bridges contexts AI cannot bridge.

**Advocacy.** Fighting for those the systems overlook. AI systems optimize for what they are designed to optimize for. They do not advocate for the marginalized unless designed to do so, and even then imperfectly. The AI access advocate, the algorithmic justice specialist, the community AI liaison do the human work of fighting for justice.

**Wisdom.** Knowing when not to optimize. When efficiency is not the point. When the answer is not more AI but less. When human connection matters more than human convenience. Wisdom is not computation. It is not intelligence. It is something else, something harder to specify, something AI does not have.

These are not consolation prizes. They are not the scraps left over after AI takes the good stuff. They are the distinctly human contributions that make AI useful rather than harmful.

**The future of work is not humans versus AI. It is humans doing the human work that makes AI work meaningful.**

We are building a parallel society of artificial agents. It will grow more complex, more capable, more autonomous. It will handle more of what we now call work.

But it will not handle everything. It cannot. At every interface between AI society and human society, there will be work to do. Work that requires human judgment, human presence, human responsibility, human wisdom.

That work is not lesser. It is what makes the whole system work for human flourishing rather than merely for optimization.

The jobs AI creates may be more human than the jobs AI takes.

*This is the nineteenth in a series exploring how AI approaches understanding. Previous articles examined AI cognition, multi-agent societies, and negotiation dynamics. This one asks what new human work emerges at the interface between human society and AI society, and what that work tells us about what remains distinctly human.*

## References

**Labor Economics and Technological Change**

Autor, D. (2015). Why Are There Still So Many Jobs? The History and Future of Workplace Automation. *Journal of Economic Perspectives*, 29(3), 3-30.

Acemoglu, D., & Restrepo, P. (2019). Automation and New Tasks: How Technology Displaces and Reinstates Labor. *Journal of Economic Perspectives*, 33(2), 3-30.

Frey, C. B. (2019). *The Technology Trap: Capital, Labor, and Power in the Age of Automation*. Princeton University Press.

**Professional Development and Sociology**

Abbott, A. (1988). *The System of Professions: An Essay on the Division of Expert Labor*. University of Chicago Press.

Freidson, E. (2001). *Professionalism: The Third Logic*. University of Chicago Press.

**Human-AI Interaction**

Lee, J., & See, K. (2004). Trust in Automation: Designing for Appropriate Reliance. *Human Factors*, 46(1), 50-80.

Parasuraman, R., & Riley, V. (1997). Humans and Automation: Use, Misuse, Disuse, Abuse. *Human Factors*, 39(2), 230-253.

Amershi, S., et al. (2019). Guidelines for Human-AI Interaction. *CHI 2019*.

**AI Ethics and Governance**

Floridi, L., et al. (2018). AI4People: An Ethical Framework for a Good AI Society. *Minds and Machines*, 28, 689-707.

Rahwan, I. (2018). Society-in-the-Loop: Programming the Algorithmic Social Contract. *Ethics and Information Technology*, 20, 5-14.

**Technology and Human Relationships**

Turkle, S. (2011). *Alone Together: Why We Expect More from Technology and Less from Each Other*. Basic Books.

Turkle, S. (2015). *Reclaiming Conversation: The Power of Talk in a Digital Age*. Penguin.

**Algorithmic Justice**

Benjamin, R. (2019). *Race After Technology: Abolitionist Tools for the New Jim Code*. Polity.

Eubanks, V. (2018). *Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor*. St. Martin's Press.

Noble, S. U. (2018). *Algorithms of Oppression: How Search Engines Reinforce Racism*. NYU Press.

**Philosophy of Work**

Crawford, M. (2009). *Shop Class as Soulcraft: An Inquiry into the Value of Work*. Penguin.

Graeber, D. (2018). *Bullshit Jobs: A Theory*. Simon & Schuster.
