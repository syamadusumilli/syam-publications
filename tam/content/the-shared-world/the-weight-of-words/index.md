---
title: "The Approximate Mind, Part 32: The Weight of Words"
subtitle: "How Language Shapes Who We See"
date: 2025-04-24
draft: false
weight: 32
description: "The words we use to describe people shape whether we see them as people. AI systems inherit and amplify these patterns."
slug: "the-weight-of-words"
tags: ["language", "representation", "framing", "perception"]
series: ["The Approximate Mind"]
series_order: 32
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

The moment a person receives a diagnosis, language reshapes reality. "Dementia patient" is not the same person as "Eleanor." The label precedes the person into every room, every interaction, every assumption about capacity.

We are building AI systems that learn and reproduce language. What happens when that language carries stigma?

### The Architecture of Stigma

Words are not neutral containers. Consider the difference between "wandering" and "exploring." Between "non-compliant" and "declining." Between "aggressive" and "distressed." Each framing implies causation, assigns blame, and suggests response.

Erving Goffman described stigma as a process that produces **"spoiled identity"**, where "an individual who might have been received easily in ordinary social intercourse possesses a trait that can obtrude itself upon attention and turn those of us whom he meets away from him" (Goffman 5). Once marked by a label, the person becomes the condition. The grandmother disappears. The diagnosis remains.

Medical language developed for clinical efficiency. Chart shorthand saves time. Categories enable billing. Standardized terminology allows communication across institutions. But efficiency for whom? The chart becomes easier to read. The person becomes harder to see.

**The label does work in the world.** It determines room assignments, staffing ratios, activity restrictions, and conversational expectations. People speak differently to "dementia patients" than to "Eleanor." They expect less. They offer less. They see less.

### What AI Systems Learn

Language models train on corpora. Medical AI trains on medical records. Those records contain decades of stigmatizing language, deficit framing, and depersonalization.

The system does not know that "patient is combative" often reflects overwhelmed staff rather than aggressive intent. The system does not know that "refuses medication" might mean "was not asked in a way she could understand." The system sees patterns. It learns to reproduce them.

Safiya Umoja Noble documented how search algorithms encode and amplify social biases, producing what she calls **"algorithmic oppression"** where "marginalized groups face discrimination through search engine results" (Noble 4). The same dynamic applies to medical AI. Systems trained on biased language produce biased outputs. They do so at scale, with the authority of automation.

**Pattern without context becomes bias at scale.**

When an AI system generates a care recommendation, it draws on patterns learned from millions of records. If those records consistently describe people with dementia using deficit language, the system learns that deficit framing is appropriate. It reproduces what it learned. The stigma compounds.

### The Rhetoric of Capacity

Deficit language dominates how we discuss cognitive change. "She can't remember." "He doesn't understand." "They are unable to." The grammar itself positions the person as lacking.

But capacity is contextual and domain-specific. Margaret cannot recall what she ate for breakfast. She can recall every verse of hymns learned in childhood. Her procedural memory for cooking remains intact even as her episodic memory fails. Which capacity defines her?

Steven Sabat's research on selfhood in Alzheimer's disease demonstrates that **"the person is not lost"** even in advanced dementia, arguing that "the self of personal identity, the self that is constructed in social interaction, can remain intact" (Sabat 277). What we call "loss" is often loss of one type of memory while others remain. What we call "inability" is often inability in one context while competence persists in another.

AI systems optimize for what they are trained to see. **If they see deficits, they serve deficits.** They offer workarounds for weaknesses rather than scaffolding for strengths. They protect rather than enable.

The alternative requires granular modeling. Not "moderate dementia" as a global label but tier-based capacity across specific domains. Memory for names versus memory for music. Morning cognition versus afternoon cognition. Familiar contexts versus novel ones. The same person at different moments, different times, different days.

### Language as Intervention

Rhetoric does not just describe reality. It shapes response.

How you describe someone determines how you treat them. "Dementia victim" invites pity. "Person living with dementia" invites relationship. "Demented" as an adjective erases personhood entirely, reducing a human being to a diagnostic category.

Tom Kitwood's work identified what he called **"malignant social psychology"**, a set of practices in dementia care that "serve to depersonalize those who have dementia, in ways that are often not fully intentional" (Kitwood 46). These practices include infantilization, which involves treating adults as children. Labeling, which reduces identity to diagnosis. Invalidation, which dismisses expressed feelings as symptoms. Objectification, which treats persons as things to be managed.

Each practice has linguistic correlates. We speak to people with dementia as if they were children. We refer to "the dementia" as the subject of sentences where the person should be. We reframe expressed distress as "behavioral symptoms" requiring intervention. We discuss people in their presence as if they were not there.

**AI systems can either encode these patterns or challenge them.**

A system trained on care notes that use malignant social psychology will learn to reproduce it. A system deliberately designed to resist these patterns must be explicitly trained on alternative framings.

### The Stigma Feedback Loop

Consider how stigma compounds through AI systems.

A person receives a diagnosis. Clinical language enters their record. AI trains on records. AI generates outputs using clinical framings. Caregivers read AI outputs. Caregivers treat the person according to the framing. The framing becomes reality.

The loop closes. The stigma self-reinforces.

Bruce Link and Jo Phelan's conceptualization of stigma identifies multiple components: "labeling, stereotyping, separation, status loss, and discrimination" that occur together "in a power situation that allows them" (Link and Phelan 377). AI systems possess exactly this power situation. They label at scale. They stereotype by pattern. They separate through categorization. They confer status through risk scores and care levels.

Breaking the loop requires **intentional counter-framing**. Not euphemism, which denies reality. Not clinical detachment, which erases humanity. Accurate language that preserves complexity and dignity.

### What Liberation AI Requires

Building systems that resist stigmatizing patterns demands explicit design choices.

The intersectionality principle means refusing to reduce anyone to a single dimension. Eleanor is not "dementia patient." She is 82 years old, Chinese-American, a former teacher, grandmother, widow, Presbyterian, Democrat, jazz lover, gardener, and mother of two. Her cognitive changes intersect with all of this. **A system that sees only the diagnosis sees almost nothing.**

Systematic harm measurement asks whether the system's language creates barriers. Does it reduce trust? Does it diminish dignity? Does it perpetuate stereotypes? These become measurable outcomes, not just ethical aspirations.

Technical approaches exist. Language auditing can identify stigmatizing patterns in training data. Reframing protocols can transform deficit language to capacity language during generation. Human dignity constraints can filter outputs that reduce persons to conditions.

Kate Swaffer, a dementia advocate diagnosed with younger-onset dementia, argues that **"language is power"** and that "the words used to describe people with dementia often add to the stigma and discrimination we face" (Swaffer 711). She notes that terms like "sufferer" and "victim" position people with dementia as passive and helpless, ignoring their continuing agency and capacity.

### The Deeper Question

Can systems without experience understand what stigma feels like?

AI can learn to avoid certain words. It cannot feel the weight of being labeled. It can generate person-centered language. It cannot comprehend why that matters.

This is the approximation problem applied to rhetoric. **The system produces appropriate outputs without experiencing meaning.** It avoids stigmatizing language because training shaped its parameters, not because it grasped the harm such language causes.

Is that enough? Perhaps. If the outputs preserve dignity, if they support rather than diminish personhood, the absence of understanding may be acceptable. We do not require that elevators understand accessibility to provide it. We do not require that ramps comprehend mobility challenges.

But we should remain aware of the gap. A system that uses non-stigmatizing language because it was trained to is categorically different from a person who chooses non-stigmatizing language because they recognize shared humanity. The first is pattern matching. The second is ethical recognition.

The person who chooses carefully knows they could choose otherwise. They feel the weight of words because they know words carry weight. The system has no such knowledge. It produces outputs. The outputs have consequences the system cannot comprehend.

### The Metaphors We Live By

George Lakoff and Mark Johnson demonstrated that **"our ordinary conceptual system, in terms of which we both think and act, is fundamentally metaphorical in nature"** (Lakoff and Johnson 3). The metaphors we use for dementia shape how we think about it.

Consider dominant metaphors. Dementia as "theft" steals the person away. Dementia as "journey" moves toward a destination. Dementia as "battle" requires fighting and eventual defeat. Each metaphor implies a story, and stories shape response.

The theft metaphor positions the person as victim and the condition as criminal. It invites grief for what was taken. It offers no frame for engaging with who remains.

The journey metaphor implies direction and destination. But destination is death, making the journey a death march. It offers no frame for dwelling in the present.

The battle metaphor demands resistance. But the condition cannot be defeated. Framing it as battle sets up inevitable failure. It offers no frame for acceptance and adaptation.

**What metaphors would serve better?**

Perhaps dementia as "weather" that changes the landscape but does not erase it. The person remains. The conditions around them shift. Some days are clearer than others. Adaptation is possible.

Perhaps dementia as "translation" where experience continues but expression changes. The person has thoughts and feelings. Articulating them becomes harder. The listener must learn a new language.

Perhaps dementia as "tide" that ebbs and flows. Capacity recedes and returns. The shoreline changes shape. The ocean remains.

AI systems trained on one metaphorical frame will reproduce that frame. Changing the frame requires deliberate intervention in training, in prompting, in output filtering.

### Margaret's Words

Abstract principles need grounding in concrete experience.

Consider Margaret on a good morning. She jokes with her daughter. She remembers a story from decades ago in vivid detail. She expresses clear preferences about what she wants to wear, what she wants to eat, whom she wants to call.

Her chart says "moderate dementia, episodic memory impaired, requires assistance with ADLs."

Both are true. But **which truth should an AI system center?**

The chart is accurate and inadequate. It captures deficits while ignoring capacities. It notes what requires assistance while missing what requires no assistance at all. It describes a category while missing a person.

Margaret's morning self could engage in meaningful conversation, make real choices, experience genuine joy. The chart does not predict this. The chart predicts a "moderate dementia patient" who needs supervision and assistance.

An AI system trained on charts will see the chart-Margaret. It will offer chart-appropriate responses. Simple sentences. Limited options. Protective constraints.

An AI system designed to see Margaret will notice her morning clarity, her humor, her preferences, her capacity. It will offer Margaret-appropriate responses. Real conversation. Meaningful choices. Scaffolded autonomy.

### Implications for Design

How do we build systems that see persons rather than diagnoses?

**Language auditing** means systematic review of training data for stigmatizing patterns. Identify deficit framings. Flag dehumanizing terminology. Measure the ratio of capacity language to incapacity language. Training data shapes model behavior. Biased training produces biased outputs.

**Reframing protocols** mean active transformation during generation. When the model produces "patient refuses," transform to "patient declines." When it produces "aggressive behavior," transform to "distress response." Not euphemism but accuracy. "Refuses" implies willful resistance. "Declines" describes choice. "Aggressive" implies intent to harm. "Distress" describes emotional state.

**Context preservation** means never reducing persons to diagnostic labels. Every output should acknowledge complexity. Every recommendation should recognize variability. Every interaction should see the person rather than the category.

**Temporal specificity** means recognizing that what is true now may not be true this afternoon. Capacity fluctuates. Mood changes. Good days and bad days are real. A system that treats "moderate dementia" as a stable state will miss the person who is having an exceptionally clear morning or an unusually difficult evening.

**Dignity constraints** mean outputs must pass human dignity review. Would this language be acceptable if spoken to the person directly? Would this framing be acceptable if the person's family read it? Would this recommendation be acceptable if the person at their clearest understood what was being decided?

### The Words We Teach Machines

We shape our tools and then our tools shape us. The sociologist Langdon Winner argued that **"artifacts have politics"**, that technical systems embody social choices and have social consequences (Winner 121). AI systems are artifacts. They have politics. The language they use is a political choice with political consequences.

AI systems trained on stigmatizing language will perpetuate stigma at scale, with the authority of automation. They will sound objective while encoding bias. They will seem neutral while reproducing harm.

The choice is not between accurate and kind language. Accuracy does not require dehumanization. Clinical precision does not require erasing personhood. **The choice is between language that sees only deficits and language that sees whole persons.**

Millions of people will interact with AI systems in healthcare, in caregiving, in daily life. Those systems will describe them, categorize them, recommend interventions for them. The words these systems use will shape how those people are perceived, treated, and valued.

The words we teach machines to use will echo forward through every interaction, every care decision, every moment of human contact mediated by artificial intelligence.

We should choose them carefully.

### References

Goffman, Erving. *Stigma: Notes on the Management of Spoiled Identity*. Prentice-Hall, 1963.

Kitwood, Tom. *Dementia Reconsidered: The Person Comes First*. Open University Press, 1997.

Lakoff, George, and Mark Johnson. *Metaphors We Live By*. University of Chicago Press, 1980.

Link, Bruce G., and Jo C. Phelan. "Conceptualizing Stigma." *Annual Review of Sociology*, vol. 27, 2001, pp. 363-385.

Noble, Safiya Umoja. *Algorithms of Oppression: How Search Engines Reinforce Racism*. NYU Press, 2018.

Sabat, Steven R. "Surviving Manifestations of Selfhood in Alzheimer's Disease: A Case Study." *Dementia*, vol. 1, no. 1, 2002, pp. 25-36.

Swaffer, Kate. "Dementia: Stigma, Language, and Dementia-Friendly." *Dementia*, vol. 13, no. 6, 2014, pp. 709-716.

Winner, Langdon. "Do Artifacts Have Politics?" *Daedalus*, vol. 109, no. 1, 1980, pp. 121-136.
