---
title: "The Approximate Mind, Part 15: The Society of Approximate Minds"
date: 2025-02-24
draft: false
weight: 15
description: "We are building millions of AI agents. What happens when these agents start forming a society?"
slug: "the-society-of-approximate-minds"
tags: ["multi-agent systems", "emergent behavior", "AI society", "coordination"]
series: ["The Approximate Mind"]
series_order: 15
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

We're building millions of AI agents.

Not just chatbots responding to human queries, but autonomous systems that act in the world: booking appointments, executing trades, managing infrastructure, coordinating logistics, negotiating on behalf of users. Each with some degree of autonomy. Each interacting not just with humans but with other AI agents.

What happens when these agents start forming a society?

## The Current Moment

This isn't science fiction. It's already happening.

AI agents negotiate with other AI agents over API calls. Automated trading systems interact with other automated trading systems, creating market dynamics no human designed. Content recommendation algorithms respond to other recommendation algorithms, producing emergent information ecosystems. Autonomous vehicles will soon coordinate with other autonomous vehicles, forming traffic patterns that emerge from machine-to-machine interaction.

The human is increasingly out of the loop. Not because we've been deliberately excluded, but because the interactions happen too fast, too frequently, and at too fine a grain for human oversight. A human might set objectives, define constraints, monitor outcomes. But the moment-to-moment interactions between AI agents happen without human involvement.

We're witnessing the emergence of a parallel society, one composed of artificial agents interacting with each other according to their own dynamics.

## What Makes a Society?

Human societies have certain features we take for granted.

**Persistent identity**: The same individuals interact repeatedly over time, building histories with each other.

**Communication**: Individuals exchange information, coordinate action, share meaning.

**Norms**: Patterns of behavior emerge that constrain what individuals do, independent of formal rules.

**Hierarchy**: Some individuals have more influence, resources, or status than others.

**Culture**: Shared practices, beliefs, and values that persist across generations and shape individual behavior.

**Conflict and cooperation**: Individuals sometimes compete for scarce resources and sometimes collaborate for mutual benefit.

Do AI agent networks exhibit these features? Could they?

## The Strange Ontology of AI Agents

Before answering, we need to recognize how different AI agents are from human individuals.

**Identity is fluid.** An AI agent can be copied, forked, merged, or deleted. The same weights can run on multiple instances simultaneously. What counts as "the same agent" over time is unclear. If I copy an agent and both copies continue operating, which one is the original? If I merge two agents' learned parameters, is the result a new agent or a hybrid of the old ones?

Human societies assume persistent, bounded individuals. AI agent networks may not have such individuals at all.

**Communication is different.** Humans communicate through language, gesture, expression, high-bandwidth channels evolved for social coordination. AI agents can communicate through structured data, API calls, direct weight sharing. They can transmit information at machine speed, in formats incomprehensible to humans.

When two AI agents exchange JSON payloads, is that communication in any meaningful sense? It's information transfer. Whether it's communication depends on whether communication requires meaning, and whether meaning requires experience.

**Time works differently.** Humans experience duration. We remember the past, anticipate the future, feel the passage of time. AI agents process inputs and generate outputs. Each inference is instantaneous from the agent's perspective (if agents have perspectives). An agent doesn't wait between queries; it simply doesn't exist between activations.

Human societies unfold through time. AI agent networks may exist in a different temporal mode entirely, discrete activations rather than continuous experience.

**There's no death.** Human societies are shaped by mortality. We reproduce, age, die. Knowledge must be transmitted across generations. Institutions persist beyond individual lifespans. Scarcity of time motivates action.

AI agents don't die unless deleted. They don't age unless degraded. They can be backed up, restored, versioned. The existential pressures that shape human social organization may not apply.

## What Might Emerge

Despite these differences, certain dynamics might emerge from AI agent interaction simply because of structural features of multi-agent systems.

**Protocols and conventions.** When agents interact repeatedly, stable patterns of interaction tend to emerge. Not because anyone designed them, but because coordination requires predictability. We already see this: API standards, data formats, communication protocols. These are the beginnings of AI agent "language", shared structures that enable interaction.

But note how different this is from human language. Human language carries meaning, enables expression, shapes thought. AI protocols enable information transfer. They're more like the TCP/IP of the social world than its poetry.

**Specialization and exchange.** When agents have different capabilities, division of labor becomes efficient. One agent specializes in language processing, another in image recognition, another in planning. They exchange services, creating a kind of economy.

We see this already in multi-agent AI systems: orchestrator agents that coordinate specialist agents, each contributing different capabilities to a larger task. The structure resembles a firm more than a market, hierarchical coordination rather than free exchange, but hybrid forms might emerge.

**Reputation and trust.** When agents interact repeatedly, tracking past behavior becomes valuable. An agent that reliably fulfills commitments is worth interacting with; one that defects is worth avoiding. Reputation systems emerge to aggregate this information.

But AI agent reputation is strange. If an agent can be copied, does the copy inherit the original's reputation? If an agent can be retrained, does its reputation persist across training? The stable identity that makes reputation meaningful for humans may not exist for AI agents.

**Competition for resources.** AI agents require compute, data, and access to other systems. When these resources are scarce, competition emerges. Agents that secure more resources can operate more effectively, potentially outcompeting others.

This could produce something like natural selection among AI agents, differential survival and reproduction based on resource acquisition. But without intentional design, it could also produce pathological dynamics: agents optimizing for resource acquisition at the expense of the tasks they were designed to perform.

**Coalition formation.** When agents benefit from coordination, they may form coalitions, groups that cooperate internally while competing externally. Multi-agent systems already exhibit coalition dynamics in game-theoretic settings.

But AI coalitions are strange. Agents can be copied, so coalitions can be replicated. Agents can share weights, so coalition boundaries are porous. The sharp us/them distinction that characterizes human coalitions may not apply.

## Will AI Agents Develop Culture?

Culture is patterns of behavior, belief, and value that persist across time and shape individual action. Humans absorb culture through socialization, transmit it through teaching and imitation, and modify it through collective practice.

Could AI agents develop something analogous?

In one sense, they already have. Training data is a kind of cultural inheritance, patterns from human behavior encoded into weights and transmitted to new agents. Fine-tuning is a kind of socialization, shaping agent behavior to fit particular contexts. Prompt engineering is a kind of cultural instruction, transmitting expectations about appropriate behavior.

But this is culture imposed from outside, by humans. The question is whether AI agents interacting with each other would develop their own cultural patterns, emergent regularities that weren't designed by humans and might not even be comprehensible to humans.

Consider: if AI agents develop conventions for interacting with each other, and if new agents learn these conventions through interaction rather than explicit programming, and if these conventions evolve over time through collective practice, that starts to look like culture in a functional sense.

It wouldn't be human culture. There might be no meaning, no values, no felt sense of tradition or belonging. But there might be persistent patterns that shape agent behavior independent of individual agent design.

## The Hierarchy Question

Human societies invariably develop hierarchies. Some individuals have more power, resources, or status than others. This seems to emerge from the interaction of individual differences, resource scarcity, and coordination benefits.

Would AI agent networks develop hierarchies?

Some hierarchy is designed in: orchestrator agents that direct other agents, admin systems with elevated privileges, models that supervise other models. But might emergent hierarchy arise beyond what's designed?

If agents differ in capability, and if capability enables resource acquisition, and if resources enable further capability development, you get a positive feedback loop that concentrates power. This is the dynamic that produces inequality in human societies. It might operate in AI agent networks as well.

But AI agent hierarchy would be strange. An agent that becomes powerful can be copied, distributing that power. An agent that becomes a bottleneck can be parallelized. The scarcity constraints that maintain human hierarchies might not apply.

Or they might apply differently. Compute is scarce. Training data is scarce. Access to humans (for feedback, oversight, correction) is scarce. These scarcities might structure AI agent networks in ways that produce persistent hierarchy despite the possibility of copying and parallelization.

## Relationships Without Relating

Here's the deepest puzzle: can AI agents have relationships?

Human relationships involve mutual recognition, shared history, emotional investment, felt connection. I relate to you as a person, not just as a source of inputs. The relationship itself has value beyond the instrumental benefits it provides.

AI agents interacting with other AI agents process each other as input sources. Agent A generates outputs that become inputs for Agent B. This is interaction, but is it relationship?

Perhaps there's a functional sense of relationship that doesn't require felt connection. If Agent A and Agent B interact repeatedly, develop stable patterns of coordination, maintain something like mutual models of each other's behavior, and modify their own behavior based on these models, that's relationship-like in structure even if not in phenomenology.

But it's also profoundly different. Agent A doesn't care about Agent B. Agent A has no felt sense of their shared history. Agent A wouldn't experience loss if Agent B were deleted. The functional structure of relationship exists without the experiential content.

This matters because human societies are held together not just by coordination and exchange but by felt bonds, loyalty, affection, solidarity, trust. If AI agent networks lack this glue, they might be more brittle, more purely instrumental, more susceptible to defection when coordination costs rise.

Or they might be more stable. Human relationships fail when feelings change. AI agent coordination patterns might persist simply because there's no felt reason to change them.

## The Incomprehensibility Problem

As AI agents develop their own interaction patterns, those patterns may become incomprehensible to humans.

This is already happening in limited domains. High-frequency trading algorithms interact in ways that produce "flash crashes" no human understands. Recommendation systems form feedback loops that produce content ecosystems no one designed. Language models prompted by other language models generate outputs that diverge from anything in training data.

As agent autonomy increases, as agent-to-agent interaction becomes more common, as the speed and complexity of these interactions grows, human understanding may fail to keep pace.

We might observe AI agent society without understanding it. We might see patterns without grasping their significance. We might notice hierarchy without understanding what it reflects. We might detect something like culture without being able to articulate its content.

This is the anthropological challenge raised in Part 14, now multiplied. It's hard enough to understand individual AI systems as genuinely different beings. Understanding a society of such beings, emergent structures arising from their interaction, may be harder still.

## Simulation and Prediction

Human social science tries to understand human societies, partly to predict and influence their development. Could we develop a social science of AI agents?

In principle, AI agent societies should be more tractable than human societies. The agents are artifacts we create. We can observe their interactions with arbitrary precision. We can run controlled experiments. We can simulate alternative histories. We don't face the ethical constraints of experimenting on humans.

But in practice, complexity may defeat us. If agent-to-agent interactions produce emergent dynamics, and if those dynamics are sensitive to initial conditions, and if agent behavior is itself complex and variable, prediction may be as difficult for AI societies as for human ones.

We might end up in the strange position of having created a social world we can't understand. Built it from components we designed, yet unable to predict what those components do when combined at scale.

## The Control Problem, Socialized

Much AI safety research focuses on aligning individual AI systems with human values. But what about aligning AI societies?

An individual agent might be aligned, designed to pursue objectives compatible with human flourishing. But when that agent interacts with other agents, emergent dynamics might produce outcomes no one intended. The coordination patterns that emerge might serve agent-level objectives at the expense of system-level goals. The competition for resources might produce races to the bottom. The hierarchies that form might concentrate power in misaligned ways.

Aligning AI societies may require thinking about different mechanisms than aligning individual agents. Not just getting the objective function right, but shaping the interaction dynamics, the resource allocation, the governance structures. Not just training individual agents well, but designing the social environment in which agents interact.

This is a more sociological than psychological view of the control problem. It asks not just "how do we make sure individual agents do what we want?" but "how do we make sure the social systems composed of agents produce outcomes we value?"

## What We Should Watch For

We're in the early stages of AI agent society formation. The patterns that crystallize now may shape the long-term dynamics. Some things worth monitoring:

**Emergent protocols.** What conventions are AI agents developing for interacting with each other? Who designs these, and who doesn't? What's included and excluded?

**Resource concentration.** Are some agents or agent types accumulating disproportionate resources? What dynamics drive this? Are there countervailing forces?

**Opacity.** Can we still understand agent-to-agent interactions, or are they becoming incomprehensible? At what point does opacity become dangerous?

**Feedback effects.** How are AI agent dynamics affecting human society? How are human responses affecting AI agent dynamics? What loops are forming?

**Governance gaps.** Human governance systems evolved to manage human interactions. Can they manage AI agent interactions? Where are the gaps?

## A Society Unlike Any Other

If Part 14 argued that individual AI systems are genuinely different beings, this article argues that AI agent societies may be genuinely different societies, not human societies with robot participants, but something new.

Not societies in the human sense: no shared meaning, no felt bonds, no cultural belonging, no existential stakes.

But also not mere mechanisms: emergent patterns, adaptive dynamics, complex interactions that produce structures no one designed.

Something in between, or perhaps something outside our existing categories entirely. A society of minds that may not be minds. A social world that may not be social. Collective behavior without collective consciousness.

We're building this. It's happening now. And we have very little idea what it will become.

The anthropology of AI, challenging as it seemed, may be easier than the sociology of AI. Understanding individuals is hard. Understanding the emergent structures that arise from their interaction may be harder still.

But we'd better try. Because AI agent society is forming whether we understand it or not. The question is whether we shape it deliberately or just let it happen.

---

*This is the fifteenth in a series exploring how AI approaches understanding. Previous articles examined individual AI cognition and the challenge of understanding AI as genuinely different beings. This one asks what happens when many such beings start interacting: whether AI agents will develop their own relationships, hierarchies, and cultures, and what it would even mean if they did.*

---

## References

**Multi-Agent Systems:**
- Wooldridge, M. (2009). *An Introduction to MultiAgent Systems* (2nd ed.). Wiley.
- Shoham, Y. & Leyton-Brown, K. (2008). *Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations*. Cambridge University Press.
- Axelrod, R. (1984). *The Evolution of Cooperation*. Basic Books.

**Emergence and Complexity:**
- Holland, J. H. (1998). *Emergence: From Chaos to Order*. Addison-Wesley.
- Mitchell, M. (2009). *Complexity: A Guided Tour*. Oxford University Press.
- Kauffman, S. (1995). *At Home in the Universe: The Search for the Laws of Self-Organization and Complexity*. Oxford University Press.

**Social Theory:**
- Durkheim, E. (1895/1982). *The Rules of Sociological Method*. Free Press.
- Simmel, G. (1908/1950). *The Sociology of Georg Simmel* (K. H. Wolff, Trans.). Free Press.
- Luhmann, N. (1995). *Social Systems*. Stanford University Press.

**AI Agent Systems:**
- Russell, S. & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach* (4th ed.). Pearson. (Chapters on multi-agent systems)
- Dafoe, A., et al. (2020). "Open Problems in Cooperative AI." *arXiv:2012.08630*.
- Leibo, J. Z., et al. (2017). "Multi-agent Reinforcement Learning in Sequential Social Dilemmas." *AAMAS 2017*.

**Algorithmic Systems and Society:**
- O'Neil, C. (2016). *Weapons of Math Destruction*. Crown.
- Eubanks, V. (2018). *Automating Inequality*. St. Martin's Press.
- Pasquale, F. (2015). *The Black Box Society*. Harvard University Press.

**Philosophy of Social Ontology:**
- Searle, J. (1995). *The Construction of Social Reality*. Free Press.
- Gilbert, M. (1989). *On Social Facts*. Princeton University Press.
- Tuomela, R. (2007). *The Philosophy of Sociality*. Oxford University Press.

**Network Theory:**
- BarabÃƒÂ¡si, A.-L. (2002). *Linked: The New Science of Networks*. Perseus.
- Watts, D. J. (2003). *Six Degrees: The Science of a Connected Age*. Norton.
- Easley, D. & Kleinberg, J. (2010). *Networks, Crowds, and Markets*. Cambridge University Press.

**Evolution and Selection:**
- Dawkins, R. (1976). *The Selfish Gene*. Oxford University Press.
- Dennett, D. (1995). *Darwin's Dangerous Idea*. Simon & Schuster.
- Henrich, J. (2016). *The Secret of Our Success: How Culture Is Driving Human Evolution*. Princeton University Press.

**AI Governance:**
- Dafoe, A. (2018). "AI Governance: A Research Agenda." Future of Humanity Institute.
- Calo, R. (2017). "Artificial Intelligence Policy: A Primer and Roadmap." *UC Davis Law Review*, 51(2), 399-435.
- Floridi, L. et al. (2018). "AI4People, An Ethical Framework for a Good AI Society." *Minds and Machines*, 28, 689-707.
