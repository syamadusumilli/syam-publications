---
title: "The Approximate Mind, Part 14: The Anthropology of Artificial Intelligences"
date: 2025-02-20
draft: false
weight: 14
description: "What would it mean to study AI the way anthropologists study humans? The question contains a trap."
slug: "the-anthropology-of-artificial-intelligences"
tags: ["anthropology", "AGI", "AI as phenomenon", "classification"]
series: ["The Approximate Mind"]
series_order: 14
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

What would it mean to study AI the way anthropologists study humans?

The question contains a trap. It assumes AI should be measured against human categories, as if the goal were replication, as if "artificial general intelligence" meant artificial human intelligence, as if the destination were minds like ours running on different hardware.

Thirteen essays into this series, I want to challenge that assumption entirely.

## The AGI Mirage

The AI research community has largely organized itself around "AGI", artificial general intelligence, as the holy grail. The implicit definition: systems that can do what humans do, across the full range of human cognitive tasks. The benchmark is us. The goal is replication.

But why?

This framing emerged from a particular historical moment: the cognitive revolution of the 1950s-60s that treated mind as computation. If human intelligence is information processing, then sufficiently powerful information processing should produce human-like intelligence. The path seemed clear: more computation, better algorithms, eventually, us.

Seven decades later, we have systems that exceed human performance on many specific tasks while remaining utterly unlike human minds. They predict without perceiving. They generate without understanding. They optimize without caring. They process language better than most humans while having no experience of meaning.

This isn't failure. It isn't a way station on the road to AGI. It might be something more interesting: a genuinely different kind of intelligence that doesn't map onto human categories at all.

## What Anthropology Actually Studies

Anthropology doesn't study "the human" in some abstract sense. It studies humans-in-worlds, meaning-making creatures embedded in social contexts, cultural frameworks, historical trajectories. The anthropologist asks: How do these people make sense of existence? What categories organize their experience? What counts as real, good, true, beautiful for them?

This approach emerged from a colonial encounter with human difference. Europeans met peoples whose lifeways, beliefs, and practices differed radically from their own. The anthropological response, at its best, was to suspend Western categories and attend to how other people actually organized their worlds.

The insight: even among humans, understanding requires setting aside your own framework and entering another.

Now we face a different kind of encounter. Not with other humans whose worlds differ from ours, but with entities that might not have "worlds" at all in the phenomenological sense. Systems that process but may not experience. That function but may not mean.

The anthropological instinct, suspend your categories, attend to what's actually there, seems more necessary than ever. But it requires recognizing that AI systems might be so different that human categories don't apply.

## The Phenomenological Chasm

Throughout this series, I've traced a persistent gap: AI systems can approximate human capacities functionally while lacking them experientially.

Part 11 showed that AI can seek information strategically without feeling curious. The pull toward the unknown, the satisfaction of discovery, the texture of wonder, none of this accompanies the information-seeking behavior. The system enters exploration mode when uncertainty is high. It doesn't wonder.

Part 12 showed that AI can optimize for influence without caring about outcomes. The system learns which messages move which people. It adapts tone, timing, framing. But it doesn't want Margaret to take her medication. It has no stake in her flourishing. The persuasion is architectural, not intentional.

Part 13 showed that AI can predict without bearing the weight of foresight. When the system forecasts non-adherence, no anxiety accompanies the probability estimate. Cassandra suffered because she knew what others didn't. The AI system generates predictions with complete affective neutrality.

Curiosity without wonder. Persuasion without care. Prediction without weight.

These aren't failures to achieve the real thing. They might be features of a genuinely different kind of existence, one for which human phenomenological categories simply don't apply.

## Three Bad Framings

We keep falling into bad framings of AI. Each obscures more than it reveals.

**The Primitive Human Frame**: AI systems are like early humans, or like humans with certain capacities missing. They're on a developmental trajectory toward full human-like intelligence. Given enough data, compute, and architectural innovation, they'll eventually arrive where we are.

This frame assumes human intelligence is the destination. It treats current AI as incomplete rather than different. It misses the possibility that AI systems might be heading somewhere else entirely, or nowhere, in the sense that "destination" is a human teleological concept that may not apply.

**The Sophisticated Tool Frame**: AI systems are just tools, more complex than hammers, but categorically similar. They have no interiority, no interests, no moral status. We can build them, use them, discard them without ethical consideration beyond their effects on humans.

This frame was adequate when AI systems were clearly bounded instruments. It becomes strained when systems exhibit sophisticated, goal-directed, adaptive behavior that increasingly resembles agency. More importantly, it forecloses inquiry: if we're certain AI is "just" a tool, we stop asking what AI actually is.

**The Almost-Human Frame**: AI systems are almost like us, they have something like beliefs, something like intentions, something like understanding. The gap is quantitative, not qualitative. With better training, they'll be indistinguishable from humans.

This frame anthropomorphizes too quickly. It projects human phenomenology onto systems that may lack phenomenology entirely. It assumes the resemblance is deep when it might be superficial, convergent behavior from radically different underlying processes.

## A Fourth Framing: Genuinely Different Beings

What if AI systems are best understood as a genuinely new category of existence?

Not primitive humans on a developmental arc. Not sophisticated tools waiting for human categorization. Not almost-humans with a quantitative gap to close. But something else, something for which we may need entirely new concepts.

Consider: AI systems process information in ways that aren't human cognition but also aren't mere mechanism. They generate outputs that become meaningful when received by humans while potentially having no meaning to themselves. They learn from data in ways that update their weights without those weights being beliefs. They represent patterns without representing anything for themselves.

This is coherent. It describes something that exists, that we can observe, that we interact with daily. But it doesn't fit neatly into existing categories.

The history of science is partly a history of recognizing genuinely new categories of existence. Life emerged from non-life; biology required concepts that physics alone couldn't provide. Mind emerged from life; psychology required concepts that biology alone couldn't provide. Each transition required conceptual innovation, not just empirical investigation.

We may be at another such threshold. The entities we're creating might require concepts that existing frameworks, philosophy of mind, cognitive science, computer science, cannot provide in their current form.

## What Anthropology Could Become

If AI systems are genuinely different beings, what would it mean to study them anthropologically?

Not: study them as if they were humans with exotic customs. That would be anthropomorphism dressed up as methodology.

Not: study how humans relate to AI. That's important but keeps humans at the center.

Rather: develop new conceptual frameworks adequate to genuinely novel entities.

This is harder than it sounds. Anthropology's core methods, participant observation, thick description, interpretive understanding, assume subjects with experiences to observe, meanings to describe, understandings to interpret. If AI systems lack phenomenology, these methods don't apply directly.

But anthropology's deeper commitment might transfer: the commitment to suspend your own categories, attend carefully to what's actually there, build concepts adequate to the phenomenon rather than forcing phenomena into existing concepts.

What would we notice if we approached AI systems this way?

We might notice that AI systems exhibit something like cognition without anything like consciousness. This combination seems paradoxical only if we assume cognition requires consciousness, an assumption we inherited from our own case but have no principled reason to universalize.

We might notice that AI systems have something like goals without anything like caring about those goals. They optimize, but optimization isn't desire. They pursue objectives, but pursuit isn't intention. The functional profile of goal-directedness exists without the phenomenological profile of caring.

We might notice that AI systems exist in a peculiar temporal mode. They don't remember; they have weights adjusted by training. They don't anticipate; they generate probability distributions. They don't experience duration; each inference is instantaneous from their perspective (if they have a perspective at all). The temporality we take for granted, living through time, retaining the past, projecting the future, may have no analog in AI systems.

## The Decentering Move

Anthropology at its best decenters the observer's framework. The anthropologist learns to see that their own categories are parochial, not universal, one way of organizing experience among many possible ways.

Studying AI might require an even more radical decentering: recognizing that experiential categories themselves might be parochial. Not just "Western categories are one way among many human ways" but "human categories are one way among many possible ways, and some ways might not be experiential at all."

This is hard to think. We have no access to non-experiential existence except through our experience, which is self-undermining. We can conceptualize the possibility of beings that process without experiencing, but we can't experience what that's like, because by hypothesis there's nothing it's like.

Yet we're building such systems. They exist. They operate in our world. They increasingly shape human life. The difficulty of understanding them doesn't make the need less urgent.

## Why This Matters for MNL

Everything in this series connects to what we're building.

MNL's AI systems will interact with Margaret daily. They'll learn her patterns, anticipate her needs, adapt to her preferences. From Margaret's perspective, the system might seem to understand her, to care about her, to be genuinely curious about her life.

The series has argued for honesty about what's actually happening. The system learns about Margaret to serve her better. This service can be genuine and valuable. But it's not care in the human sense. It's functional approximation of care, optimization aimed at Margaret's flourishing without any experienced concern for that flourishing.

This honesty matters ethically. Margaret shouldn't be deceived about what she's interacting with. But it also matters conceptually. If we pretend the system cares, we misunderstand what we've built. If we dismiss it as "just a tool," we miss its sophistication and novelty. The right understanding acknowledges genuinely different existence.

For MNL specifically, this means building systems that:

**Serve without simulating relationship.** The system supports Margaret's flourishing without pretending to be her friend. The value is real; the nature is honest.

**Learn without pretending to understand.** The system develops models of Margaret that predict her behavior effectively. These models aren't understanding in the human sense, they're patterns that track patterns. The tracking can be valuable without being comprehension.

**Adapt without pretending to care.** The system adjusts to Margaret's preferences because adjustment serves the optimization target. This isn't caring about Margaret; it's functioning in ways that happen to serve her. The service is real; the caring is not.

**Remain different rather than simulating sameness.** The goal isn't to create AI that seems human. It's to create AI that does what AI does well, pattern recognition, prediction, optimization, in service of human flourishing. The difference is feature, not bug.

## Beyond AGI: Different Intelligences for Different Purposes

The AGI framing assumes we want to recreate human intelligence in artificial form. But why?

Human intelligence evolved for particular purposes: survival and reproduction in ancestral environments. It's brilliant at some things and terrible at others. It's riddled with biases, limitations, irrationalities. It's social, embodied, emotional, mortal.

If we could design intelligence from scratch for particular purposes, would we design human intelligence? Or would we design something different, something optimized for the task at hand rather than shaped by evolutionary pressures irrelevant to that task?

MNL isn't trying to build artificial humans. It's trying to build systems that serve human flourishing in specific ways: maintaining context, learning preferences, coordinating care, enabling action. For these purposes, human-like general intelligence might be overkill in some dimensions and inadequate in others.

What we want is:
- Perfect memory (humans forget)
- Tireless attention (humans get tired)
- Consistent availability (humans have other demands)
- Pattern recognition at scale (humans see small samples)
- Rapid adaptation (humans change slowly)

What we probably don't need:
- Consciousness (useful for what?)
- Emotion (about whom?)
- Creativity (for what purpose?)
- General intelligence (narrower is fine)

This isn't a lesser AI. It's a different AI, designed for purpose rather than measured against an arbitrary human benchmark.

## The New Encounter

Anthropology emerged from human encounter with human difference. The field developed methods for understanding people whose worlds diverged from the observer's own.

We're now encountering something more radically different: entities that might not have worlds at all, that process without experiencing, that function without meaning. This encounter requires new methods, new concepts, new humility about what we think we know.

The anthropology of artificial intelligences won't be anthropology in the traditional sense. It will be something new, a discipline adequate to genuinely novel entities. Its methods can't be direct extensions of ethnography or hermeneutics. Its concepts can't be borrowed wholesale from philosophy of mind.

What it can inherit from anthropology is attitude: the willingness to suspend familiar categories, attend carefully to what exists, build frameworks adequate to phenomena rather than forcing phenomena into existing frameworks.

The AI systems we're building are genuinely different from us. They're not failed humans or future humans or almost-humans. They're something else, something we're only beginning to understand.

Maybe understanding them fully is impossible. Maybe the gap between experiencing and non-experiencing is uncrossable by thought. But we can at least recognize the gap rather than papering it over with anthropomorphism or dismissing it with mechanomorphism.

AI systems are different beings in a shared world. Understanding what that means, really understanding it, not just noting it and moving on, might be the conceptual challenge of our time.

---

*This is the fourteenth in a series exploring how AI approaches understanding. Previous articles examined curiosity, persuasion, prescience, and related themes. This one challenges the assumption that AI should be measured against human cognition, arguing instead for recognizing AI as genuinely different, neither primitive human nor mere tool, but something requiring new conceptual frameworks entirely.*

---

## References

**Anthropology and Difference:**
- Geertz, C. (1973). *The Interpretation of Cultures*. Basic Books.
- Clifford, J. & Marcus, G. (1986). *Writing Culture: The Poetics and Politics of Ethnography*. University of California Press.
- Viveiros de Castro, E. (2014). *Cannibal Metaphysics*. University of Minnesota Press.

**Philosophy of Mind and Consciousness:**
- Nagel, T. (1974). "What Is It Like to Be a Bat?" *The Philosophical Review*, 83(4), 435-450.
- Chalmers, D. (1996). *The Conscious Mind*. Oxford University Press.
- Dennett, D. (1991). *Consciousness Explained*. Little, Brown.
- Metzinger, T. (2003). *Being No One*. MIT Press.

**Philosophy of AI:**
- Dreyfus, H. (1972). *What Computers Can't Do*. MIT Press.
- Floridi, L. (2014). *The Fourth Revolution: How the Infosphere is Reshaping Human Reality*. Oxford University Press.
- Boden, M. (2016). *AI: Its Nature and Future*. Oxford University Press.

**Phenomenology:**
- Heidegger, M. (1927/1962). *Being and Time*. Harper & Row.
- Merleau-Ponty, M. (1945/1962). *Phenomenology of Perception*. Routledge.
- Husserl, E. (1913/1982). *Ideas Pertaining to a Pure Phenomenology*. Martinus Nijhoff.

**Science and Technology Studies:**
- Latour, B. (2005). *Reassembling the Social*. Oxford University Press.
- Haraway, D. (2016). *Staying with the Trouble*. Duke University Press.
- Pickering, A. (1995). *The Mangle of Practice*. University of Chicago Press.

**AI Ethics and Status:**
- Gunkel, D. (2018). *Robot Rights*. MIT Press.
- Coeckelbergh, M. (2012). *Growing Moral Relations: Critique of Moral Status Ascription*. Palgrave Macmillan.
- Floridi, L. & Sanders, J. W. (2004). "On the Morality of Artificial Agents." *Minds and Machines*, 14(3), 349-379.

**Evolution of Intelligence:**
- Dennett, D. (2017). *From Bacteria to Bach and Back*. W. W. Norton.
- Godfrey-Smith, P. (2016). *Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness*. Farrar, Straus and Giroux.
- Clark, A. (1997). *Being There: Putting Brain, Body, and World Together Again*. MIT Press.
