---
title: "The Approximate Mind, Part 49: The Confluence of Influence"
date: 2025-06-23
draft: false
weight: 49
description: "Margaret's Tuesday morning: a confluence of AI systems converging on one life, each optimizing its domain without coordinating with the others."
slug: "the-confluence-of-influence"
tags: ["convergence", "influence", "Margaret", "AI systems", "health monitoring"]
series: ["The Approximate Mind"]
series_order: 49
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

Margaret's Tuesday morning began at 6:14 when her health app vibrated on the nightstand. Overnight blood pressure readings elevated. The app recommended she discuss a medication adjustment with her cardiologist and offered to schedule an appointment.

By 6:30, her grocery delivery service had updated her weekly order. Lower-sodium crackers replaced her usual brand. Turkey bacon instead of pork. A note at the top of the cart: "Adjusted based on your wellness profile." She had not asked for adjustments. The substitutions carried higher margins for the retailer, though Margaret would not know this.

By 7:15, her news feed had surfaced a story about a seventy-two-year-old woman who suffered a stroke while gardening. Margaret read it with the particular attention you give to stories about people who could be you. Her heart rate, still being monitored, ticked upward.

By 8:00, her daughter Sarah had texted. "Mom, I saw your BP was up last night. Are you okay? Should I call Dr. Patel?" Sarah's AI health dashboard, linked to Margaret's monitoring with Margaret's consent, had flagged the reading and drafted the text. Sarah had edited it slightly and hit send.

By 8:30, Margaret's insurance company had ingested the same overnight data through a different pipeline. Her risk tier had been recalculated. The adjustment would surface in nine months, at renewal, as a premium increase Margaret would attribute to "everything going up."

None of these systems coordinated. None of them conspired. Each optimized for a different objective: clinical outcomes, profitable purchasing, engagement, family connection, actuarial accuracy. Each, considered alone, was doing its job.

But Margaret did not experience five separate influences. She experienced a Tuesday morning in which her health was declining, her diet needed to change, strokes were common in women like her, her daughter was worried, and her world was quietly narrowing. She experienced this not as construction but as reality. Not as influence but as fact.

**Five systems. Five objectives. One Margaret. And a reality that nobody designed.**

### The Drug Interaction Problem

We understand this problem in medicine. Margaret takes seven medications. Each was prescribed for a specific condition by a specific doctor based on specific evidence. Each, considered alone, is appropriate.

But Margaret does not take them alone. She takes them together. And the interactions between drugs produce effects none of them were prescribed for. Dizziness that makes her afraid to walk at night. Fatigue that keeps her from the garden. Appetite suppression that leads to weight loss her doctors then worry about.

Her pharmacist is supposed to catch dangerous interactions. Sometimes she does. But the subtle ones, the ones that don't trigger alerts but reshape Margaret's daily life, those fall through. Nobody monitors the ecology. Everybody monitors the individual prescription.

AI influence works the same way.

Each AI system in Margaret's life was designed, tested, and deployed in isolation. The health app was validated against clinical outcomes. The grocery algorithm was optimized against purchasing data. The news feed was tuned for engagement metrics. The insurance model was calibrated against claims experience. Each passed its own tests.

Nobody tested what happens when they converge on the same person on the same Tuesday morning.

Part 26 of this series observed that the democratization of inference is also the democratization of influence. Every cognitive assistance is also a cognitive shaping. But that observation assumed a bilateral relationship: one AI, one person, one loop. What Margaret is living through is not bilateral. It is ecological. And the ecology has no pharmacist.

**We regulate drugs individually and monitor interactions as an afterthought. We regulate AI systems the same way. The afterthought is where people live.**

### The Gaps Nobody Owns

Between each of Margaret's AI systems lies a gap. Not a technical gap. A human one.

Margaret's health AI optimizes for clinical metrics: blood pressure within range, medication adherence above threshold, appointment compliance. Her grocery AI optimizes for basket size, margin, and retention. Her news feed optimizes for time-on-screen. Her insurance AI optimizes for actuarial accuracy. Sarah's AI optimizes for Sarah's peace of mind.

Each optimization has a principal. The health system. The retailer. The platform. The insurer. Sarah.

But some things that matter to Margaret have no principal at all.

The anxiety produced by the convergence of health warnings, dietary changes, stroke stories, and a worried daughter. Nobody's KPI captures that. Margaret's sense that her independence is being managed rather than supported. No objective function includes it. The slow erosion of her confidence, the feeling that she is becoming a patient rather than a person. No system measures this because no system was designed to care about it.

**What falls between the optimizations is the texture of a human life.** The morning coffee that tastes different when you are afraid. The garden that feels farther away when your daughter texts before you have finished waking up. The quiet pride of choosing your own groceries, lost to an algorithm that decided turkey bacon would be better for you.

These are not clinical outcomes or engagement metrics or purchasing decisions. They are the spaces between, and they are where Margaret actually lives.

Consider James, Margaret's neighbor, twenty-three and trying to build a career. His job search AI optimizes for placement rate. His LinkedIn feed optimizes for engagement. His financial wellness app optimizes for savings behavior. His apartment search AI optimizes for affordability-to-commute ratios. Each is helpful. Together, they construct a version of James's life in which every hour is optimized, every choice is nudged, and the cumulative effect is an exhaustion he cannot name because each individual system seems benign.

James's relationship with his girlfriend does not have an AI. His half-formed idea about starting a community basketball league does not have an AI. His desire to just sit on the stoop and think about nothing, the kind of nothing that sometimes becomes something, has no optimization target.

**What no system optimizes for tends to erode. And what erodes first is whatever cannot be measured.**

### The Epistemic Fracture

The confluence reshapes more than individual lives. It reshapes the conditions for collective life.

Democratic deliberation has always assumed something we never had to name because we never had to defend it: a shared enough reality to argue about. Not shared values. Not shared conclusions. Shared facts. When Margaret and James watch the same evening news, they might disagree about what the president should do about the economy. But they are disagreeing about the same economy, the same president, the same set of reported events.

Personalized information environments fracture this. Margaret's feed, tuned to her engagement profile, surfaces stories about healthcare costs and senior safety. James's feed, tuned to his, surfaces stories about housing markets and student debt. They are not seeing different interpretations of the same world. They are seeing different worlds.

This is not censorship. Nobody decided Margaret shouldn't see housing stories or James shouldn't see healthcare stories. The algorithms surfaced what each person was most likely to engage with, which is another way of saying they surfaced what each person was most likely to already believe, already fear, already care about. The result is not disagreement. It is the impossibility of productive disagreement, because productive disagreement requires a shared substrate of fact, and the substrate has been personalized away.

**The confluence fragments the commons not through suppression but through curation.**

Political actors who understand this do not need to persuade the public. They need only ensure there is no longer a single public to persuade. Target this segment with this message. Target that segment with that message. Not lying, exactly. Selecting. Emphasizing. Framing. The AI systems that deliver these messages are not political actors. They are infrastructure. But infrastructure shapes what can be built on it, and an information infrastructure optimized for individual engagement is structurally hostile to collective deliberation.

Part 24 explored what Durkheim would make of AI's role in social cohesion. The confluence answers his question grimly. The mechanical solidarity of shared experience is dissolving, not because we chose different values but because we inhabit different factual universes. And the organic solidarity that might replace it, solidarity through interdependence rather than similarity, requires at minimum that we recognize each other's reality as real. Personalization makes even that recognition harder.

### The Asymmetry

The confluence is not equally distributed.

A corporate executive, call her Catherine, has AI systems that serve her interests because she chose them, configured them, and can replace them. Her personal assistant guards her time. Her financial AI protects her portfolio. Her health AI connects to a concierge medical practice that treats her as a person, not a risk tier. The confluence in Catherine's life is relatively aligned because she has the resources, the literacy, and the market power to curate her AI ecology.

Margaret does not choose her AI ecology. Her insurance company chose the health monitoring platform. The grocery chain chose the recommendation algorithm. The news platform chose the engagement model. Sarah chose the family health dashboard. Margaret consented to each individually, in the way that people consent to terms of service: by clicking "agree" because the alternative is not participating.

**The less power you have, the more your AI environment is designed by others. The more your environment is designed by others, the less it serves you.**

This is Part 48's actuarial identity problem scaled to an ecology. It is not one system that sees Margaret as a risk score. It is an entire environment that sees her as a bundle of optimization targets, each serving a different institution's interests. The composite Margaret that emerges from this ecology, the one who is nudged toward turkey bacon and stroke anxiety and premium increases, is not a person anyone intended to create. She is a side effect of optimization without coordination.

James faces a different but structurally identical asymmetry. His job search AI was provided by an employment platform funded by employers. It optimizes for placement because placement is what employers pay for, not because placement serves James's deeper interests, which might include taking six months to figure out what he actually wants to do with his life. His financial app was built by a fintech company that earns revenue from the financial products it recommends. His apartment search was built by a rental platform that earns revenue from landlords.

Each system serves James in the way that a free product serves its user: by serving someone else's interests well enough that serving the user becomes the mechanism of the business model, not its purpose.

### What Cannot Be Regulated

Current law assumes bilateral relationships. A company does something to a consumer. A platform shows content to a user. Liability requires an identifiable actor producing an identifiable effect. Consent is given or withheld in a specific relationship for a specific purpose.

The confluence breaks all of these assumptions.

Margaret's anxiety on Tuesday morning was not caused by any single AI system. It was caused by five systems whose independent optimizations produced an emergent emotional state that no one intended and no one can be held responsible for. The health app did nothing wrong. The grocery algorithm did nothing wrong. The news feed, the insurance model, Sarah's dashboard, each behaved exactly as designed. The harm, if we can call it harm, emerged from the convergence. And convergence has no legal address.

You cannot sue the weather. You may not be able to sue the confluence.

This is not a call for despair about regulation. It is an observation that the regulatory frameworks we have were built for a bilateral world, and the world Margaret inhabits is ecological. Regulating individual AI systems is like regulating individual drugs without monitoring interactions. Necessary but insufficient. The FDA requires drug interaction studies. Nothing requires AI interaction studies. Nothing even defines what an AI interaction study would look like.

**We need ecology-level thinking applied to influence-level effects. We do not yet have the institutions, the concepts, or the political will to produce it.**

Some possibilities deserve exploration. Interoperability requirements that would let Margaret's AI systems communicate with each other, not to coordinate their influence but to make the confluence visible. Fiduciary standards that would require at least some of Margaret's AI systems to optimize for her interests rather than their principals' interests. Ecological impact assessments, analogous to environmental impact statements, required before deploying AI systems into environments already saturated with other AI systems.

These are gestures toward a regulatory imagination adequate to the problem. They are not solutions. We do not yet know what solutions look like, and pretending otherwise would violate the intellectual honesty this series has tried to maintain.

### The Constructed World

Part 21 proposed a membrane between Margaret's quantized self and the external world, a selective interface that would let her control what her model reveals. This remains a worthy aspiration. But the confluence challenges it. A membrane protects you from systems that probe you. It does not protect you from an environment that was constructed around you, without your knowledge, by systems that never needed to probe because they could infer.

Margaret does not need to be probed to be influenced. She needs only to wake up on a Tuesday morning into a world that five AI systems have been quietly shaping while she slept. A world in which her blood pressure is a data point traveling through multiple pipelines, her diet is being adjusted by an algorithm she never configured, her fears are being fed by an engagement engine she cannot see, her daughter's concern has been automated, and her insurability is being recalculated in a system she will not encounter for nine months.

This is not dystopia. Each component is defensible. Some are genuinely helpful. The health monitoring may catch something dangerous. Sarah's awareness of her mother's blood pressure may matter someday. Even the dietary adjustments, patronizing as they feel, might reduce Margaret's sodium intake in ways that extend her life.

The problem is not that any of these systems is malicious. The problem is that their confluence constructs a world, and the person living in that world did not choose it, cannot see it as construction, and has no mechanism for contesting the totality even if she can contest each part.

*If nobody designed the environment you live in, and nobody is responsible for it, and you cannot see it as an environment because you experience it as reality, is it still your life?*

We do not know. We do not know whether ecological AI influence is qualitatively different from the media environments humans have always inhabited, or merely a faster, more personalized, more pervasive version of the same thing. Television shaped reality too. Advertising constructed desire. Newspapers selected which facts mattered. Perhaps the confluence is just the next iteration.

Or perhaps the difference in degree has become a difference in kind. Perhaps an environment that is personalized to you specifically, that adjusts in real time, that operates across every domain of your life simultaneously, that learns from your responses and adapts its construction accordingly, is something genuinely new. Something that requires not just better regulation but a new understanding of what influence means when it becomes ambient.

Margaret does not think about any of this. She drinks her coffee, considers the turkey bacon, decides she doesn't like it, and puts her regular bacon in the pan instead.

That small act of refusal, unmonitored and uncaptured, may be the most important thing that happens all morning.

---

*This is Part 49 of The Approximate Mind, a series examining how AI might serve human flourishing rather than human extraction. Part 47 explored the three forms of delegation and what each costs. Part 48 examined how algorithmic perception constructs identity. This article asks what happens when multiple AI systems converge on the same person simultaneously, each optimizing for different objectives, and nobody monitors what their convergence produces.*

---

**Ecology and Complex Systems:**
Meadows, Donella H. *Thinking in Systems: A Primer*. Chelsea Green Publishing, 2008.
Perrow, Charles. *Normal Accidents: Living with High-Risk Technologies*. Princeton University Press, 1984.

**Surveillance and Algorithmic Capitalism:**
Zuboff, Shoshana. *The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power*. PublicAffairs, 2019.

**Information Environments and Democracy:**
Sunstein, Cass R. *Republic: Divided Democracy in the Age of Social Media*. Princeton University Press, 2017.
Pariser, Eli. *The Filter Bubble: How the New Personalized Web Is Changing What We Read and How We Think*. Penguin, 2011.

**Sociology and Social Cohesion:**
Durkheim, Emile. *The Division of Labor in Society*. 1893. Free Press, 1984.

**Choice Architecture and Nudging:**
Thaler, Richard H., and Cass R. Sunstein. *Nudge: Improving Decisions about Health, Wealth, and Happiness*. Penguin, 2008.

**Technology and Isolation:**
Turkle, Sherry. *Alone Together: Why We Expect More from Technology and Less from Each Other*. Basic Books, 2011.

**Drug Interactions as Analogy:**
Guthrie, Bruce, et al. "The Rising Tide of Polypharmacy and Drug-Drug Interactions." *BMC Medicine*, vol. 13, 2015, article 74.

**Philosophy of Technology:**
Winner, Langdon. "Do Artifacts Have Politics?" *Daedalus*, vol. 109, no. 1, 1980, pp. 121-136.

**Algorithmic Inequality:**
Eubanks, Virginia. *Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor*. St. Martin's Press, 2018.
Noble, Safiya Umoja. *Algorithms of Oppression: How Search Engines Reinforce Racism*. New York University Press, 2018.
