---
title: "The Approximate Mind, Part 48: You Think, Therefore I Am"
date: 2025-06-19
draft: false
weight: 48
description: "When AI does the thinking, something about the relationship between thought and self shifts. You think, therefore I am. But who is the you?"
slug: "you-think-therefore-i-am"
tags: ["identity", "cognition", "outsourcing thought", "selfhood"]
series: ["The Approximate Mind"]
series_order: 48
authors:
  - "Syam Adusumilli"
  - "Yagn Adusumilli"
showAuthor: true
showDate: true
showReadingTime: true
showTableOfContents: true
---

Margaret applied for a new credit card last month. She did not know, when she submitted the application, that a system had already decided who she was.

It knew her zip code, which told it something about her neighborhood's median income and historical default rates. It knew her age, which placed her in an actuarial category with its own risk profile. It knew she had searched for "best credit cards for seniors" three days earlier, which told it she was shopping around. It knew the browser she used, the device she submitted from, the time of day she applied.

From these signals, the system constructed a Margaret. Not Margaret as she knows herself: a woman who has never missed a payment in forty years, who keeps a handwritten ledger of her expenses, who considers debt a moral failing and treats her obligations with something close to reverence. That Margaret is invisible to the system. The system's Margaret is a probability distribution, a predicted behavior, a risk score.

She was approved, as it happens. But the interest rate she was offered reflected the system's Margaret, not Margaret's Margaret. And she will never know the difference, because the system does not explain which Margaret it saw.

*I think, therefore I am,* Descartes wrote. The foundation of identity is the self's own thinking, the irreducible fact of consciousness authenticating itself from the inside out.

Something has been quietly inverted. The systems that now mediate Margaret's access to credit, healthcare, insurance, and a hundred other dimensions of modern life do not care what Margaret thinks. They think about Margaret. And what they think becomes, functionally, what she is.

*You think, therefore I am.* The "you" is the algorithm. And the "I" that results is not a self discovered through reflection but an identity assigned through computation.

### The Mirror That Projects

Charles Taylor argued that identity is not something we develop alone. We become who we are through dialogue with the people who matter to us, through their recognition and their misrecognition, through the friction of being seen by others who are themselves trying to see. Your mother's understanding of you is partial and biased but it is also, in Taylor's sense, a genuine act of recognition. She sees a consciousness and responds to its particularity, even when she gets the particulars wrong.

Algorithmic systems have inserted themselves into this process, but they are not performing recognition. They are performing classification. The difference is fundamental. Recognition involves seeing someone. Classification involves sorting something. When Margaret's credit application is processed, the system does not see Margaret. It assigns an instance to a category. Yet the consequences of that assignment, approval or denial, rate offered or withheld, carry the same weight that recognition and misrecognition carry in human life.

The system is a mirror, but it does not reflect. It projects. What it projects is not who you are but what you are worth, to whom, and for what purpose.

Consider how this plays out in Margaret's healthcare. Her insurer's risk model has constructed a version of her from diagnostic codes, pharmacy claims, utilization patterns, and demographic correlates. This version predicts her expected costs over the next twelve months. Based on that prediction, her care management program decides how aggressively to intervene, what resources to allocate, how many nurse calls to schedule.

Part 32 of this series explored how clinical language shapes what AI systems see. The chart says "non-compliant with medication regimen." Margaret says she skips her evening dose because it makes her dizzy and she is afraid of falling in the dark on the way to the bathroom. The chart-Margaret is a risk to be managed. The actual Margaret is a person making a reasonable calculation about competing dangers. But the system's Margaret, the one that determines her care, is built from the chart.

**The self encounters itself through a mirror that was never designed to show the person. It was designed to show the institution what it needs to know about the person. These are not the same thing.**

### Who You Are Before You Act

The economic dimension is where the inversion achieves its sharpest expression.

In the world Descartes imagined, identity arises from what the individual does: from thinking, choosing, acting. In algorithmic capitalism, identity is increasingly determined before the individual acts, based on what people statistically similar to them have done.

This is actuarial logic, and it predates AI by decades. Insurance companies have always assigned risk based on group resemblance. But AI has generalized this logic across the entire economic landscape. Hiring algorithms score candidates based on behavioral predictors derived from previous hires. Lending algorithms assess risk based on variables that correlate with historical default. Pricing algorithms adjust what you pay based on your predicted willingness to pay. In each case, your economic identity is not earned through your actions but assigned through your resemblance to a statistical population.

Virginia Eubanks documented what this means for people living in poverty. In child welfare, homeless services, and public benefits, predictive models construct risk profiles that effectively pre-punish people for belonging to demographic categories associated with negative outcomes. If people in your circumstances tend to need emergency services, you are an emergency cost. If people in your neighborhood tend to default, you are a default risk. Your actual behavior, your thrift, your reliability, your ingenuity in stretching what little you have, disappears behind the group prediction.

Think about what this means for a young man named James, twenty-three, Black, from a zip code the model has learned to associate with high claim rates. James has never missed a payment on anything. He works two jobs. He saves. He is, by any individual measure, exactly the kind of person a lender should want. But the model does not see James. It sees the zip code, the age, the demographic category. It sees the statistical James, and the statistical James carries risks that the actual James does not.

**Your value is determined before you act, by a system that has already decided what your actions will be.**

This is the series' "I AM NOT AVERAGE" problem writ large. James is not average. Margaret is not average. Nobody is average. But actuarial identity treats everyone as a deviation from their group's mean, and the group was assigned, not chosen.

### The Social Sort

Identity is never purely individual. We are who we are in relation to others, through the groups we belong to and are sorted into.

Erving Goffman described how people manage the impressions others form of them, navigating between the identity that others project onto them and the identity they experience from within. You might be read as "working class" by your accent, your clothes, your address, and you might manage that reading, amplify it in some contexts, minimize it in others, complicate it with details that don't fit the category. This navigation, messy and never fully resolved, was part of how individuals shaped their own social identity and, in aggregate, how categories themselves evolved.

Algorithmic identity assignment short-circuits this negotiation. When a system determines that Margaret's age, medication history, zip code, and browsing behavior place her in a particular segment, the determination is not offered as a hypothesis to be discussed. It is enacted as infrastructure. She receives the ads, the offers, the prices, and the care management protocols that correspond to her assigned segment. She experiences the consequences of a social identity she never chose and cannot effectively contest.

There is no backstage, in Goffman's terms, where Margaret can drop the performance and be herself. The algorithmic perception follows her across platforms, across institutions, across time. She cannot present a different version of herself to the credit card company than the one the data has already constructed, because the data arrived before she did.

Safiya Umoja Noble showed how search engines reproduced racial and gender stereotypes not through explicit programming but through statistical patterns in training data. The system does not need to believe anything. It needs only to reflect patterns generated by a society that has organized itself around certain beliefs for centuries. The algorithm inherits the social imagination and operationalizes it at scale.

**The group model precedes the individual. You are sorted before you are seen.**

### The Flattening

Culture is how groups make meaning. It is not a list of preferences.

But to an algorithmic system, culture is exactly a list of preferences. Language use, media consumption, food purchases, religious affiliation inferred from location data near houses of worship, musical tastes. These behavioral signals are aggregated into cultural profiles that institutions use to segment markets, target messages, and allocate resources.

The problem is not that these proxies are inaccurate, though they often are. The problem is that they flatten culture into consumption and reduce meaning-making to pattern-matching. Margaret's relationship to her Catholic faith, for instance, carries layers of meaning that have accumulated over seven decades: the comfort of ritual, the community of the parish, the arguments with doctrine she has never resolved, the memory of her mother's rosary, the complicated feelings about the Church's failures. The algorithm sees "Catholic" as a variable that predicts certain purchasing patterns and media preferences. The entire interior dimension, the part that makes it identity rather than demographics, is invisible.

But the system's response to the visible part shapes the cultural landscape Margaret inhabits. The content surfaced for her, the communities suggested, the options presented, all reflect the algorithm's model of her culture rather than her culture itself. Over time, the environment constructed around her preferences begins to feel like the culture itself, narrower and more coherent than the messy, contradictory, evolving thing culture actually is.

Yuk Hui has written about what he calls technodiversity, the idea that different cultures have historically integrated technology with their worldviews in distinct ways, and that the global spread of a single technological paradigm threatens this plurality. AI systems trained predominantly on English-language data, built within particular epistemological assumptions, and deployed worldwide through platform capitalism carry a cultural logic that presents itself as neutral infrastructure. When these systems determine how a Navajo elder or a Dalit student encounters the digital world, they impose a cultural frame while claiming merely to serve preferences.

**Personalization is the mechanism of flattening. By modeling you as a bundle of preferences, the system replaces the cultural question, what does this mean, with the market question, what will you click on.**

### The Loop

These distortions do not operate in parallel. They form a loop.

The individual encounters herself through algorithmic classification. That classification is shaped by group-level patterns she cannot escape. Those group patterns reflect cultural assumptions embedded in training data. The economic consequences of classification constrain her material conditions, which generate the behavioral data that feeds the next cycle of classification.

Margaret's credit score affects her insurance rate. Her insurance rate affects her healthcare options. Her healthcare options shape her medication adherence. Her medication adherence generates the data that informs her next risk assessment. Each turn of the loop narrows the version of Margaret that institutions encounter, and each narrowing makes it harder for the actual Margaret to be seen.

Pierre Bourdieu described something like this with his concept of habitus: social structures internalized as dispositions that reproduce those structures through practice. But algorithmic identity adds a new mechanism. The dispositions no longer need to be internalized. They are externalized into computational systems that enforce them regardless of what Margaret thinks or feels or believes about herself. She does not need to accept her credit score for it to determine her interest rate. She does not need to agree with her risk profile for it to shape her care.

**The habitus has been automated. And in being automated, it has been removed from the domain of human negotiation where it might, slowly, be transformed.**

### What Remains

If identity is increasingly constituted by algorithmic perception rather than individual reflection, what forms of resistance are available?

One is strategic opacity: the deliberate effort to make yourself illegible. Cash instead of credit cards, VPNs, false information provided to data collectors. James C. Scott analyzed how subordinate populations have always resisted state legibility projects through small acts of refusal and misdirection. But opacity carries costs. In a world where algorithmic legibility is increasingly required for access to services, employment, and participation, choosing invisibility means choosing exclusion.

A second is collective contestation: the political demand that these systems be made transparent, accountable, and subject to democratic governance. The EU's AI Act, algorithmic auditing movements, grassroots data rights campaigns. But the systems evolve faster than the regulations that attempt to govern them.

A third is epistemic: maintaining ways of knowing yourself that are irreducible to data. This means practices of self-reflection, storytelling, communal meaning-making, embodied experience that no behavioral analytics can capture. It means insisting that the question *Who am I?* cannot be answered by any system that has never asked it.

We do not know which of these, alone or in combination, will prove adequate. We do not know whether the loop can be interrupted at all, or whether the velocity of algorithmic identity construction has already outpaced our capacity to contest it. This is speculative territory, and intellectual honesty requires saying so.

What we do know is this:

### The Unasked Question

The deepest distortion may be the simplest to name. In a world organized around *you think, therefore I am*, no system that models Margaret has ever asked her the question that every genuine human encounter begins with.

*Who are you?*

Not "what category do you belong to?" Not "what is your predicted behavior?" Not "what have people like you historically done?" But the irreducibly open question that invites a response no model could predict.

Margaret, asked that question over coffee, might talk about her garden. About her late husband's laugh. About her fear of losing her independence and her refusal to admit it. About the book she read last week that changed how she thinks about forgiveness. None of this would improve a risk model. All of it is who she is.

Descartes was wrong about many things. But he was right that the thinking which constitutes the self must be one's own. When it belongs to another, what it constitutes is not a self but a product.

*Cogito ergo sum* was always aspirational. Most people do not arrive at identity through solitary rational reflection. They arrive at it through the messy, relational, embodied, culturally saturated process of living among others who see them, misrecognize them, challenge them, and sometimes, in rare and precious moments, actually know them.

The threat of *you think, therefore I am* is not that algorithmic identity is worse than this process. It is that algorithmic identity is faster, cheaper, and scalable. And in being all of these things, it renders the original process obsolete. Not because the original was inferior. Because no one can afford it anymore.

**That is the inversion. Not a philosophical argument. A business case.**

---

*This is Part 48 of The Approximate Mind, a series examining how AI might serve human flourishing rather than human extraction. Part 47 explored the three forms of delegation, cognition, execution, and burden, and what each costs. This article inverts Descartes' foundational claim to ask what happens when identity is assigned by computation rather than discovered through reflection, and whether the messy human process of recognition can survive being outpriced.*

---

**Philosophy of Identity and Recognition:**
Taylor, Charles. *Sources of the Self: The Making of the Modern Identity*. Harvard University Press, 1989.
Bourdieu, Pierre. *Outline of a Theory of Practice*. Translated by Richard Nice, Cambridge University Press, 1977.

**Surveillance and Algorithmic Capitalism:**
Zuboff, Shoshana. *The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power*. PublicAffairs, 2019.
Noble, Safiya Umoja. *Algorithms of Oppression: How Search Engines Reinforce Racism*. New York University Press, 2018.

**Sociology of Identity and Stigma:**
Goffman, Erving. *Stigma: Notes on the Management of Spoiled Identity*. Prentice-Hall, 1963.

**Inequality and Algorithmic Systems:**
Eubanks, Virginia. *Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor*. St. Martin's Press, 2018.

**Technology, Culture, and Resistance:**
Hui, Yuk. *The Question Concerning Technology in China: An Essay in Cosmotechnics*. Urbanomic, 2016.
Scott, James C. *Weapons of the Weak: Everyday Forms of Peasant Resistance*. Yale University Press, 1985.

**Political Economy:**
Marx, Karl. *Capital: Volume I*. Translated by Ben Fowkes, Penguin Books, 1976.
